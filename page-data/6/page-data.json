{"componentChunkName":"component---src-templates-posts-js","path":"/6","result":{"data":{"allContentfulPost":{"edges":[{"node":{"title":"Structuring Your TensorFlow Models","id":"f471f3cd-3437-5277-bd49-39be01aa1d50","slug":"structuring-tf","publishDate":"April 20, 2018","heroImage":{"id":"434fa86e-ac5b-52f4-8bd1-6ac1432526e2","title":"cover-datascience","fluid":{"aspectRatio":1.5,"src":"//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&h=1200&q=50 1800w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&h=1200&q=50&fm=webp 1800w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"4d7470d9-5f03-52b3-9467-540a06ac45c4","childMarkdownRemark":{"id":"6051bcbf-86f2-572c-b3ef-aa86f537857f","timeToRead":4,"html":"<p>이 글은 저자의 허락을 받아 번역한 글 입니다. <a href=\"https://danijar.com/structuring-your-tensorflow-models/\">원문 링크</a></p>\n<p>TensorFlow에서 모델을 정의하다보면 어느새 많은 양의 코드가 생성된 경험이 있을 것 입니다. 어떻게 하면 가독성과 재사용성이 높은 코드로 구성할 수 있을까요? 여기에서 실제 동작하는 예시 코드를 확인하실 수 있습니다. <a href=\"https://gist.github.com/danijar/8663d3bbfd586bffecf6a0094cd116f2\">Gist Link</a></p>\n<br>\n<h2 id=\"defining-the-compute-graph\" style=\"position:relative;\"><a href=\"#defining-the-compute-graph\" aria-label=\"defining the compute graph permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Defining the Compute Graph</h2>\n<p>모델 당 하나의 클래스부터 시작하는 것이 좋습니다. 그 클래스의 인터페이스는 무엇인가요? 일반적으로 모델은 input data와 target placeholder를 연결하며 training, evaluation 그리고 inference 관련 함수를 제공합니다.</p>\n<script src=\"https://gist.github.com/Swalloow/c90dea8bce9ceee147851656f48fac9f.js\"></script>\n<p>위의 코드가 기본적으로 TensorFlow codebase에서 모델이 정의되는 방식입니다. 그러나 여기에도 몇 가지 문제가 있습니다. 가장 중요한 문제는 전체 그래프가 단일 함수 생성자로 정의된다는 점 입니다. 이렇게 되면 가독성이 떨어지며 재사용이 어렵습니다.</p>\n<br>\n<h2 id=\"using-properties\" style=\"position:relative;\"><a href=\"#using-properties\" aria-label=\"using properties permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Using Properties</h2>\n<p>함수가 호출 될 때마다 그래프는 확장되기 때문에 함수로 분할하는 것만으로는 부족합니다. 따라서 함수를 처음 호출하는 시점에 operation들이 그래프에 추가되도록 해야합니다. 이러한 방식을 기본적으로 <strong>lazy-loading</strong>이라고 합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">Model</span><span class=\"token punctuation\">:</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> data<span class=\"token punctuation\">,</span> target<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        data_size <span class=\"token operator\">=</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">.</span>get_shape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n        target_size <span class=\"token operator\">=</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span>target<span class=\"token punctuation\">.</span>get_shape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n        weight <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>Variable<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>truncated_normal<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>data_size<span class=\"token punctuation\">,</span> target_size<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        bias <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>Variable<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>constant<span class=\"token punctuation\">(</span><span class=\"token number\">0.1</span><span class=\"token punctuation\">,</span> shape<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>target_size<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        incoming <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>matmul<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">,</span> weight<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> bias\n        self<span class=\"token punctuation\">.</span>_prediction <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>softmax<span class=\"token punctuation\">(</span>incoming<span class=\"token punctuation\">)</span>\n        cross_entropy <span class=\"token operator\">=</span> <span class=\"token operator\">-</span>tf<span class=\"token punctuation\">.</span>reduce_sum<span class=\"token punctuation\">(</span>target<span class=\"token punctuation\">,</span> tf<span class=\"token punctuation\">.</span>log<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>_prediction<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>_optimize <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>train<span class=\"token punctuation\">.</span>RMSPropOptimizer<span class=\"token punctuation\">(</span><span class=\"token number\">0.03</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>minimize<span class=\"token punctuation\">(</span>cross_entropy<span class=\"token punctuation\">)</span>\n        mistakes <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>not_equal<span class=\"token punctuation\">(</span>\n            tf<span class=\"token punctuation\">.</span>argmax<span class=\"token punctuation\">(</span>target<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> tf<span class=\"token punctuation\">.</span>argmax<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>_prediction<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>_error <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reduce_mean<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>cast<span class=\"token punctuation\">(</span>mistakes<span class=\"token punctuation\">,</span> tf<span class=\"token punctuation\">.</span>float32<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token decorator annotation punctuation\">@property</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">prediction</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>_prediction\n\n    <span class=\"token decorator annotation punctuation\">@property</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">optimize</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>_optimize\n\n    <span class=\"token decorator annotation punctuation\">@property</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">error</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>_error</code></pre></div>\n<p>위의 방식이 첫 번째 예제보다 훨씬 좋습니다. 이제 코드는 독립적인 함수로 구성되어 있습니다. 그러나 아직 코드는 lazy-loading으로 인해 약간 복잡해보입니다. 이를 어떻게 개선 할 수 있는지 보도록 하겠습니다.</p>\n<br>\n<h2 id=\"lazy-property-decorator\" style=\"position:relative;\"><a href=\"#lazy-property-decorator\" aria-label=\"lazy property decorator permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Lazy Property Decorator</h2>\n<p>파이썬은 아주 유연한 언어입니다. 이제 마지막 예제에서 중복 코드를 제거하는 방법을 보여드리겠습니다. 우리는 <code class=\"language-text\">@property</code>처럼 동작하지만 한번만 함수를 평가하는 decorator를 사용할 것입니다. decorator는 함수(접두사를 앞에 붙임)의 이름을 따서 멤버에 결과를 저장하고 나중에 호출되는 시점에 해당 값을 반환합니다. custom decorator를 아직 사용해본적이 없다면, <a href=\"http://blog.apcelent.com/python-decorator-tutorial-with-example.html\">이 가이드</a>를 참고하시면 됩니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">Model</span><span class=\"token punctuation\">:</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> data<span class=\"token punctuation\">,</span> target<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>data <span class=\"token operator\">=</span> data\n        self<span class=\"token punctuation\">.</span>target <span class=\"token operator\">=</span> target\n        self<span class=\"token punctuation\">.</span>_prediction <span class=\"token operator\">=</span> <span class=\"token boolean\">None</span>\n        self<span class=\"token punctuation\">.</span>_optimize <span class=\"token operator\">=</span> <span class=\"token boolean\">None</span>\n        self<span class=\"token punctuation\">.</span>_error <span class=\"token operator\">=</span> <span class=\"token boolean\">None</span>\n\n    <span class=\"token decorator annotation punctuation\">@property</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">prediction</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> <span class=\"token keyword\">not</span> self<span class=\"token punctuation\">.</span>_prediction<span class=\"token punctuation\">:</span>\n            data_size <span class=\"token operator\">=</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>get_shape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n            target_size <span class=\"token operator\">=</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>target<span class=\"token punctuation\">.</span>get_shape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n            weight <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>Variable<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>truncated_normal<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>data_size<span class=\"token punctuation\">,</span> target_size<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n            bias <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>Variable<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>constant<span class=\"token punctuation\">(</span><span class=\"token number\">0.1</span><span class=\"token punctuation\">,</span> shape<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>target_size<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n            incoming <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>matmul<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">,</span> weight<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> bias\n            self<span class=\"token punctuation\">.</span>_prediction <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>softmax<span class=\"token punctuation\">(</span>incoming<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>_prediction\n\n    <span class=\"token decorator annotation punctuation\">@property</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">optimize</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> <span class=\"token keyword\">not</span> self<span class=\"token punctuation\">.</span>_optimize<span class=\"token punctuation\">:</span>\n            cross_entropy <span class=\"token operator\">=</span> <span class=\"token operator\">-</span>tf<span class=\"token punctuation\">.</span>reduce_sum<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>target<span class=\"token punctuation\">,</span> tf<span class=\"token punctuation\">.</span>log<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>prediction<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n            optimizer <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>train<span class=\"token punctuation\">.</span>RMSPropOptimizer<span class=\"token punctuation\">(</span><span class=\"token number\">0.03</span><span class=\"token punctuation\">)</span>\n            self<span class=\"token punctuation\">.</span>_optimize <span class=\"token operator\">=</span> optimizer<span class=\"token punctuation\">.</span>minimize<span class=\"token punctuation\">(</span>cross_entropy<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>_optimize\n\n    <span class=\"token decorator annotation punctuation\">@property</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">error</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> <span class=\"token keyword\">not</span> self<span class=\"token punctuation\">.</span>_error<span class=\"token punctuation\">:</span>\n            mistakes <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>not_equal<span class=\"token punctuation\">(</span>\n                tf<span class=\"token punctuation\">.</span>argmax<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>target<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> tf<span class=\"token punctuation\">.</span>argmax<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>prediction<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n            self<span class=\"token punctuation\">.</span>_error <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reduce_mean<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>cast<span class=\"token punctuation\">(</span>mistakes<span class=\"token punctuation\">,</span> tf<span class=\"token punctuation\">.</span>float32<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>_error</code></pre></div>\n<p>위의 decorator를 사용해서 예시 코드는 아래와 같이 간결해졌습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> functools\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">lazy_property</span><span class=\"token punctuation\">(</span>function<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    attribute <span class=\"token operator\">=</span> <span class=\"token string\">'_cache_'</span> <span class=\"token operator\">+</span> function<span class=\"token punctuation\">.</span>__name__\n\n    <span class=\"token decorator annotation punctuation\">@property</span>\n    <span class=\"token decorator annotation punctuation\">@functools<span class=\"token punctuation\">.</span>wraps</span><span class=\"token punctuation\">(</span>function<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">decorator</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> <span class=\"token keyword\">not</span> <span class=\"token builtin\">hasattr</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> attribute<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token builtin\">setattr</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> attribute<span class=\"token punctuation\">,</span> function<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> <span class=\"token builtin\">getattr</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> attribute<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">return</span> decorator</code></pre></div>\n<p>생성자에서 property를 언급했다는 부분이 중요합니다. 이렇게 구성한다면 <code class=\"language-text\">tf.initialize_variables()</code>를 실행할 때 전체 그래프가 정의됩니다.</p>\n<br>\n<h2 id=\"organizing-the-graph-with-scopes\" style=\"position:relative;\"><a href=\"#organizing-the-graph-with-scopes\" aria-label=\"organizing the graph with scopes permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Organizing the Graph with Scopes</h2>\n<p>이제 코드에서 모델을 정의하는 부분은 깔끔해졌지만, 그래프의 연산 부분은 여전히 복잡합니다. 만일 <a href=\"https://www.tensorflow.org/programmers_guide/graph_viz\">그래프를 시각화</a>한다면, 서로 연결되어 있는 노드가 많이 나타날 것 입니다. 이를 해결하기 위한 방법은 <code class=\"language-text\">tf.name_scope(&#39;name&#39;)</code> 또는 <code class=\"language-text\">tf.variable_scope(&#39;name&#39;)</code>을 사용하여 각 함수의 내용을 래핑하는 것 입니다. 이렇게 하면 노드들은 그래프 상에서 그룹화되어 있을 것 입니다. 우리는 이전에 만들었던 decorator를 이용하여 이를 자동으로 적용시켜보겠습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">Model</span><span class=\"token punctuation\">:</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> data<span class=\"token punctuation\">,</span> target<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>data <span class=\"token operator\">=</span> data\n        self<span class=\"token punctuation\">.</span>target <span class=\"token operator\">=</span> target\n        self<span class=\"token punctuation\">.</span>prediction\n        self<span class=\"token punctuation\">.</span>optimize\n        self<span class=\"token punctuation\">.</span>error\n\n    <span class=\"token decorator annotation punctuation\">@lazy_property</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">prediction</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        data_size <span class=\"token operator\">=</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>get_shape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n        target_size <span class=\"token operator\">=</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>target<span class=\"token punctuation\">.</span>get_shape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n        weight <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>Variable<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>truncated_normal<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>data_size<span class=\"token punctuation\">,</span> target_size<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        bias <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>Variable<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>constant<span class=\"token punctuation\">(</span><span class=\"token number\">0.1</span><span class=\"token punctuation\">,</span> shape<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>target_size<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        incoming <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>matmul<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">,</span> weight<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> bias\n        <span class=\"token keyword\">return</span> tf<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>softmax<span class=\"token punctuation\">(</span>incoming<span class=\"token punctuation\">)</span>\n\n    <span class=\"token decorator annotation punctuation\">@lazy_property</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">optimize</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        cross_entropy <span class=\"token operator\">=</span> <span class=\"token operator\">-</span>tf<span class=\"token punctuation\">.</span>reduce_sum<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>target<span class=\"token punctuation\">,</span> tf<span class=\"token punctuation\">.</span>log<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>prediction<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        optimizer <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>train<span class=\"token punctuation\">.</span>RMSPropOptimizer<span class=\"token punctuation\">(</span><span class=\"token number\">0.03</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> optimizer<span class=\"token punctuation\">.</span>minimize<span class=\"token punctuation\">(</span>cross_entropy<span class=\"token punctuation\">)</span>\n\n    <span class=\"token decorator annotation punctuation\">@lazy_property</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">error</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        mistakes <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>not_equal<span class=\"token punctuation\">(</span>\n            tf<span class=\"token punctuation\">.</span>argmax<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>target<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> tf<span class=\"token punctuation\">.</span>argmax<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>prediction<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> tf<span class=\"token punctuation\">.</span>reduce_mean<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>cast<span class=\"token punctuation\">(</span>mistakes<span class=\"token punctuation\">,</span> tf<span class=\"token punctuation\">.</span>float32<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>lazy caching 이외에도 TensorFlow의 기능을 포함시키므로 decorator에 새로운 이름을 지정했습니다. 그 외의 나머지 부분은 이전과 동일합니다.</p>\n<p>이제 <code class=\"language-text\">@define_scope</code> decorator를 통해 <code class=\"language-text\">tf.variable_scope()</code>에 인자를 전달할 수 있습니다. 예를 들어 해당 scope에 default initializer를 정의할 수 있습니다. 이 부분이 더 궁금하다면 <a href=\"https://gist.github.com/danijar/8663d3bbfd586bffecf6a0094cd116f2\">전체 예제 코드</a>를 확인해보시면 됩니다.</p>\n<br>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h2>\n<p><a href=\"https://danijar.com/structuring-your-tensorflow-models/\">https://danijar.com/structuring-your-tensorflow-models/</a></p>\n<br>","excerpt":"이 글은 저자의 허락을 받아 번역한 글 입니다. 원문 링크 TensorFlow…"}}}},{"node":{"title":"Data Science inconvenient truth","id":"8d868e76-473f-5075-bfa6-fef3751eb0d8","slug":"data-science-inconvenient-truth","publishDate":"April 01, 2018","heroImage":{"id":"434fa86e-ac5b-52f4-8bd1-6ac1432526e2","title":"cover-datascience","fluid":{"aspectRatio":1.5,"src":"//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&h=1200&q=50 1800w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&h=1200&q=50&fm=webp 1800w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"9ac3e892-037b-56dd-823b-b494ff739acd","childMarkdownRemark":{"id":"ee66eb82-0a50-5b28-8ce7-83a5921f67eb","timeToRead":1,"html":"<h2 id=\"데이터과학의-불편한-진실\" style=\"position:relative;\"><a href=\"#%EB%8D%B0%EC%9D%B4%ED%84%B0%EA%B3%BC%ED%95%99%EC%9D%98-%EB%B6%88%ED%8E%B8%ED%95%9C-%EC%A7%84%EC%8B%A4\" aria-label=\"데이터과학의 불편한 진실 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>데이터과학의 불편한 진실</h2>\n<ul>\n<li>Data is never clean (데이터는 절대 깨끗하지 않다)</li>\n<li>You will spend most of your time cleaning and preparing data (당신은 분석의 대부분의 시간을 전처리 단계에서 보내게 될 것이다)</li>\n<li>95% of tasks do not require deep learning (95% 일은 Deep Learning을 필요로 하지 않는다)</li>\n<li>In 90% of cases generalized linear regression will do the trick (실제 분석의 90%는 GLM으로 해결된다 )</li>\n<li>Big Data is just a tool (빅 데이터는 단지 도구일 뿐이다)</li>\n<li>You should embrace the Bayesian approach (당신은 베이지안 접근을 포용해야 한다)</li>\n<li>No one cares how you did it (사용자 입장에서는 네가 어떤 방법을 사용했는가는 중요하지 않다) </li>\n<li>Academia and business are two different worlds (학계와 산업계는 서로 다른 세계이다)</li>\n<li>Presentation is key - be a master of Power Point (프리젠테이션이 핵심이다: PowerPoint의 마스터가 되라)</li>\n<li>All models are false, but some are useful (모든 모델은 틀렸다, 하지만 몇몇은 유용하다)</li>\n<li>There is no fully automated Data Science. You need to get your hands dirty (완전 자동화된 데이터 과학같은 것은 없다. 인간이 개입되어야 할 부분이 있다)</li>\n</ul>\n<br>\n<p>Ref: <a href=\"https://www.kdnuggets.com/2015/05/data-science-inconvenient-truth.html\">https://www.kdnuggets.com/2015/05/data-science-inconvenient-truth.html</a></p>","excerpt":"데이터과학의 불편한 진실 Data is never clean (데이터는 절대 깨끗하지 않다) You will spend most of your…"}}}},{"node":{"title":"Deep Learning Programming Style: Symbolic, Imperative","id":"6aef0ac4-f29d-525a-aa13-9d322d1953cb","slug":"deep-learning-style","publishDate":"January 05, 2018","heroImage":{"id":"434fa86e-ac5b-52f4-8bd1-6ac1432526e2","title":"cover-datascience","fluid":{"aspectRatio":1.5,"src":"//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&h=1200&q=50 1800w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&h=1200&q=50&fm=webp 1800w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"285dd567-d6ee-52b3-bfb5-644c12ac1f42","childMarkdownRemark":{"id":"52a8846c-a4f3-5482-82c0-142d02eca58f","timeToRead":5,"html":"<p>TensorFlow 1.5 버전부터 <a href=\"https://github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/contrib/eager\">Eager Execution</a> 이라는 기능이 추가되었습니다.\n다시 말해서 <code class=\"language-text\">imperative programming style</code>을 지원한다고 적혀있는데, 기존의 방식과 어떤 차이가 있는지 알아보겠습니다.\nMXNet의 <a href=\"https://mxnet.incubator.apache.org/architecture/program_model.html\">Deep Learning Programming Style</a> 문서를 번역한 내용입니다.</p>\n<br>\n<h2 id=\"deep-learning-programming-style\" style=\"position:relative;\"><a href=\"#deep-learning-programming-style\" aria-label=\"deep learning programming style permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Deep Learning Programming Style</h2>\n<p>우리는 항상 <strong>성능과 최적화</strong>에 대한 고민을 합니다. 하지만 그 이전에 잘 동작하는 코드인지 여부가 중요합니다. 이제는 다양한 딥러닝 라이브러리들이 존재하지만 각자 프로그래밍 방식에 대해 다른 접근 방식을 가지고 있기 때문에 학습하는 것도 힘들며, 이를 이용하여 명확하고 직관적인 deep learning 코드를 작성하는 것도 어렵습니다.</p>\n<p>이 문서에서는 가장 중요한 두 가지 디자인 패턴에 집중하려고 합니다.</p>\n<ol>\n<li>Whether to embrace the symbolic or imperative paradigm for mathematical computation.</li>\n<li>Whether to build networks with bigger (more abstract) or more atomic operations.</li>\n</ol>\n<hr>\n<h2 id=\"symbolic-vs-imperative-programs\" style=\"position:relative;\"><a href=\"#symbolic-vs-imperative-programs\" aria-label=\"symbolic vs imperative programs permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Symbolic vs. Imperative Programs</h2>\n<p>만일 당신이 파이썬 또는 C++ 개발자라면, 이미 <code class=\"language-text\">Imperative program</code>과 친숙할 것 입니다.\nImperative style program들은 바로 연산을 수행합니다. 대부분의 파이썬 코드들이 imperative 한 형태를 보여주는데, 예를 들면 아래와 같은 Numpy 코드를 말합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\na <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span>\nb <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token number\">2</span>\nc <span class=\"token operator\">=</span> b <span class=\"token operator\">*</span> a\nd <span class=\"token operator\">=</span> c <span class=\"token operator\">+</span> <span class=\"token number\">1</span></code></pre></div>\n<p>프로그램이 <code class=\"language-text\">c = b * a</code>를 수행하도록 명령을 내리면, 실제로 연산이 실행됩니다.</p>\n<p>반면에 <code class=\"language-text\">Symbolic program</code>은 조금 다릅니다. Symbolic-style program에서는 먼저 function (potentially complex) 을 정의합니다. <strong>function을 정의했다고 해서 실제 연산이 수행되는 것은 아닙니다.</strong> 우리는 그저 placeholder 값에 function을 정의한 것 뿐 입니다. 이 과정 이후에 function을 컴파일 할 수 있으며, 실제 입력 값을 통해 이를 평가하게 됩니다. 아래는 위에서 언급했던 imperative 코드를 symbolic style로 변환한 예제입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">A <span class=\"token operator\">=</span> Variable<span class=\"token punctuation\">(</span><span class=\"token string\">'A'</span><span class=\"token punctuation\">)</span>\nB <span class=\"token operator\">=</span> Variable<span class=\"token punctuation\">(</span><span class=\"token string\">'B'</span><span class=\"token punctuation\">)</span>\nC <span class=\"token operator\">=</span> B <span class=\"token operator\">*</span> A\nD <span class=\"token operator\">=</span> C <span class=\"token operator\">+</span> Constant<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># compiles the function</span>\nf <span class=\"token operator\">=</span> <span class=\"token builtin\">compile</span><span class=\"token punctuation\">(</span>D<span class=\"token punctuation\">)</span>\nd <span class=\"token operator\">=</span> f<span class=\"token punctuation\">(</span>A<span class=\"token operator\">=</span>np<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> B<span class=\"token operator\">=</span>np<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token operator\">*</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>보시다시피 symbolic 버전에서는 <code class=\"language-text\">C = B * A</code>가 수행되는 시점에 실제로 연산이 일어나지 않습니다. 대신에 이 operation은 연산 과정을 표현하는 <strong>computation graph (aka. symbolic graph)</strong> 를 생성합니다. 예를 들면, D의 연산을 위해 아래와 같은 computation graph가 생성됩니다.</p>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1hjs2EOtub_KjPqj4nflGGR1Rjcaklp7M\" alt=\"comp_graph\"></p>\n<p>대부분의 symbolic-style 프로그램들은 명시적으로든 암시적으로든 컴파일 단계를 포함합니다. 이를 통해 computation graph를 언제든 호출할 수 있는 함수로 변환시켜줍니다. 위의 예제에서도 실제 연산은 코드의 마지막 줄에서만 수행됩니다. 이를 통해 얻을 수 있는 점은 computation graph를 작성하는 단계와 실행하는 단계를 명확히 분리할 수 있다는 것 입니다. Neural Network에서도 우리는 전체 모델을 단일 computation graph로 정의합니다.</p>\n<p><code class=\"language-text\">Torch</code>, <code class=\"language-text\">Chiner</code> 그리고 <code class=\"language-text\">Minerva</code>와 같은 딥러닝 라이브러리들은 imperative style을 사용하고 있습니다. symbolic-style을 사용하는 딥러닝 라이브러리로는 <code class=\"language-text\">Theano</code>, <code class=\"language-text\">CGT</code> 그리고 <code class=\"language-text\">TensorFlow</code>가 있습니다. 그리고 <code class=\"language-text\">CXXNet</code> 이나 <code class=\"language-text\">Caffe</code>와 같은 라이브러리들은 설정파일에 의존하는 방식으로 symbolic style을 지원합니다. (ex. Caffe의 prototxt)\n이제 두 가지 딥러닝 프로그래밍 방식에 대해 이해했으니, 각 방식의 장점에 대해 알아보겠습니다.</p>\n<hr>\n<h2 id=\"imperative-programs-tend-to-be-more-flexible\" style=\"position:relative;\"><a href=\"#imperative-programs-tend-to-be-more-flexible\" aria-label=\"imperative programs tend to be more flexible permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Imperative Programs Tend to be More Flexible</h2>\n<p>imperative program은 프로그래밍 언어의 flow와 상당히 잘 맞아들어가며 유연하게 동작하는 것 처럼 보입니다. 그렇다면 왜 수 많은 딥러닝 라이브러리들이 symbolic 패러다임을 선택할까요? 가장 큰 이유는 <strong>메모리 사용량과 속도 측면에서의 효율성</strong> 때문입니다. 위에서 언급했던 예제로 돌아가 천천히 설명드리겠습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">a <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span>\nb <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token number\">2</span>\nc <span class=\"token operator\">=</span> b <span class=\"token operator\">*</span> a\nd <span class=\"token operator\">=</span> c <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></code></pre></div>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1hjs2EOtub_KjPqj4nflGGR1Rjcaklp7M\" alt=\"comp_graph\"></p>\n<p>주어진 array의 각 셀이 8 바이트의 메모리를 소모한다고 가정해보겠습니다. 콘솔에서 위의 프로그램을 실행하면 메모리가 얼마나 소모될까요?</p>\n<p>imperative program에서는 각 라인마다 메모리 할당이 요구됩니다. 사이즈가 10인 array가 4개 할당되므로 <code class=\"language-text\">4 * 10 * 8 = 320 bytes</code>의 메모리가 요구됩니다.\n반면 computation graph에서는 궁극적으로 d가 필요하다는 것을 알고 있기 때문에, 즉시 값을 메모리에 할당하는 대신에 <strong>메모리를 재사용</strong>할 수 있습니다. 예를 들어 b를 위해 할당된 공간에 c를 저장하도록 재사용하고, c를 위해 할당된 공간에 다시 d를 저장하도록 한다면 결국 요구되는 메모리는 <code class=\"language-text\">2 * 10 * 8 = 160 bytes</code> 절반으로 줄어들게 됩니다.</p>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1L42AX9v9qyDYbx2qkw_rGlubWOm-NzJD\" alt=\"comp_graph_fold\"></p>\n<p>Symbolic program은 사실 이보다 더 엄격합니다. 우리가 D에 대한 컴파일을 호출하면, 시스템은 오직 d 값이 필요하다는 사실만 인지합니다. 따라서 위와 같은 경우, 즉시 연산에 의해 c는 존재하지 않는 값으로 취급합니다.</p>\n<p>symbolic program이 안전하게 메모리를 재사용함으로 인해 우리가 얻는 장점은 분명 있습니다. 하지만, 나중에 우리가 c에 대해 접근해야하는 경우가 생긴다면 난감해집니다. 따라서 imperative program은 모든 가능한 경우의 수에 접근해야 할 때 더 좋은 대안이 될 수 있습니다. 대표적으로 파이썬 콘솔에서 imperative 버전의 코드를 실행시킨다면, 미래에 발생할 수 있는 변수를 중간 과정을 통해 미리 검사할 수 있습니다.</p>\n<p>Symbolic program은 <code class=\"language-text\">operation folding</code> 최적화도 수행해줍니다. 다시 위의 예시를 살펴보면 곱셈과 합 연산이 하나의 operation으로 합쳐지는 것을 그래프를 통해 확인할 수 있습니다. 만일 연산이 GPU 프로세서에 의해 실행된다면, 두 개가 아닌 하나의 GPU 커널만 실행될 것 입니다. 실제로 이는 CXXNet, Caffe와 같은 라이브러리에서 연산을 수행하는 방식입니다. Operation folding 방식은 계산 효율을 향상시켜줍니다.</p>\n<p>아시다시피 imperative program에서는 중간 값이 나중에 참조될 수 있기 때문에 operation folding 방식을 수행할 수 없습니다. 반면, computation graph에서는 전체 계산 그래프를 얻을 수 있고 어떤 값을 필요로하는지 알 수 있기 때문에 operation folding이 가능합니다.</p>\n<hr>\n<h2 id=\"case-study-backprop-and-autodiff\" style=\"position:relative;\"><a href=\"#case-study-backprop-and-autodiff\" aria-label=\"case study backprop and autodiff permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Case Study: Backprop and AutoDiff</h2>\n<p>이제 <code class=\"language-text\">auto differentiation</code>이나 <code class=\"language-text\">backpropagation</code>과 같은 문제를 통해 두 가지 프로그래밍 모델을 비교해보겠습니다. (chaining rule이 어떻게 동작하는지 보여주겠다)\n미분은 모델을 훈련시키는 메커니즘이기 때문에 딥러닝에 있어 정말 중요합니다. 우선 대부분의 딥러닝 모델에서 loss function을 정의하는데 이는 모델이 예측한 값이 실제 값과 얼마나 멀리 떨어져 있는지를 말합니다. 그리고 나서 훈련 데이터를 모델에게 전달하고, 각 step에서 모델의 parameter를 업데이트하여 loss를 최소화합니다. 즉, parameter가 업데이트 하는 방향은 loss function 결과에 의해 결정됩니다.</p>\n<p>imperative와 symbolic 방식 모두 <code class=\"language-text\">gradient</code> 계산을 수행할 수 있습니다. 먼저 아래의 파이썬 코드를 통해 imperative program이 어떻게 automatic differentiation을 수행하는지 알아보겠습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">array</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">object</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"\n    Simple Array object that support autodiff.\n    \"\"\"</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> value<span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>value <span class=\"token operator\">=</span> value\n        <span class=\"token keyword\">if</span> name<span class=\"token punctuation\">:</span>\n            self<span class=\"token punctuation\">.</span>grad <span class=\"token operator\">=</span> <span class=\"token keyword\">lambda</span> g <span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span>name <span class=\"token punctuation\">:</span> g<span class=\"token punctuation\">}</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__add__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> other<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">assert</span> <span class=\"token builtin\">isinstance</span><span class=\"token punctuation\">(</span>other<span class=\"token punctuation\">,</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">)</span>\n        ret <span class=\"token operator\">=</span> array<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>value <span class=\"token operator\">+</span> other<span class=\"token punctuation\">)</span>\n        ret<span class=\"token punctuation\">.</span>grad <span class=\"token operator\">=</span> <span class=\"token keyword\">lambda</span> g <span class=\"token punctuation\">:</span> self<span class=\"token punctuation\">.</span>grad<span class=\"token punctuation\">(</span>g<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> ret\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__mul__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> other<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">assert</span> <span class=\"token builtin\">isinstance</span><span class=\"token punctuation\">(</span>other<span class=\"token punctuation\">,</span> array<span class=\"token punctuation\">)</span>\n        ret <span class=\"token operator\">=</span> array<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>value <span class=\"token operator\">*</span> other<span class=\"token punctuation\">.</span>value<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">def</span> <span class=\"token function\">grad</span><span class=\"token punctuation\">(</span>g<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            x <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>grad<span class=\"token punctuation\">(</span>g <span class=\"token operator\">*</span> other<span class=\"token punctuation\">.</span>value<span class=\"token punctuation\">)</span>\n            x<span class=\"token punctuation\">.</span>update<span class=\"token punctuation\">(</span>other<span class=\"token punctuation\">.</span>grad<span class=\"token punctuation\">(</span>g <span class=\"token operator\">*</span> self<span class=\"token punctuation\">.</span>value<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">return</span> x\n        ret<span class=\"token punctuation\">.</span>grad <span class=\"token operator\">=</span> grad\n        <span class=\"token keyword\">return</span> ret\n\n<span class=\"token comment\"># some examples</span>\na <span class=\"token operator\">=</span> array<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'a'</span><span class=\"token punctuation\">)</span>\nb <span class=\"token operator\">=</span> array<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'b'</span><span class=\"token punctuation\">)</span>\nc <span class=\"token operator\">=</span> b <span class=\"token operator\">*</span> a\nd <span class=\"token operator\">=</span> c <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n<span class=\"token keyword\">print</span> d<span class=\"token punctuation\">.</span>value\n<span class=\"token keyword\">print</span> d<span class=\"token punctuation\">.</span>grad<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># Results</span>\n<span class=\"token comment\"># 3</span>\n<span class=\"token comment\"># {'a': 2, 'b': 1}</span></code></pre></div>\n<hr>\n<h2 id=\"model-checkpoints\" style=\"position:relative;\"><a href=\"#model-checkpoints\" aria-label=\"model checkpoints permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Model Checkpoints</h2>\n<p>모델을 저장하고 다시 불러오는 일 또한 중요합니다. 보통 Neural Network 모델을 저장한다는 것은 네트워크의 구조, 설정 값 그리고 weight 값의 저장을 의미합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">A <span class=\"token operator\">=</span> Variable<span class=\"token punctuation\">(</span><span class=\"token string\">'A'</span><span class=\"token punctuation\">)</span>\nB <span class=\"token operator\">=</span> Variable<span class=\"token punctuation\">(</span><span class=\"token string\">'B'</span><span class=\"token punctuation\">)</span>\nC <span class=\"token operator\">=</span> B <span class=\"token operator\">*</span> A\nD <span class=\"token operator\">=</span> C <span class=\"token operator\">+</span> Constant<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n\nD<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span><span class=\"token string\">'mygraph'</span><span class=\"token punctuation\">)</span>\nD2 <span class=\"token operator\">=</span> load<span class=\"token punctuation\">(</span><span class=\"token string\">'mygraph'</span><span class=\"token punctuation\">)</span>\nf <span class=\"token operator\">=</span> <span class=\"token builtin\">compile</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>D2<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># more operations</span>\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></code></pre></div>\n<p>설정 값을 체크하는 일은 symbolic program이 더 유리 합니다. symbolic 구조에서는 실제 연산을 수행할 필요가 없기 때문에 computation graph를 그대로 serialize 하면 됩니다.\n반면에 Imperative program은 연산 할 때 실행되기 때문에 코드 자체를 설정 파일로 저장하거나 그 위에 또 다른 레이어를 구성해야합니다.</p>\n<hr>\n<h2 id=\"parameter-updates\" style=\"position:relative;\"><a href=\"#parameter-updates\" aria-label=\"parameter updates permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Parameter Updates</h2>\n<p>computation graph의 경우 연산과정은 쉽게 설명할 수 있지만 parameter 업데이트에 대해서는 명확하지 못합니다. parameter update는 기본적으로 값의 변경(mutation)을 요구하기 때문에 computation graph의 개념과 맞지 않습니다. 따라서 대부분의 symbolic program들은 persistent state를 갱신하기 위해 special update 구문을 사용하고 있습니다.</p>\n<p>반면에 imperative style에서는 parameter 업데이트를 작성하는 것이 쉽습니다. 특히 서로 연관된 여러 업데이트가 필요할 때 더욱 그렇습니다. Symbolic program의 경우 업데이트 문은 사용자가 호출 할 때 실행됩니다. 이런 점에서 대부분의 symbolic deep learning 라이브러리는 parameter 업데이트에 대해 gradient 연산을 수행하면서 업데이트를 수행하는 imperative style로 다시 돌아갑니다.</p>\n<hr>\n<h2 id=\"there-is-no-strict-boundary\" style=\"position:relative;\"><a href=\"#there-is-no-strict-boundary\" aria-label=\"there is no strict boundary permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>There Is No Strict Boundary</h2>\n<p>두 가지 프로그래밍 스타일을 비교해서, 하나만 사용하라는 말이 아닙니다. 명령형 프로그램을 상징형 프로그램처럼 만들거나 그 반대도 가능합니다. 예를 들면, Python으로 JIT (just-in-time) 컴파일러를 작성하여 명령형 Python 프로그램을 컴파일 할 수 있습니다. 하지만 두 가지 아키텍쳐를 이해하는 것은 수 많은 딥러닝 라이브러리의 추상화와 그 차이를 이해하는데 도움이 됩니다. <strong>결국, 우리는 프로그래밍 스타일 간에 명확한 경계선이 없다고 결론을 내릴 수 있습니다.</strong></p>\n<br>","excerpt":"TensorFlow 1.5 버전부터 Eager Execution…"}}}},{"node":{"title":"제플린 노트북 자동 실행 스크립트 만들기","id":"ae1bd687-b1b2-535f-a826-8e600505d3df","slug":"zeppelin-bootstrap","publishDate":"September 13, 2017","heroImage":{"id":"dab22ea8-d54d-52a6-852a-278ba3b19a2b","title":"cover-dataengineering","fluid":{"aspectRatio":1.499531396438613,"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50 1600w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50&fm=webp 1600w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"a29a76e7-6151-5f0f-8eb3-1c75a8fb8f5a","childMarkdownRemark":{"id":"1107c2c0-8dc1-5eca-a038-7c07e481d047","timeToRead":1,"html":"<p>제플린 노트북을 사용하다보면 가끔 제플린 어플리케이션을 재시작해야 하는 경우가 있습니다.\n이 때, view 또는 udf 등록을 위해 처음 실행시켜야 하는 노트북이 있다면 참 번거롭습니다.\n하지만 <strong>Zeppelin Notebook API</strong> 사용한다면 이를 쉽게 자동화 할 수 있습니다.</p>\n<br>\n<h2 id=\"zeppelin-notebook-api\" style=\"position:relative;\"><a href=\"#zeppelin-notebook-api\" aria-label=\"zeppelin notebook api permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Zeppelin Notebook API</h2>\n<p>제플린은 노트북 자동실행을 위한 REST API를 제공합니다.\n하지만 제플린에 인증이 걸려있다면, 인증을 거쳐야만 API를 사용할 수 있습니다.\n따라서, 먼저 curl로 세션 값을 받고 해당 노트북 아이디를 호출하시면 됩니다.</p>\n<p>노트북 아이디는 해당 노트 URL의 가장 마지막 값 입니다. (ex 2AZPHY918)\n아래의 스크립트는 아이디가 user, 패스워드가 1234인 경우를 예시로 들었습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">#!/bin/sh\nsudo /usr/lib/zeppelin/bin/zeppelin-daemon.sh stop\nsleep 3\nsudo /usr/lib/zeppelin/bin/zeppelin-daemon.sh start\n\nsleep 15\n\nSESSION=&quot;`curl -i --data &#39;userName=user&amp;password=1234)&#39; -X POST http://zeppelin-url.com:8890/api/login | grep &#39;Set-Cookie: JSESSIONID=&#39; | cut -d &#39;:&#39; -f2 |  tail -1 | cut -d &#39;;&#39; -f1`&quot;\necho $SESSION\ncurl -i -b ${SESSION} -X POST http://zeppelin-url.com:8890/api/notebook/job/NOTEBOOK_ID</code></pre></div>\n<p>Notebook API를 활용하면 노트북 실행 뿐만 아니라, Cron이나 노트북 권한 설정도 자동화할 수 있습니다.\n자세한 내용은 아래의 공식문서에서 확인하실 수 있습니다.</p>\n<br>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h2>\n<ul>\n<li><a href=\"https://zeppelin.apache.org/docs/0.7.3/rest-api/rest-notebook.html\">https://zeppelin.apache.org/docs/0.7.3/rest-api/rest-notebook.html</a></li>\n</ul>\n<br>","excerpt":"제플린 노트북을 사용하다보면 가끔 제플린 어플리케이션을 재시작해야 하는 경우가 있습니다.\n이 때, view 또는 udf…"}}}},{"node":{"title":"AWS EMR에서 S3 사용 시 주의사항","id":"5a25926b-645a-574b-ba52-5085574c8619","slug":"aws-emr-s3-spark","publishDate":"September 09, 2017","heroImage":{"id":"dab22ea8-d54d-52a6-852a-278ba3b19a2b","title":"cover-dataengineering","fluid":{"aspectRatio":1.499531396438613,"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50 1600w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50&fm=webp 1600w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"0b9a2387-1599-568e-a5c4-5841d21e528b","childMarkdownRemark":{"id":"2f4a55d7-9be9-53d7-a09a-4da4630a4cde","timeToRead":3,"html":"<p>AWS EMR에서 Spark을 사용하는 경우, S3를 저장소로 사용하는 경우가 많습니다.\n이때 주의해야 할 사항들을 정리해보았습니다.</p>\n<ul>\n<li><strong>최근 수정사항</strong> : 해당 이슈는 EMR 최신 버전에서 대부분 해결되었습니다.</li>\n<li>자세한 내용은 <a href=\"https://aws.amazon.com/ko/blogs/korea/improve-apache-spark-write-performance-on-apache-parquet-formats-with-the-emrfs-s3-optimized-committer/\">Parquet 형식의 EMRFS S3 최적화 커미터를 통한 Apache Spark 쓰기 성능 개선하기</a> 에서 확인하시기 바랍니다.</li>\n</ul>\n<br>\n<h2 id=\"aws-emr-spark-그리고-s3\" style=\"position:relative;\"><a href=\"#aws-emr-spark-%EA%B7%B8%EB%A6%AC%EA%B3%A0-s3\" aria-label=\"aws emr spark 그리고 s3 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>AWS EMR, Spark 그리고 S3</h2>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1FWjProAZAa51cBO6nkLEq1kRhpeG8uhQ\"></p>\n<br>\n<p>Daily로 돌려야 하는 ETL 작업의 경우 위와 같이 간단한 아키텍쳐로 구성하는 경우가 많습니다.\n대부분의 경우 저장소로 S3를 적극 활용하게 됩니다.\n최초 입수되는 로그를 저장하기도 하고, Transformation 작업 이후 중간 또는 최종 데이터로 저장하기도 합니다.</p>\n<br>\n<h2 id=\"문제-상황\" style=\"position:relative;\"><a href=\"#%EB%AC%B8%EC%A0%9C-%EC%83%81%ED%99%A9\" aria-label=\"문제 상황 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>문제 상황</h2>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">java.io.IOException: Connection reset by peer\nERROR ContextCleaner: Error cleaning broadcast 5</code></pre></div>\n<p>최근 Spark RDD 코드를 DataFrame으로 리팩토링 하던 중에 위와 같은 오류를 겪었습니다.\n일별 로그를 불러와서 전처리하고 다시 저장하는데 s3 write 부분에서 갑자기 Executor의 Connection이 끊기는 문제였습니다.</p>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1jKeOxJk_fjDCXfmkLr_GVxTiMAY5IKAQ\"></p>\n<br>\n<p>Ganglia 모니터링 결과를 보면 중간에 약 15분의 공백이 있는데,\n이 부분이 Connection이 중간에 끊기고 다시 뜰 때까지 걸리는 시간입니다.</p>\n<br>\n<h2 id=\"s3n-s3a-s3\" style=\"position:relative;\"><a href=\"#s3n-s3a-s3\" aria-label=\"s3n s3a s3 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>S3N, S3A, S3</h2>\n<p>먼저 S3는 File System이 아닌 <strong>Object Storage</strong> 라는 점을 알고 계셔야 합니다.\n따라서, S3에 분산저장하는 경우, 우리는 Hadoop 클라이언트를 거쳐 저장하게 됩니다.\nHadoop은 <code class=\"language-text\">S3N, S3A, S3</code> 이렇게 세 가지 시스템 클라이언트를 제공합니다. 각 클라이언트는 URI 스키마를 통해 접근할 수 있습니다.</p>\n<ul>\n<li><strong>S3N (s3n://)</strong> : S3N은 S3에 일반 파일을 읽고 쓰는 기본 파일 시스템입니다. S3N은 안정적이며 널리 사용되고 있지만 현재는 업데이트가 중단되었습니다. S3N의 단점은 파일 엑세스가 한번에 5GB로 제한되어 있다는 점입니다.</li>\n<li><strong>S3A (s3a://)</strong> : S3A는 S3N을 개선한 다음 버전의 파일 시스템입니다. S3A는 Amazon의 라이브러리를 사용하여 S3와 상호 작용합니다. S3A는 5GB 이상의 파일 액세스를 지원하며 성능이 많이 향상되었습니다.</li>\n<li><strong>S3 (s3://)</strong> : S3는 Hadoop 0.10 버전부터 나온 블록 기반의 S3 파일 시스템 입니다. 따라서 파일이 HDFS에 있는 것과 같이 블록으로 저장됩니다.</li>\n</ul>\n<p>EMR은 EMRFS 라는 파일 시스템이 별도로 존재합니다.\nEMR의 S3 파일 시스템과 Hadoop에서의 S3 파일 시스템은 서로 다르기 때문에 항상 주의하셔야 합니다.\nEMR의 경우 <strong>s3</strong> 로 사용하는 것을 권장하고 있습니다. 반면에 s3a의 경우 EMRFS와 호환되지 않는다고 합니다.\n물론 실행 될 때도 있지만 위와 같은 오류가 발생할 수도 있습니다.</p>\n<br>\n<h2 id=\"parquet-저장-성능-개선하기\" style=\"position:relative;\"><a href=\"#parquet-%EC%A0%80%EC%9E%A5-%EC%84%B1%EB%8A%A5-%EA%B0%9C%EC%84%A0%ED%95%98%EA%B8%B0\" aria-label=\"parquet 저장 성능 개선하기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Parquet 저장 성능 개선하기</h2>\n<p>위의 오류는 URI를 s3로 수정해서 해결할 수 있었습니다.\n하지만 S3에 parquet로 저장하는 속도가 너무 느려 이 부분을 개선해보기로 했습니다.</p>\n<p>먼저 Spark에는 Parquet 빌드 속도를 개선하기 위해 <code class=\"language-text\">DirectParquetOutputCommitter</code>라는 기능이 있었습니다.\n하지만, S3에 저장할 때 이 기능을 사용하는 경우 데이터 유실이 발생할 수 있었습니다.\n<a href=\"https://issues.apache.org/jira/browse/SPARK-10063\">SPARK-10063 JIRA 티켓 참고</a></p>\n<p>이러한 이유로 Spark 2.0 버전부터 이 옵션은 사라졌습니다. 그러나, 성능 개선이 필요했기 때문에 Spark 사용자들은 대안을 요구했습니다.\n본래의 FileCommiter가 느린 이유는 rename 연산 때문이었습니다.\n실제 파일 시스템(HDFS)에서 rename 연산은 대상 파일 시스템의 임시 디렉토리로 출력 한 다음, 디렉토리의 이름을 커밋하는 방식으로 O(1)이 소요됩니다.\n하지만 Object Storage에 저장하는 경우, 데이터 사이즈만큼 O(N)이 소요됩니다.</p>\n<p>이 문제는 s3guard와 s3a의 도움으로 해결되었습니다.\ngetFileStatus()에서의 S3 HTTP 콜을 생략하고 dynamo metadata 저장 등을 통해 해결했다는데 자세한 내용은 <a href=\"https://issues.apache.org/jira/browse/MAPREDUCE-4815\">MAPREDUCE-4815 JIRA 티켓</a>을 보시는게 나을 듯 합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version 2\nspark.speculation False</code></pre></div>\n<p>적용하는 방법은 위의 Spark property 옵션을 추가해주시면 됩니다. Spark 2.1, Hadoop 2.7.2 버전 이상부터 사용가능 합니다.\n하지만 Spark 문서에도 나와있듯이 아직 failure에 대한 보장이 떨어집니다.\n따라서 먼저 로컬 HDFS에 임시저장 후 distcp 명령어를 사용하여 S3로 저장해주시면 됩니다.\nHadoop 2.8 버전부터는 s3guard가 기본으로 들어가기 때문에 안정화 될 것 이라고 합니다.</p>\n<p>결과는 로그 1억 건 기준 <strong>약 10배</strong> 의 성능 개선을 확인할 수 있었습니다.\n두서없이 정리하다보니 좀 글이 복잡해졌네요. 결론은 '옵션을 추가하자' 입니다.</p>\n<br>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h2>\n<ul>\n<li><a href=\"https://github.com/steveloughran/hadoop/blob/s3guard/HADOOP-13786-committer/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/s3a_committer_architecture.md\">S3A Commiter가 아키텍쳐 및 구현 세부사항에 대하여 정리한 글</a></li>\n<li><a href=\"https://aws.amazon.com/ko/premiumsupport/knowledge-center/emr-file-system-s3/\">AWS 공식 문서에서 정리한 글 : S3N, S3A, S3</a></li>\n</ul>\n<br>","excerpt":"AWS EMR에서 Spark을 사용하는 경우, S…"}}}},{"node":{"title":"Spark의 Shuffling 이해하기","id":"ca419fc7-c4a8-5527-8671-c1e508769568","slug":"spark-shuffling","publishDate":"August 25, 2017","heroImage":{"id":"dab22ea8-d54d-52a6-852a-278ba3b19a2b","title":"cover-dataengineering","fluid":{"aspectRatio":1.499531396438613,"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50 1600w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50&fm=webp 1600w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"2a8f6e13-331a-57ae-9646-63d4ef59b0a6","childMarkdownRemark":{"id":"232124a0-9bf3-5161-8345-9ff38e52cee0","timeToRead":2,"html":"<p>효율적인 Spark Application을 개발하기 위해 <strong>Shuffling</strong> 은 상당히 중요한 개념입니다.\n이에 대해 간단히 정리해보았습니다.</p>\n<br>\n<h2 id=\"spark-architecture-shuffle\" style=\"position:relative;\"><a href=\"#spark-architecture-shuffle\" aria-label=\"spark architecture shuffle permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark Architecture: Shuffle</h2>\n<p><img src=\"/assets/images/shuffle.png\"></p>\n<p>Shuffle을 설명하기 전에 한 가지 예시를 들어보겠습니다.\n테이블에 전화 통화 기록 목록이 있고 매일 발생한 통화량을 계산한다고 가정 해보겠습니다.\n“날짜”를 키로 설정하고 각 레코드에 대해 값으로 “1”을 지정한 다음, 각 키의 값을 합산하여 결과 값을 계산할 수 있을 것 입니다.</p>\n<p>만일 데이터가 여러 클러스터에 저장되어 있다면 어떻게 해야 동일한 키의 값을 합산할 수 있을까요?\n이를 위한 유일한 방법은 같은 키의 모든 값을 동일한 시스템에 두는 것입니다. 그런 다음 이 값들을 합치면 됩니다.</p>\n<br>\n<h2 id=\"narrow-and-wide-transformation\" style=\"position:relative;\"><a href=\"#narrow-and-wide-transformation\" aria-label=\"narrow and wide transformation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Narrow and Wide Transformation</h2>\n<p><img src=\"/assets/images/narrow_and_wide.png\"></p>\n<p>몇 가지 사례를 통해 더 자세히 알아보겠습니다.\n만일 데이터가 이미 키 값으로 파티셔닝 되어 있고 키 값에 대해 변화를 주고 싶다면, 좌측의 그림처럼 수행하게 됩니다.\n<code class=\"language-text\">filter(), sample(), map(), flatMap()</code> 등의 transformation이 이에 해당하며, 이 경우 Shuffle이 필요 없습니다.\n이를 <strong>Narrow Transformation</strong> 이라고 합니다.</p>\n<p>반면, 서로 다른 파티션으로부터 특정한 값을 기준으로 추출하고 싶은 경우, 그 값을 기준으로 Shuffle이 발생하게 됩니다.\n<code class=\"language-text\">groupByKey(), reduceByKey()</code> 등이 이에 해당하며, 이를 <strong>Wide Transformation</strong> 이라고 합니다.</p>\n<br>\n<h2 id=\"shuffled-hashjoin\" style=\"position:relative;\"><a href=\"#shuffled-hashjoin\" aria-label=\"shuffled hashjoin permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Shuffled HashJoin</h2>\n<p><img src=\"/assets/images/shuffle_join.png\"></p>\n<p>두 개의 테이블을 <code class=\"language-text\">Join</code> 할 때에도 Shuffle 이 발생할 수 있습니다.\n위의 예시 처럼 두 테이블에서 키 값을 기준으로 Join 하게 되면, 동일한 키를 가진 데이터가 동일한 파티션으로 이동합니다.</p>\n<p>하지만 이 때, 셔플 되는 데이터의 양이 성능에 영향을 미칠 수 있습니다.\n만일 C의 데이터의 크기가 A보다 훨씬 크다면, C에 대한 작업으로 인해 전체의 수행시간이 오래 걸리게 될 것 입니다.</p>\n<br>\n<h2 id=\"broadcast-hashjoin\" style=\"position:relative;\"><a href=\"#broadcast-hashjoin\" aria-label=\"broadcast hashjoin permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Broadcast HashJoin</h2>\n<p><img src=\"/assets/images/broadcast_join.png\"></p>\n<p>이를 개선하기 위해 Spark에서는 <strong>Broadcast Join</strong> 을 제공합니다.\n이 경우 RDD 중 하나가 모든 파티션으로 브로드 캐스팅되며 복사됩니다.\n만일 RDD 중 하나가 다른 것에 비해 상당히 작다면 큰 RDD가 전혀 셔플 할 필요가 없습니다.\n작은 RDD 만 모든 작업자 서버에 복사해야 하므로 Broadcast Join은 전체적으로 네트워크 트래픽을 줄여주는 효과가 있습니다.</p>\n<p>Spark 1.2에서는 <code class=\"language-text\">spark.sql.autoBroadcastJoinThreshold</code> 값을 설정해주어야 했지만,\n2.0 이후 버전의 경우 Spark SQL이 알아서 최적화 잘 해줍니다.</p>\n<br>\n<h2 id=\"spark-shuffle-properties\" style=\"position:relative;\"><a href=\"#spark-shuffle-properties\" aria-label=\"spark shuffle properties permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark Shuffle Properties</h2>\n<ul>\n<li><code class=\"language-text\">spark.shuffle.compress</code>: 엔진이 shuffle 출력을 압축할지 여부를 지정</li>\n<li><code class=\"language-text\">spark.shuffle.spill.compress</code>: 중간 shuffle spill 파일을 압축할지 여부를 지정</li>\n</ul>\n<p>Shuffle에는 위의 두 가지 중요한 Spark Property 가 있습니다.</p>\n<p>둘 다 기본적으로 값이 “true”이며, <code class=\"language-text\">spark.io.compression.codec</code> 압축 코덱을 기본으로합니다.\n그리고 위에서 설명한 것처럼 Spark에는 여러 가지 셔플 구현이 있습니다.\n특정 구현에서 사용되는 Shuffle은 <code class=\"language-text\">spark.shuffle.manager</code> 값에 의해 결정됩니다.\n가능한 옵션은 <strong>hash, sort, tungsten-sort</strong> 이며, “sort” 옵션은 기본적으로 Spark 1.2.0부터 시작합니다.</p>\n<p>이외에도 Spark Shuffle 관련된 Property는 아래의 공식문서에서 확인하실 수 있습니다.\n<a href=\"https://spark.apache.org/docs/latest/configuration.html#shuffle-behavior\">https://spark.apache.org/docs/latest/configuration.html#shuffle-behavior</a></p>\n<br>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h2>\n<ul>\n<li><a href=\"https://0x0fff.com/spark-architecture-shuffle\">https://0x0fff.com/spark-architecture-shuffle</a></li>\n<li><a href=\"https://www.slideshare.net/databricks/strata-sj-everyday-im-shuffling-tips-for-writing-better-spark-programs\">https://www.slideshare.net/databricks/strata-sj-everyday-im-shuffling-tips-for-writing-better-spark-programs</a></li>\n</ul>\n<br>","excerpt":"효율적인 Spark Application을 개발하기 위해 Shuffling…"}}}}]}},"pageContext":{"basePath":"","paginationPath":"","pageNumber":5,"humanPageNumber":6,"skip":31,"limit":6,"numberOfPages":15,"previousPagePath":"/5","nextPagePath":"/7"}}}