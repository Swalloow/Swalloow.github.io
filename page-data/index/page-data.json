{"componentChunkName":"component---src-templates-posts-js","path":"/","result":{"data":{"allContentfulPost":{"edges":[{"node":{"title":"Spark on Kubernetes: 커스텀 스케줄러","id":"1dfb7a05-4413-5c9f-8adb-d57b59ed0b1e","slug":"spark-on-kubernetes-scheduler","publishDate":"June 08, 2023","heroImage":{"id":"dab22ea8-d54d-52a6-852a-278ba3b19a2b","title":"cover-dataengineering","fluid":{"aspectRatio":1.499531396438613,"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50 1600w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50&fm=webp 1600w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"b20ebf01-afe9-56d6-840c-ab90a0fa08b2","childMarkdownRemark":{"id":"c17d3327-d125-5078-a762-aa617298a0a5","timeToRead":3,"html":"<p>Spark 3.4 버전부터 Customized K8S Scheduler 기능이 GA 되었습니다 👏🏻<br>\n그래서 오늘은 커스텀 스케줄러가 왜 필요하고 어떻게 적용할 수 있는지 정리해보려고 합니다.</p>\n<p><br><br></p>\n<h2 id=\"spark-kubernetes-scheduling\" style=\"position:relative;\"><a href=\"#spark-kubernetes-scheduling\" aria-label=\"spark kubernetes scheduling permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark Kubernetes Scheduling</h2>\n<p><img src=\"https://drive.google.com/uc?export=view&#x26;id=1Kwomj5BeazAYa72lEB4PQDcKUe46BizX\" alt=\"spark-k8s\"></p>\n<p>쿠버네티스 환경에서 spark-submit을 실행하면 pod가 실행되는 순서는 다음과 같습니다.</p>\n<ul>\n<li>spark-submit 명령어 실행</li>\n<li>Kube API를 통해 driver pod 생성</li>\n<li>driver pod → API Server에 executor 생성 요청</li>\n<li>Kube API를 통해 executor pod 생성</li>\n</ul>\n<p>위와 같이 driver가 executor를 관리함에 따라 동적으로 리소스를 확장할 수 있지만\ndriver가 생성되기 전까지 전체 executor에 필요한 리소스를 알 수 없다는 단점이 있습니다.\n이러한 이유로 클러스터 내에 리소스가 고갈된 상황에서 성능 문제가 발생할 수 있습니다.</p>\n<br>\n<p><strong>클러스터 내에 리소스가 고갈된 경우</strong><br>\n<img src=\"https://drive.google.com/uc?export=view&#x26;id=1pNKR_jMSjpHgJakbDiyJIETkKBoftIXe\" alt=\"allo1\"></p>\n<p>클러스터의 리소스 풀이 요청 받은 리소스보다 부족한 상황이라고 가정해보겠습니다.\n위 그림에서 녹색은 실제로 노드에 할당되어 running 중인 pod, 빨간색은 리소스가 부족으로 인해 pending 상태의 pod 입니다.</p>\n<p>각 앱은 리소스 경쟁에 의해 driver와 executor 1개씩 정상적으로 생성되어 3개의 앱이 실행 중인 상태입니다. 하지만 3개의 앱은 executor 리소스를 확보하지 못했기 때문에 작업을 완료할 수 없습니다. EKS 환경이라면 노드 리소스를 확보하더라도 VPC IP 고갈 문제로 인해 이러한 상황을 충분히 마주칠 수 있습니다.</p>\n<br>\n<p><img src=\"https://drive.google.com/uc?export=view&#x26;id=1ErqVM9uKqc8WZO0-BNIRlBBA5DRsNeYS\" alt=\"allo2\"></p>\n<p>위와 같이 리소스 내에서 가능한 앱이 정상적으로 실행, 종료되고 나머지는 대기하는게 더 효율적이라고 볼 수 있습니다.\n쿠버네티스에서는 기본 스케줄러가 배치 작업에 최적화된 형태가 아니기 때문에 위와 같은 문제가 발생할 수 있습니다. 이를 해결하기 위해 <code class=\"language-text\">kube-batch</code>, <code class=\"language-text\">volcano</code>, <code class=\"language-text\">yunikorn</code> 등의 커스텀 배치 스케줄러가 개발되었습니다.</p>\n<p><br><br></p>\n<h2 id=\"spark-app-aware-scheduling\" style=\"position:relative;\"><a href=\"#spark-app-aware-scheduling\" aria-label=\"spark app aware scheduling permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark App-aware Scheduling</h2>\n<p>기본 스케줄러는 <code class=\"language-text\">filtering</code>, <code class=\"language-text\">scoring</code> 과정을 거쳐 pod가 실행될 최적의 노드를 찾습니다. 이 때 스케줄 단위는 pod 입니다. 반면 대용량 배치 작업에서는 동시에 수 백개의 pod가 생성되기도 합니다. 또한 동시에 여러 작업이 실행되기 때문에 우선순위, 조직 별 리소스 제한 등을 고려해서 안정적으로 작업을 마치기 위한 대기열이 필요합니다.</p>\n<p>커스텀 배치 스케줄러에서는 이를 해결하기 위해 <strong>앱 단위로 스케줄을 결정하는 App-aware 방식을 사용</strong>합니다. 뿐만 아니라 대용량 배치 작업을 위해 Job Ordering, Hierarchy Resource Queue, Node Sorting 단계에서 다양한 스케줄링 알고리즘을 지원합니다.</p>\n<p><strong>Bin Packing</strong><br>\n<img src=\"https://drive.google.com/uc?export=view&#x26;id=10yt7Xf1w4pWqLf8Z0cYJQbA5BzUSX_--\" alt=\"binpack\"></p>\n<p>Node Sorting 단계를 예시로 들어보겠습니다.\n기본 스케줄러에서 driver, executor pod가 여러 노드에 고르게 분산하면 앱은 네트워크 지연, 셔플 시 원격에서 데이터를 가져와야 하는 상황이 발생합니다.</p>\n<p>이 때 테트리스처럼 <strong>Bin Packing 방식을 적용한다면 어플리케이션을 최대한 가깝게 할당</strong>할 수 있습니다. 클라우드 환경에서 이를 적용하면 노드 scale-in도 원활하게 수행할 수 있습니다.</p>\n<p><br><br></p>\n<h2 id=\"spark-gang-scheduling\" style=\"position:relative;\"><a href=\"#spark-gang-scheduling\" aria-label=\"spark gang scheduling permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark Gang Scheduling</h2>\n<p>앞서 리소스가 고갈된 상황의 경우, Gang Scheduling을 사용한다면 안정적으로 작업을 실행할 수 있습니다. 노드가 0대인 상황에서 앱이 제출되었다고 가정해보겠습니다.</p>\n<p><img src=\"https://drive.google.com/uc?export=view&#x26;id=1zT9Y-cjE65n-H-WgVDefQ02oI0ZVCjWH\" alt=\"gang1\"></p>\n<p>위의 그림은 기본 스케줄러를 적용했을 때 모습입니다.<br>\n필요한 최소 리소스가 미리 정해져있으나 노드 생성까지 대기 시간이 발생합니다.</p>\n<ul>\n<li>driver 리소스 요청 → 1대 생성</li>\n<li>executor 리소스 요청 → 2대 생성</li>\n</ul>\n<p><img src=\"https://drive.google.com/uc?export=view&#x26;id=1kGf51ivLYDgS8ZsuAIYEdNAAN643ze2r\" alt=\"gang2\"></p>\n<p>위의 그림은 gang 스케줄링을 적용했을 때 모습입니다.<br>\n한번에 필요한 리소스를 확보하여 대기 시간을 최소화합니다.</p>\n<ul>\n<li>driver 리소스 요청 → placeholder 리소스 요청 → 노드 3대 생성</li>\n<li>driver, executor pod 즉시 할당</li>\n</ul>\n<p>여기에서 placeholder pod은 아무 동작도 안하지만 미리 리소스를 확보하기 위해 존재하는 dummy pod 입니다. 만약 리소스를 확보하지 못하는 상황이라면 앱은 대기합니다.\n<strong>Gang Scheduling은 FIFO 큐와 함께 실행하여 리소스 경쟁으로 인한 교착상태에 빠지지 않도록</strong> 할 수 있습니다. </p>\n<p><img src=\"https://drive.google.com/uc?export=view&#x26;id=1NwRgb5nl-b4hecA_PYxuNC59M_5irQbq\" alt=\"kubemark\"></p>\n<p>또한 동시 실행 Pod가 많을 수록 스케줄링 성능 향상을 기대할 수 있습니다. 위 그림은 Yunikorn에서 kubemark를 통해 벤치마크한 결과입니다. 회사 환경에서 spark 작업 시간을 기준으로 테스트했을 때도 성능 향상을 확인할 수 있었습니다.</p>\n<p>다음 글에서는 Spark 3.4 버전에서 공식적으로 지원하는 Volcano, Yunikorn에 대해 이어서 정리해보겠습니다.</p>\n<br>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h2>\n<ul>\n<li><a href=\"https://yunikorn.apache.org/docs/\">https://yunikorn.apache.org/docs/</a></li>\n<li><a href=\"https://blog.cloudera.com/spark-on-kubernetes-gang-scheduling-with-yunikorn/\">https://blog.cloudera.com/spark-on-kubernetes-gang-scheduling-with-yunikorn/</a></li>\n</ul>","excerpt":"Spark 3.4 버전부터 Customized K8S Scheduler 기능이 GA…"}}}},{"node":{"title":"베를린에서 2개월 살아남기","id":"4e00984a-c0d1-5aab-8955-f5a875b99cd9","slug":"berlin","publishDate":"May 10, 2023","heroImage":{"id":"1faaada3-e12b-5548-8532-08b7c04dc7eb","title":"cover-personal","fluid":{"aspectRatio":1.694915254237288,"src":"//images.ctfassets.net/tushy4jlcik7/3ltdJp06NzCExAWz9OF8Ak/d8ca530c80e7c79a7bd7e4c396c0ae00/cover_personal.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/3ltdJp06NzCExAWz9OF8Ak/d8ca530c80e7c79a7bd7e4c396c0ae00/cover_personal.jpg?w=450&h=266&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/3ltdJp06NzCExAWz9OF8Ak/d8ca530c80e7c79a7bd7e4c396c0ae00/cover_personal.jpg?w=900&h=531&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/3ltdJp06NzCExAWz9OF8Ak/d8ca530c80e7c79a7bd7e4c396c0ae00/cover_personal.jpg?w=1400&h=826&q=50 1400w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/3ltdJp06NzCExAWz9OF8Ak/d8ca530c80e7c79a7bd7e4c396c0ae00/cover_personal.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/3ltdJp06NzCExAWz9OF8Ak/d8ca530c80e7c79a7bd7e4c396c0ae00/cover_personal.jpg?w=450&h=266&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/3ltdJp06NzCExAWz9OF8Ak/d8ca530c80e7c79a7bd7e4c396c0ae00/cover_personal.jpg?w=900&h=531&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/3ltdJp06NzCExAWz9OF8Ak/d8ca530c80e7c79a7bd7e4c396c0ae00/cover_personal.jpg?w=1400&h=826&q=50&fm=webp 1400w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/3ltdJp06NzCExAWz9OF8Ak/d8ca530c80e7c79a7bd7e4c396c0ae00/cover_personal.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"406962d2-e346-525d-8ab4-b286caa135e8","childMarkdownRemark":{"id":"c16b2fce-7b50-5f1b-a64e-7a47e8efddc5","timeToRead":3,"html":"<p>우연히 회사에서 좋은 기회를 얻게 되어 독일에서 2개월 근무한 후기<br>\n베를린 생활부터 유럽의 개발 문화까지 그 동안 겪은 경험을 정리해보려 합니다.</p>\n<br>\n<h2 id=\"베를린-아파트\" style=\"position:relative;\"><a href=\"#%EB%B2%A0%EB%A5%BC%EB%A6%B0-%EC%95%84%ED%8C%8C%ED%8A%B8\" aria-label=\"베를린 아파트 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>베를린 아파트</h2>\n<p>숙소가 정말 평이 좋고 실제로 시설도 좋은 아파트인데 엘리베이터가 없었다.\n여기는 그라운드 개념이 있어서 5층이 한국으로 치면 6층인데 다들 아무렇지 않게 걸어올라간다.\n빨래, 건조를 하려면 6층을 왕복 3번 다녀야 하는데 이게 제일 힘들었다.</p>\n<p>그리고 모든 문을 열쇠로 열어야 하는데 이게 정말 난감하다.\n최근에 열쇠를 들고 다녀본 기억이 없다보니 두고 다닐 수 있는데\n열쇠를 두고 오면 한화 약 20만원 정도의 벌금을 내야한다.\n열쇠 분실로 인한 사고 방지를 위해 문을 교체해버리기 때문에 저런 비용이 발생한다.\n개인적인 생각으로는 그냥 도어락을 도입하는게 나아보였다 🙏🏻</p>\n<br>\n<h2 id=\"베를린-교통과-생활\" style=\"position:relative;\"><a href=\"#%EB%B2%A0%EB%A5%BC%EB%A6%B0-%EA%B5%90%ED%86%B5%EA%B3%BC-%EC%83%9D%ED%99%9C\" aria-label=\"베를린 교통과 생활 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>베를린 교통과 생활</h2>\n<p>처음 도착하자마자 놀란건 일요일에 모든 마트, 식료품가게가 문을 닫는다는 것이다.\n만약을 위해 베를린 전체에서 세 군데 정도 대형마트만 문을 연다.\n결국 생활용품을 사기 위해 베를린 중앙역까지 갔는데 뉴스에서만 보던 그림을 볼 수 있었다.\n재난상황마냥 줄이 끝까지 이어져있고 마트 내부 물품은 다 털려있었다.\n그래도 독일은 국가가 통제하기 때문에 마트 물가가 정말 저렴하다.</p>\n<p>베를린 대중교통으로는 트램, 지하철, 버스를 제일 많이 탄다.\n종일권을 끊으면 모든 교통수단을 무제한으로 탈 수 있다.\n근데 이 보다 더 좋은 교통수단은 자전거다.\n자전거 도로가 너무 잘되어 있어서 대중교통 이용하는 것보다 시간이 빠를 때가 많다.\n도시 간 이동으로는 flixbus와 ICE 고속열차를 많이 이용한다.\n독일의 고속철은 워낙 악명 높아서 취소되는 일이 빈번하다 들었는데\n역시나 당일 오전 출발 5분 전에 ICE 열차 취소를 겪었다.</p>\n<p><img src=\"https://drive.google.com/uc?export=view&#x26;id=1bQ-fcJEpPBuqhW7EMzENYWS_fRnrEbQO\" alt=\"berlin1\"></p>\n<p>오전 ICE 전체 지연으로 베를린 중앙역이 전쟁터가 된 모습, 환불을 받기 위해 전부 줄서있는 모습</p>\n<p>6시간 연착이라고 당당하게 설명하는데 사람들이 너무 당연하게 받아들여서 당황했다.<br>\n근데 더 놀라운 것은 환불 절차다.\n한국은 유럽과 같은 은행코드를 사용하지 않아 앱에서 환불 받을 수 없었다.\n방법은 DB 본사에 우편으로 직접 적어서 보내는 것 밖에 없다. 당연히 이메일도 불가능했다.</p>\n<br>\n<h2 id=\"베를린-음식\" style=\"position:relative;\"><a href=\"#%EB%B2%A0%EB%A5%BC%EB%A6%B0-%EC%9D%8C%EC%8B%9D\" aria-label=\"베를린 음식 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>베를린 음식</h2>\n<p><img src=\"https://drive.google.com/uc?export=view&#x26;id=1eCZBp1oUw_pM1JPmDB5lNDunKB_VrABJ\" alt=\"berlin2\"></p>\n<p>베를린은 너무 다양한 인종이 살고 있어 전 세계 음식을 다 먹어볼 수 있다.<br>\n그 중에서 독일 음식은 흰색 소세지와 커리부어스트만 먹어보면 된다. 나머지는 별로였다.\n햄버거의 어원이 함부르크에서 나온 만큼 수제버거도 맛있는 곳이 많다.</p>\n<p><img src=\"https://drive.google.com/uc?export=view&#x26;id=1-be935C_pyhBFXHbl2jPWYy1KPENzfJs\" alt=\"berlin3\"></p>\n<p>보통 해외에서 새벽에 다니면 위험한 경우가 많은데 베를린은 서울보다 더 밤새도록 노는 도시라 정말 안전하다.<br>\n오이스터 바에 가면 비싼 돈을 주고 굴을 먹을 수 있는데 비린내가 안나고 우유 맛이 난다.</p>\n<p><img src=\"https://drive.google.com/uc?export=view&#x26;id=19zETeYM4gEsYYv72beliY9ktshklSU27\" alt=\"berlin4\"></p>\n<p>때마침 베를린 빛 축제가 진행 중이어서 관광지에서 다양한 야경을 볼 수 있었다.</p>\n<br>\n<h2 id=\"베를린-개발-문화-스타트업\" style=\"position:relative;\"><a href=\"#%EB%B2%A0%EB%A5%BC%EB%A6%B0-%EA%B0%9C%EB%B0%9C-%EB%AC%B8%ED%99%94-%EC%8A%A4%ED%83%80%ED%8A%B8%EC%97%85\" aria-label=\"베를린 개발 문화 스타트업 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>베를린 개발 문화, 스타트업</h2>\n<p>유럽도 일하는 방식은 비슷했으나 한국과 비교했을 때 일을 디테일하게 한다고 느꼈다.\n예를 들면 RFC 문서를 정말 자세히 작성하고 오픈소스와 같은 플랫폼 운영 방식을 가지고 있었다.</p>\n<p>그리고 정서 상 한국은 정말 겸손한 반면 여기는 잘한 일이 생기면 자랑하고 모두가 축하하는 분위기였다.\n해외 감성으로 네트워킹, 파티도 자주 열린다.\n독일은 아프면 바로 병가를 15일까지 낼 수 있고 휴가가 25일이다.</p>\n<p>금요일에는 모두 일찍 퇴근하고 앞에서 맥주를 마신다.\n모든 과정을 경험해보니 절대 한국만큼의 개발 속도가 나올 수는 없겠다는 생각이 들었지만\n반대로 유럽에서 여유롭게 사는 법을 배운 것 같다.</p>\n<p>데이터 분야에서는 특별한 차이가 있는데 개인정보에 대한 사람들의 인식이다.\n한국에 있을 때도 많이 들어봤던 GDPR이라는 규정에 대해서도 알게 되었다.\n대부분 사용자들은 개인정보를 절대 서비스에 넘기려하지 않는다.</p>\n<p>그러다보니 데이터 기반의 서비스를 만드는 사람들은 정말 난감할 때가 많은데 개인화 추천이 가장 대표적이다.\n일단 사용자를 식별해야 개인화를 할텐데 여기는 개인을 정의하는 것부터가 어려운 문제다.</p>\n<p><img src=\"https://drive.google.com/uc?export=view&#x26;id=15fC9nRrxwNR9pGZSp7h3UnoUwUzSXPvD\" alt=\"berlin5\"></p>\n<p>베를린의 옛 건물을 내부만 리모델링해서 사용한 스타트업 공유 오피스도 방문했었다.<br>\n자동차 회사가 많은 나라답게 다양한 모빌리티 스타트업을 만날 수 있었다.<br>\n베를린에는 유럽 내의 스타트업이 많은 편인데 이 도시가 유럽에서 가장 글로벌하기 때문이다.<br>\n시골로 내려가면 기술에 대한 거부반응을 가진 사람이 많은 반면 베를린은 해외에서 온 이민자가 많다.\n그래서 독일어가 있음에도 영어를 정말 많이 사용한다.</p>\n<p>개발자 한정 2년만 근무하면 시민권도 쉽게 얻을 수 있다.\n대신 세금으로 절반을 가져간다 💸<br>\n이 말을 듣고 한국에서 살아야겠다는 생각이 들었다.</p>\n<p>인터넷이 정말 느려서 불편하다고 생각했는데 이 정도면 유럽에서 엄청 빠른 편이라고 한다.<br>\n하지만 스웨덴 스톡홀름에 가보니 독일 인터넷이 느리다는걸 확신할 수 있었다.</p>\n<br>\n<p>어쨋든 무사히 돌아와서 다행이라는 생각이 들었다.\n영어로 일하고 회의하는 것은 정말 많은 노력이 필요했다.<br>\n대신 유럽에서 근무하면 주말 동안 주변 국가 여행할 수 있다는 점이 큰 장점이다.<br>\n요즘 재택이 많아지다보니 해외에서 근무 가능한 회사들도 많이 생기고 있는데<br>\n만약 유럽에서 살고 싶다면 베를린에서 살아보는 것도 괜찮은 것 같다.</p>\n<br>","excerpt":"우연히 회사에서 좋은 기회를 얻게 되어 독일에서…"}}}},{"node":{"title":"MLOps 관련 책, 강의 리뷰 (DMLS, FSDL)","id":"13892440-db0e-57df-aac0-cd290e75a53a","slug":"mlops-dmls-fsdl","publishDate":"September 13, 2022","heroImage":{"id":"1faaada3-e12b-5548-8532-08b7c04dc7eb","title":"cover-personal","fluid":{"aspectRatio":1.694915254237288,"src":"//images.ctfassets.net/tushy4jlcik7/3ltdJp06NzCExAWz9OF8Ak/d8ca530c80e7c79a7bd7e4c396c0ae00/cover_personal.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/3ltdJp06NzCExAWz9OF8Ak/d8ca530c80e7c79a7bd7e4c396c0ae00/cover_personal.jpg?w=450&h=266&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/3ltdJp06NzCExAWz9OF8Ak/d8ca530c80e7c79a7bd7e4c396c0ae00/cover_personal.jpg?w=900&h=531&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/3ltdJp06NzCExAWz9OF8Ak/d8ca530c80e7c79a7bd7e4c396c0ae00/cover_personal.jpg?w=1400&h=826&q=50 1400w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/3ltdJp06NzCExAWz9OF8Ak/d8ca530c80e7c79a7bd7e4c396c0ae00/cover_personal.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/3ltdJp06NzCExAWz9OF8Ak/d8ca530c80e7c79a7bd7e4c396c0ae00/cover_personal.jpg?w=450&h=266&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/3ltdJp06NzCExAWz9OF8Ak/d8ca530c80e7c79a7bd7e4c396c0ae00/cover_personal.jpg?w=900&h=531&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/3ltdJp06NzCExAWz9OF8Ak/d8ca530c80e7c79a7bd7e4c396c0ae00/cover_personal.jpg?w=1400&h=826&q=50&fm=webp 1400w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/3ltdJp06NzCExAWz9OF8Ak/d8ca530c80e7c79a7bd7e4c396c0ae00/cover_personal.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"91b90df6-308d-53ad-ae81-6b45069a2853","childMarkdownRemark":{"id":"849ece24-487b-5a7e-a7b9-dfa2eaaca7e7","timeToRead":1,"html":"<p>MLOps는 다양한 지식과 컴포넌트를 다루고 있어 따로 공부하다보면 중요한 부분을 놓치고 도구에만 집착하게 되는 경우도 많습니다. 반면 알려진 책이나 강의를 들으니 퍼즐 조각들이 맞춰지는 것처럼 흩어져 있는 지식들이 하나로 정리되는 느낌을 받을 수 있었습니다.</p>\n<p>이 글에서는 MLOps 관련 자료 중 유명한 <strong>Full Stack Deep Learning</strong> 강의와 <strong>Designing Machine Learning Systems</strong> 책을 리뷰해보려 합니다.\nMLOps에 대해 관심있거나 시작하기 위해 자료를 찾는 분들에게 도움이 될 수 있을 것 같습니다.</p>\n<br>\n<h2 id=\"full-stack-deep-learning\" style=\"position:relative;\"><a href=\"#full-stack-deep-learning\" aria-label=\"full stack deep learning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Full Stack Deep Learning</h2>\n<p><img src=\"https://drive.google.com/uc?export=view&#x26;id=1zdLxqWwSlLAwxfI_YSvATPgnadHtxg3N\" alt=\"fsdl\"></p>\n<p>FSDL 강의는 MLOps 전반적인 주제를 모두 다루는 온라인 강의입니다.<br>\n아래 사이트 또는 유튜브에서 최신 강의를 볼 수 있습니다.<br>\n링크: <a href=\"https://fullstackdeeplearning.com/\">https://fullstackdeeplearning.com/</a></p>\n<p><strong>좋았던 점</strong><br>\n매년 강의의 내용이 최신 트렌드를 최대한 반영하기 위해 업데이트 되고 있으며 Lab이라는 실습 과정이 준비되어 있습니다. CoLab 환경에서 실습할 수 있도록 자료가 준비되어 있는데 특히 ML 테스트 챕터의 자료가 좋았습니다.<br>\n중간에 다양한 오픈소스나 도구들을 소개해주는데 직접 구축하는 경우에도 아이디어를 얻을 수 있어 유용했습니다.</p>\n<p><strong>아쉬운 점</strong><br>\n많은 내용을 다루다보니 특정 주제는 간단하게 이미지나 링크만 공유하고 넘어가는 경우가 있어 설명이 부족한 경우가 있습니다. 제대로 이해하려면 제공되는 학습 자료들을 모두 찾아서 봐야 했습니다.<br>\n절대 사용할 일이 없을 법한 초기 스타트업들의 SaaS를 소개할때가 있는데 광고를 받은게 아닌가 싶은 생각이 들었습니다.</p>\n<br>\n<h2 id=\"designing-machine-learning-systems\" style=\"position:relative;\"><a href=\"#designing-machine-learning-systems\" aria-label=\"designing machine learning systems permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Designing Machine Learning Systems</h2>\n<p><img src=\"https://drive.google.com/uc?export=view&#x26;id=1dgiLa6nKVdS_CzevNlvYxuvss45yWHIV\" alt=\"dmls\"></p>\n<p>DMLS 책은 스탠포드 MLOps 강의로 유명한 Chip Huyen 교수님이 최근에 출판한 책입니다.<br>\n아직 한글판은 없어 Oreilly Learning 또는 Amazon에서 받아볼 수 있습니다.</p>\n<p><strong>좋았던 점</strong><br>\nFSDL보다 더 구체적인 사례를 들어 전반적인 내용을 이해하기 쉽게 설명한다고 느꼈습니다. 특히 구조적으로 설명해주고 바로 실무에 적용할 수 있도록 여러 가이드라인을 제시해주는 부분이 많습니다.<br>\n여러 오픈소스나 도구에 대해서도 기능을 구체적으로 다루기보다 어떤 기준으로 선택해야 하는지를 설명합니다. 실제 프로덕션 환경에서 마주치는 문제들을 소개하고 어떻게 해결하는지에 대한 내용을 미국 빅테크 기업들의 사례를 통해 설명하는 부분이 좋았습니다.</p>\n<p><strong>아쉬운 점</strong><br>\nMLOps와 데이터플랫폼의 역할을 완전히 나누어 두고 이건 우리의 역할이 아니라고 단정 짓는 부분들이 있습니다. 맞는 말이지만 어느 정도 같이 보고 싶은 분들에게는 아쉬울 수 있을 것 같습니다.</p>\n<br>\n<p>처음 시작한다면 접근성이 좋은 FSDL 강의를 보고 이후에 DMLS 책을 보는걸 추천드립니다.<br>\n특히 Data Distribution Shifts and Monitoring 목차는 FSDL을 먼저 확인한 다음 책을 보는게 이해하는데 도움이 되었습니다.</p>","excerpt":"MLOps…"}}}},{"node":{"title":"Spark on Kubernetes: 스팟 인스턴스 사용을 위한 기능들","id":"1e8b83c6-ab58-5682-b4e9-2c995faf6103","slug":"spark-on-kubernetes-spot-instance","publishDate":"July 23, 2022","heroImage":{"id":"dab22ea8-d54d-52a6-852a-278ba3b19a2b","title":"cover-dataengineering","fluid":{"aspectRatio":1.499531396438613,"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50 1600w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50&fm=webp 1600w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"456f045e-0520-51cd-b056-4f330449b7a5","childMarkdownRemark":{"id":"dbcbd6f9-ba42-5df6-8d65-cab1263fa8e9","timeToRead":3,"html":"<p>스팟 인스턴스 유형을 사용하면 온디맨드에 비해 70~90%의 비용을 절감할 수 있습니다.\n하지만 스팟 인스턴스는 가격 입찰, 가용성 등 여러 이유로 중단될 수 있습니다.\n따라서 스팟 인스턴스를 사용한다면 노드가 중단되는 상황에 대비할 수 있어야 합니다.\n이 글에서는 Spark on Kubernetes를 스팟 인스턴스 위에서 안정적으로 운영하기 위해 필요한 설정들을 정리해보려 합니다.</p>\n<p><br><br></p>\n<h2 id=\"driver는-on-demand에-할당하기\" style=\"position:relative;\"><a href=\"#driver%EB%8A%94-on-demand%EC%97%90-%ED%95%A0%EB%8B%B9%ED%95%98%EA%B8%B0\" aria-label=\"driver는 on demand에 할당하기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>driver는 on-demand에 할당하기</h2>\n<p>중단된 노드에 있던 <code class=\"language-text\">driver pod</code>가 종료되는 경우, Spark 작업은 실패하게 됩니다. <code class=\"language-text\">executor pod</code>가 종료되는 경우, 캐시된 데이터 또는 셔플 파일을 잃게 되지만 새로운 executor를 통해 이를 다시 계산하기 때문에 전체 작업이 실패하지는 않습니다.</p>\n<p>위와 같은 이유로 <strong>driver는 온디맨드 인스턴스에 할당</strong>하는 것이 안전합니다.\n노드 그룹을 분리하고 <code class=\"language-text\">nodeSelector</code>를 활용한다면 driver는 온디맨드에서, executor는 스팟에서 실행하도록 설정할 수 있습니다.</p>\n<p><br><br></p>\n<h2 id=\"적절한-인스턴스-유형-선택하기\" style=\"position:relative;\"><a href=\"#%EC%A0%81%EC%A0%88%ED%95%9C-%EC%9D%B8%EC%8A%A4%ED%84%B4%EC%8A%A4-%EC%9C%A0%ED%98%95-%EC%84%A0%ED%83%9D%ED%95%98%EA%B8%B0\" aria-label=\"적절한 인스턴스 유형 선택하기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>적절한 인스턴스 유형 선택하기</h2>\n<p>일부 인스턴스 유형은 해당 시점의 spot market 상황에 따라 안정적으로 확보하지 못할 수도 있습니다. 확보를 못하게 되면 executor는 계속 pending 상태에 머무르게 되고 전체 수행시간도 지연됩니다.</p>\n<p>사용량에 비해 크기가 큰 인스턴스 유형을 선택했다면, 여러 <code class=\"language-text\">executor pod</code>가 하나의 노드에 할당됩니다. 이 때 해당 노드가 중단된다면 여러 executor가 종료되므로 재계산에 더 많은 시간이 소요됩니다.</p>\n<p>위와 같은 이유로 적절한 인스턴스 유형을 선택하는 것이 spot kill을 줄이는데 도움이 됩니다.\nKarpenter를 사용한다면, 여러 인스턴스 유형을 지정하여 Pod의 리소스 요청량에 가장 적합한 노드를 프로비저닝 할 수 있습니다. 또한 <code class=\"language-text\">Instance Fleet</code>의 <code class=\"language-text\">Allocation Strategy</code>에 따라 가장 안정적으로 확보 가능한 인스턴스 유형을 선택할 수 있습니다.</p>\n<p><br><br></p>\n<h2 id=\"spark-31-graceful-executor-decommissioning\" style=\"position:relative;\"><a href=\"#spark-31-graceful-executor-decommissioning\" aria-label=\"spark 31 graceful executor decommissioning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark 3.1: Graceful Executor Decommissioning</h2>\n<p><code class=\"language-text\">Graceful Executor Decommissioning</code>은 Spark 3.1 버전에 추가된 기능입니다.\n이 기능을 통해 <strong>노드가 중단되더라도 최소한의 손실로 Spark 작업이 지속되도록 설정</strong>할 수 있습니다. 이를 사용하려면 먼저 클러스터에 <code class=\"language-text\">Node Termination Handler</code>가 설치되어 있어야 합니다. <code class=\"language-text\">Node Termination Handler</code>는 클라우드에 따라 다르게 설치할 수 있도록 지원하고 있습니다.</p>\n<p>이제 노드가 중단되었을 때 과정을 아래 그림을 통해 확인해보겠습니다.</p>\n<p><img src=\"https://drive.google.com/uc?export=view&#x26;id=1AWx29kv1p1V3MHI6gPqfvXwQWcLhkGGV\" alt=\"spark-decom\"></p>\n<ol>\n<li>스팟 인스턴스가 중단되기 약 120초 전에 <code class=\"language-text\">Termination Handler</code>의 notice 발생</li>\n<li>driver가 해당 executor를 blacklist에 추가하고 신규 task의 스케줄링을 차단</li>\n<li>중단되는 노드에 있던 캐시된 데이터, 셔플 파일을 다른 노드로 복제</li>\n<li>실패 처리된 task를 이어서 수행 (복제한 파일을 그대로 활용)</li>\n</ol>\n<br>\n<p>위의 과정을 통해 노드가 중단되었을 때 재계산을 최소화 할 수 있습니다.<br>\n이 기능에는 다음과 같이 일부 제한 사항도 존재합니다.</p>\n<p>120초의 시간 제한이 있기 때문에 <strong>옮겨야할 파일이 아주 큰 경우, 일부 파일 손실이 발생</strong>할 수 있습니다. 일반적으로 non-SSD 볼륨은 분당 최대 15GB, SSD 볼륨은 35~40GB 까지 가능합니다. 동시에 많은 executor가 spot kill 당하는 경우, 동일한 이유로 파일 손실이 발생할 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">spark.decommission.enabled\nspark.storage.decommission.enabled\nspark.storage.decommission.rddBlocks.enabled\nspark.storage.decommission.shuffleBlocks.enabled</code></pre></div>\n<p><code class=\"language-text\">Graceful Executor Decommissioning</code>은 위의 설정을 통해 활성화 할 수 있습니다.</p>\n<p><br><br></p>\n<h2 id=\"spark-32-executor-pvc-reuse\" style=\"position:relative;\"><a href=\"#spark-32-executor-pvc-reuse\" aria-label=\"spark 32 executor pvc reuse permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark 3.2: Executor PVC Reuse</h2>\n<p><img src=\"https://drive.google.com/uc?export=view&#x26;id=1X7Ud-SEi0jwoZjuWtaqCrXTsK-5X97Zd\" alt=\"spark-reuse\"></p>\n<p><code class=\"language-text\">Executor PVC Reuse</code>는 Spark 3.2 버전에 추가된 기능입니다.\n이 기능을 통해 spot kill 이후에도 <strong>동일한 PVC 연결을 통해 셔플 파일을 재사용</strong>할 수 있습니다. 이를 사용하려면 먼저 클러스터에 <code class=\"language-text\">Dynamic PVC</code>에 대한 설정이 필요합니다.</p>\n<p>현재는 NVMe 기반의 SSD에서 사용이 어렵다는 제한 사항이 있습니다.<br>\n또한 PVC가 즉시 재사용 불가능한 상황이라면 race condition이 발생할 수도 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">spark.kubernetes.driver.reusePersistentVolumeClaim\nspark.kubernetes.driver.ownPersistentVolumeClaim\nspark.kubernetes.executor.volumes.persistentVolumeClaim.data.options.*\nspark.kubernetes.executor.volumes.persistentVolumeClaim.data.mount.*</code></pre></div>\n<p>Executor PVC Reuse는 위의 설정을 통해 활성화 할 수 있습니다.</p>\n<br>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h2>\n<ul>\n<li><a href=\"https://databricks.com/it/dataaisummit/session/how-make-apache-spark-kubernetes-run-reliably-spot-instances/\">https://databricks.com/it/dataaisummit/session/how-make-apache-spark-kubernetes-run-reliably-spot-instances/</a></li>\n<li><a href=\"https://issues.apache.org/jira/browse/SPARK-20624\">https://issues.apache.org/jira/browse/SPARK-20624</a></li>\n<li><a href=\"https://issues.apache.org/jira/browse/SPARK-35593\">https://issues.apache.org/jira/browse/SPARK-35593</a></li>\n</ul>","excerpt":"스팟 인스턴스 유형을 사용하면 온디맨드에 비해 70~9…"}}}},{"node":{"title":"쿠버네티스에서 GPU 리소스를 효율적으로 활용하는 방법","id":"4f44d330-2a1d-5f91-a35c-744c2ed60fc7","slug":"gpu-utilization","publishDate":"July 08, 2022","heroImage":{"id":"f36c235f-3e3e-517d-bd80-697bc6183072","title":"cover-devops","fluid":{"aspectRatio":1.5,"src":"//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=1080&h=720&q=50 1080w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=1080&h=720&q=50&fm=webp 1080w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"3bdf2497-323d-5fe7-900e-cda640c9fff6","childMarkdownRemark":{"id":"d6c114d7-5f57-51d4-ab3c-80b0d5b4125c","timeToRead":3,"html":"<p>GPU는 강력한 연산 기능을 제공하지만 비용이 많이 들기 때문에 제한된 리소스를 효율적으로 활용하는 것이 중요합니다. 이번 글에서는 NVIDIA GPU의 리소스 공유를 지원하기 위한 방법으로 <strong>Time Slicing</strong>과 <strong>MIG</strong>에 대해 정리해보려 합니다.</p>\n<br>\n<h2 id=\"gpu-리소스가-낭비되고-있다\" style=\"position:relative;\"><a href=\"#gpu-%EB%A6%AC%EC%86%8C%EC%8A%A4%EA%B0%80-%EB%82%AD%EB%B9%84%EB%90%98%EA%B3%A0-%EC%9E%88%EB%8B%A4\" aria-label=\"gpu 리소스가 낭비되고 있다 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GPU 리소스가 낭비되고 있다?</h2>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1OJV0IVyYU3NjbRYFbM2ZueKAMVZi5_4y\" alt=\"utilization\"></p>\n<p>여러 아키텍쳐(암페어, 파스칼 등)로 구성된 GPU들을 모아 쿠버네티스 노드 풀을 구성하고 사용자들은 GPU 리소스를 할당받아 사용하는 환경이라고 가정해보겠습니다. 사용자들은 GPU 할당을 못 받는 상황임에도 실제 GPU 사용량을 측정해보면 생각보다 낮게 유지되고 있는 경우가 있습니다. 워크로드에 따라 필요한 리소스가 다르기 때문입니다.</p>\n<p>노트북 환경은 항상 개발을 하는게 아니기 때문에 idle 상태로 대기하는 시간이 많습니다. 작은 배치 사이즈로 운영되는 인퍼런스의 경우, 트래픽에 따라 사용량이 달라질 수 있습니다.\n따라서 이런 상황에서는 <strong>항상 리소스를 점유하기 보다 필요할 때 bursting 가능한 방식으로 운영</strong>하는 것이 효율적입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Pod\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> cuda<span class=\"token punctuation\">-</span>vector<span class=\"token punctuation\">-</span>add\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">restartPolicy</span><span class=\"token punctuation\">:</span> OnFailure\n  <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> cuda<span class=\"token punctuation\">-</span>vector<span class=\"token punctuation\">-</span>add\n      <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"k8s.gcr.io/cuda-vector-add:v0.1\"</span>\n      <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">limits</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">nvidia.com/gpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1 </span><span class=\"token comment\"># GPU 1개 요청하기</span></code></pre></div>\n<p>쿠버네티스에서는 디바이스 플러그인을 통해 Pod가 GPU 리소스를 요청할 수 있습니다.\n하지만 <strong>Pod는 하나 이상의 GPU만 요청할 수 있으며 CPU와 달리 GPU의 일부(fraction)를 요청하는 것은 불가능</strong>합니다. 예를 들어 간단한 실험에 최신 버전의 고성능 GPU 1개를 온전히 할당 받는 것은 낭비입니다. NVIDIA 문서에서는 SW/HW 관점에서 GPU 리소스를 효율적으로 사용하기 위해 다양한 방법을 소개합니다. 그 중 <strong>Time Slicing</strong>과 <strong>MIG</strong>에 대해 알아보겠습니다.</p>\n<br>\n<h2 id=\"time-slicing\" style=\"position:relative;\"><a href=\"#time-slicing\" aria-label=\"time slicing permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Time Slicing</h2>\n<p>Time Slicing은 GPU의 <strong>시간 분할 스케줄러</strong>입니다.\n파스칼 아키텍쳐부터 지원하는 <strong>compute preemption</strong> 기능을 활용한 방법입니다.\n각 컨테이너는 공평하게 <code class=\"language-text\">timeslice</code>를 할당받게 되지만 전환할 때 <code class=\"language-text\">context switching</code> 비용이 발생합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> ConfigMap\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> time<span class=\"token punctuation\">-</span>slicing<span class=\"token punctuation\">-</span>config\n  <span class=\"token key atrule\">namespace</span><span class=\"token punctuation\">:</span> gpu<span class=\"token punctuation\">-</span>operator\n<span class=\"token key atrule\">data</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">a100-40gb</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span><span class=\"token punctuation\">-</span>\n    <span class=\"token key atrule\">version</span><span class=\"token punctuation\">:</span> v1\n    <span class=\"token key atrule\">sharing</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">timeSlicing</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n        <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> nvidia.com/gpu\n          <span class=\"token key atrule\">replicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">8</span>\n        <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> nvidia.com/mig<span class=\"token punctuation\">-</span>1g.5gb\n          <span class=\"token key atrule\">replicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n  <span class=\"token key atrule\">tesla-t4</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span><span class=\"token punctuation\">-</span>\n    <span class=\"token key atrule\">version</span><span class=\"token punctuation\">:</span> v1\n    <span class=\"token key atrule\">sharing</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">timeSlicing</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n        <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> nvidia.com/gpu\n          <span class=\"token key atrule\">replicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">4</span></code></pre></div>\n<p><strong>NVIDIA GPU Operator</strong>에서는 위와 같이 <code class=\"language-text\">ConfigMap</code>을 사용하거나 <code class=\"language-text\">node label</code>을 통해 설정할 수 있습니다. 설정한 이후에 노드를 확인해보면 아래와 같이 리소스에 값이 추가된 것을 확인할 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\">$ kubectl describe node $NODE\n\n<span class=\"token key atrule\">status</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">capacity</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">nvidia.com/gpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">8</span>\n  <span class=\"token key atrule\">allocatable</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">nvidia.com/gpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">8</span></code></pre></div>\n<p>최대 8개 컨테이너까지 <code class=\"language-text\">timeslice</code> 방식으로 shared GPU를 사용할 수 있다는 것을 의미합니다. 이 방법은 <strong>GPU 메모리 limit 설정을 강제하는 것이 아니기 때문에 OOM이 발생</strong>할 수도 있습니다. 이를 방지하려면 GPU를 사용하는 컨테이너 수를 모니터링하고 <code class=\"language-text\">Tensorflow</code>나 <code class=\"language-text\">PyTorch</code> 같은 프레임워크에서 총 GPU 메모리 제한 설정이 필요합니다.</p>\n<br>\n<h2 id=\"multi-instance-gpu-mig\" style=\"position:relative;\"><a href=\"#multi-instance-gpu-mig\" aria-label=\"multi instance gpu mig permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Multi instance GPU (MIG)</h2>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1bJYen4Q33jEa9yHcp4LR3NOPNNzTMZCR\" alt=\"mig\"></p>\n<p>MIG는 A100과 같은 <strong>암페어 아키텍처 기반 GPU를 최대 7개의 개별 GPU 인스턴스로 분할해서 사용</strong>할 수 있는 기능입니다.\n분할된 인스턴스를 <strong>파티션</strong>이라고 부르는데, 각 파티션은 물리적으로 격리되어 있기 때문에 안전하게 병렬로 사용할 수 있습니다.</p>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=12kdw7KZWuph3ADBsJO4FMD4m1fIxufAj\" alt=\"partition\"></p>\n<p>위의 표와 같이 설정을 통해 파티션 크기를 조정할 수 있습니다. 표에서 unit은 하나의 파티션에 몇 개가 할당되는지를 의미합니다.\nA100의 경우, 최대 7개의 <code class=\"language-text\">compute unit</code>과 8개의 <code class=\"language-text\">memory unit</code>을 가질 수 있습니다 (각 5GB 메모리). 파티션은 <code class=\"language-text\">&lt;compute&gt;g.&lt;memory&gt;gb</code> 형식을 따르고 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\">$ kubectl label nodes $NODE nvidia.com/mig.config=all<span class=\"token punctuation\">-</span>1g.5gb\n$ kubectl describe node $NODE\n\n<span class=\"token key atrule\">status</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">capacity</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">nvidia.com/gpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">7</span>\n  <span class=\"token key atrule\">allocatable</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">nvidia.com/gpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">7</span></code></pre></div>\n<p>이번에도 노드 설정 후, 값을 확인해보면 7이 들어가 있습니다.<br>\n<code class=\"language-text\">1g.5gb</code> 크기의 파티션을 7개까지 사용할 수 있다는 의미입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Deployment\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> cuda<span class=\"token punctuation\">-</span>vectoradd\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">replicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">7</span>\n  <span class=\"token key atrule\">template</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">nodeSelector</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">nvidia.com/gpu.product</span><span class=\"token punctuation\">:</span> A100<span class=\"token punctuation\">-</span>SXM4<span class=\"token punctuation\">-</span>40GB<span class=\"token punctuation\">-</span>MIG<span class=\"token punctuation\">-</span>1g.5gb\n    <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> vectoradd\n      <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> nvidia/samples<span class=\"token punctuation\">:</span>vectoradd<span class=\"token punctuation\">-</span>cuda11.2.1\n      <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">limits</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">nvidia.com/gpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span></code></pre></div>\n<p>위와 같이 MIG를 통해 Pod 마다 1개의 파티션을 갖도록 설정해서 7개의 replica 구성하는 것도 가능합니다. 이처럼 사용자는 <strong>MIG를 통해 GPU를 최대로 활용</strong>할 수 있습니다.</p>\n<br>\n<h2 id=\"time-slicing-vs-mig\" style=\"position:relative;\"><a href=\"#time-slicing-vs-mig\" aria-label=\"time slicing vs mig permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Time Slicing vs MIG</h2>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1NYVmMl0RyQVEnL5toybX8DH-AwpF1Bw2\" alt=\"compare\"></p>\n<p>두 방식을 비교해보면 위의 표와 같습니다.\nTime Slicing 방식은 7개 이상의 컨테이너를 사용할 수 있습니다. 따라서 <strong>bursting 워크로드에 적합한 방식</strong>이라고 볼 수 있습니다. 반면 <strong>MIG는 적은 양의 고정된 사용량을 가지는 워크로드에 적합</strong>합니다.\nA100은 MIG를 통해 분할하고 그 외의 GPU는 Time Slicing을 사용하는 방식으로 함께 사용할 수 있으니 워크로드에 맞는 방식을 선택하는 것이 중요합니다.</p>\n<br>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h2>\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=X876kr-LkPA\">Kubecon 2022 - Improving GPU Utilization using Kubernetes</a></li>\n<li><a href=\"https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/overview.html\">https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/overview.html</a></li>\n</ul>","excerpt":"GPU는 강력한 연산 기능을 제공하지만 비용이 많이 들기 때문에 제한된 리소스를 효율적으로 활용하는 것이 중요합니다. 이번 글에서는 NVIDIA…"}}}},{"node":{"title":"Airflow worker에 KEDA AutoScaler 적용한 후기","id":"b10d61e7-5adc-53e4-b2da-9142ae8bffb5","slug":"airflow-worker-keda-autoscaler","publishDate":"June 24, 2022","heroImage":{"id":"dab22ea8-d54d-52a6-852a-278ba3b19a2b","title":"cover-dataengineering","fluid":{"aspectRatio":1.499531396438613,"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50 1600w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50&fm=webp 1600w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"ea004cc8-b136-57cb-b9c0-22e9a7a65cf3","childMarkdownRemark":{"id":"2a2780a6-435b-5112-9cc3-94f01693acb8","timeToRead":4,"html":"<p>Airflow에서 실행되는 배치 작업들은 특정 시간 또는 야간에 많이 수행되고 이외의 시간은 상대적으로 여유로운 경우가 많습니다. 이러한 상황에서 오토스케일링을 적용한다면 효율적으로 리소스를 최적화하여 사용할 수 있습니다.</p>\n<p>만약 쿠버네티스 위에서 Celery Executor를 사용한다면 worker의 오토스케일링을 위해 KEDA를 고려해볼 수 있습니다. 이 글에서는 Airflow worker에 KEDA AutoScaler를 적용하면서 겪었던 여러 문제들과 해결 과정에 대해 정리해보려 합니다.</p>\n<p><br><br></p>\n<h2 id=\"keda-autoscaler\" style=\"position:relative;\"><a href=\"#keda-autoscaler\" aria-label=\"keda autoscaler permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>KEDA AutoScaler</h2>\n<p>KEDA는 쿠버네티스에서 이벤트 기반 오토스케일링을 쉽게 구현할 수 있도록 지원하는 컴포넌트입니다. 쿠버네티스의 HPA와 함께 동작하며 다양한 built-in scaler를 통해 유연하게 오토스케일링 조건을 설정할 수 있습니다.</p>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1ASRyxTdtpbFqt6qGdRhD98OBSbTa-i8k\" alt=\"keda\"></p>\n<p>만약 Airflow에 적용한다면 위의 그림과 같은 형태로 구성됩니다.\n사용자는 KEDA의 <code class=\"language-text\">ScaledObject</code> CRD를 생성하여 클러스터에 배포합니다.\nKEDA는 쿠버네티스의 API Server와 통신하며 Operator와 같은 형태로써 컨트롤 루프에 따라 동작합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> keda.sh/v1alpha1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> ScaledObject\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> airflow<span class=\"token punctuation\">-</span>worker\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">scaleTargetRef</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> airflow<span class=\"token punctuation\">-</span>worker\n  <span class=\"token key atrule\">pollingInterval</span><span class=\"token punctuation\">:</span> <span class=\"token number\">10</span>\n  <span class=\"token key atrule\">cooldownPeriod</span><span class=\"token punctuation\">:</span> <span class=\"token number\">30</span>\n  <span class=\"token key atrule\">minReplicaCount</span><span class=\"token punctuation\">:</span> <span class=\"token number\">3</span>\n  <span class=\"token key atrule\">maxReplicaCount</span><span class=\"token punctuation\">:</span> <span class=\"token number\">10</span>\n  <span class=\"token key atrule\">triggers</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> postgresql\n      <span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">connectionFromEnv</span><span class=\"token punctuation\">:</span> AIRFLOW_CONN_AIRFLOW_DB\n        <span class=\"token key atrule\">query</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"\"</span></code></pre></div>\n<p><code class=\"language-text\">ScaledObject</code>는 위와 같이 무엇을 기준으로 트리거할지, 스케일링 정책 등을 정의할 수 있습니다. KEDA는 <code class=\"language-text\">minReplicaCount</code>에 따라 다르게 동작하는데 <code class=\"language-text\">minReplicaCount</code>가 0인 경우, KEDA가 trigger 지표를 통해 직접 처리하지만 1 이상인 경우에는 KEDA가 Metrics Server에 전달만하고 HPA를 통해 처리됩니다. 각 옵션에 대한 자세한 설명은 <a href=\"https://keda.sh/docs/2.7/concepts/scaling-deployments/\">공식 문서</a>에서 확인할 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">SELECT</span> ceil<span class=\"token punctuation\">(</span><span class=\"token function\">COUNT</span><span class=\"token punctuation\">(</span><span class=\"token operator\">*</span><span class=\"token punctuation\">)</span>::<span class=\"token keyword\">decimal</span> <span class=\"token operator\">/</span> {{ celery<span class=\"token punctuation\">.</span>worker_concurrency }}<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">FROM</span> task_instance\n<span class=\"token keyword\">WHERE</span> state<span class=\"token operator\">=</span><span class=\"token string\">'running'</span> <span class=\"token operator\">OR</span> state<span class=\"token operator\">=</span><span class=\"token string\">'queued'</span></code></pre></div>\n<p>Airflow에서 사용하는 <code class=\"language-text\">ScaledObject</code>의 트리거 쿼리는 위와 같이<code class=\"language-text\">celery.worker_concurrency</code> 설정을 기준으로 하고 있습니다. 예를 들어 concurrency 설정이 12이며 running 또는 queued 상태의 task instance가 10에서 23으로 증가한 상황이라고 가정해보겠습니다. desired state가 1에서 2로 변경되었기 때문에 deployment의 replica 수는 2로 확장 됩니다. 스케줄이 모두 종료된 이후 다시 task instance가 10으로 줄어들면 replica 수는 1로 축소 됩니다.</p>\n<p>Airflow 공식 차트에서는 KEDA 관련 옵션을 지원하고 있기 때문에 <a href=\"https://airflow.apache.org/docs/helm-chart/stable/keda.html\">공식 문서</a>를 통해 쉽게 적용할 수 있습니다.<br>\n하지만 문제는 적용한 이후에 발생했습니다.</p>\n<br>\n<h2 id=\"적용-후에-발생한-문제\" style=\"position:relative;\"><a href=\"#%EC%A0%81%EC%9A%A9-%ED%9B%84%EC%97%90-%EB%B0%9C%EC%83%9D%ED%95%9C-%EB%AC%B8%EC%A0%9C\" aria-label=\"적용 후에 발생한 문제 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>적용 후에 발생한 문제</h2>\n<p>적용 후에 실행 중인 task의 로그가 갑자기 끊기면서 강제로 실패 처리되는 문제가 있었습니다.<br>\n시간을 보니 worker가 Scale-In 되는 시점에 발생했고 크게 두 가지 문제를 확인할 수 있었습니다.</p>\n<br>\n<h3 id=\"1-hpa의-replica-flapping-문제\" style=\"position:relative;\"><a href=\"#1-hpa%EC%9D%98-replica-flapping-%EB%AC%B8%EC%A0%9C\" aria-label=\"1 hpa의 replica flapping 문제 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. HPA의 replica flapping 문제</h3>\n<p>먼저 의도한 것보다 Scale-In/Out이 너무 빈번하게 발생했습니다.\n새로 노드가 뜨는데 시간이 소요되므로 배치가 많은 시간 대에도 잦은 스케일 조정이 발생하는 것은 비효율적입니다. 이러한 문제를 HPA에서는 <strong>replica flapping</strong> 이라고 말합니다.\nHPA는 이를 제어하기 위해 <strong>안정화 윈도우와 스케일링 정책</strong>을 지원하고 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">behavior</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">scaleDown</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">stabilizationWindowSeconds</span><span class=\"token punctuation\">:</span> <span class=\"token number\">600</span></code></pre></div>\n<p>위와 같이 <code class=\"language-text\">stabilizationWindowSeconds</code> 설정을 600으로 설정하면 이전 10분 동안의 모든 목표 상태를 고려해서 가장 높은 값으로 설정합니다. 현재 시점에 scaleDown 조건을 만족하더라도 즉시 수행되는게 아니라 10분이 지난 시점에 scaleDown이 수행됩니다. 이를 통해 잦은 스케일 조정을 제한할 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">behavior</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">scaleDown</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">policies</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> Pods\n      <span class=\"token key atrule\">value</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n      <span class=\"token key atrule\">periodSeconds</span><span class=\"token punctuation\">:</span> <span class=\"token number\">300</span></code></pre></div>\n<p><code class=\"language-text\">scaleDown.polices</code>를 통해 Scale-In 발생 시 replica 변경 허용에 대한 정책을 지정할 수 있습니다. 위의 예시는 5분 내에 최대 1개의 replica를 scaleDown 하도록 허용하는 정책입니다. 이를 통해 계단식으로 천천히 pod를 축소할 수 있습니다.</p>\n<p>현재 Airflow 공식 차트에서는 KEDA의 advanced 옵션을 지원하지 않아 <a href=\"https://github.com/apache/airflow/pull/24220\">PR</a>을 추가했습니다.<br>\n차트 1.7 버전부터 사용하실 수 있습니다.</p>\n<br>\n<h3 id=\"2-worker-warm-shutdown-문제\" style=\"position:relative;\"><a href=\"#2-worker-warm-shutdown-%EB%AC%B8%EC%A0%9C\" aria-label=\"2 worker warm shutdown 문제 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Worker Warm Shutdown 문제</h3>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=10C7umomf0yxDRGav3STo5-uX3i5JUmdA\" alt=\"celery\"></p>\n<p>celery worker의 warm shutdown이 제대로 이루어지지 않았기 때문에 task의 로그가 갑자기 끊기면서 강제로 실패 했습니다. Airflow의 CeleryExecutor는 위와 같이 여러 프로세스를 통해 수행됩니다. 이 때 실제로 task를 실행하는 프로세스는 main 프로세스가 아니라 subprocess 입니다. celery에서는 실행 중인 task가 처리된 이후에 종료할 수 있도록 <strong>warm shutdown</strong>을 지원하고 있습니다. worker의 main process가 <code class=\"language-text\">SIGTERM</code>을 받으면 task가 종료될때까지 기다리게 됩니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"># warm shutdown log\nworker: Warm shutdown (MainProcess)\n\n -------------- celery@fcd56490a11f v4.4.7 (cliffs)\n--- ***** -----\n-- ******* ---- Linux-5.4.0-1045-aws-x86_64-with-debian-10.8\n- *** --- * ---\n- ** ---------- [config]\n- ** ---------- .&gt; app:         airflow.executors.celery_executor:0x7f95\n- ** ---------- .&gt; transport:   redis://redis:6379/0\n- ** ---------- .&gt; results:     postgresql://airflow:**@postgres/airflow\n- *** --- * --- .&gt; concurrency: 16 (prefork)\n-- ******* ---- .&gt; task events: OFF (enable -E to monitor tasks in this worker)\n--- ***** -----\n -------------- [queues]\n                .&gt; default          exchange=default(direct) key=default\n\n[tasks]\n  . airflow.executors.celery_executor.execute_command</code></pre></div>\n<p><a href=\"https://swalloow.github.io/container-tini-dumb-init/\">이전 글</a>에서 설명한 것처럼 Airflow 공식 차트에서 worker pod은 <code class=\"language-text\">DUMB_INIT_SETSID=0</code>으로 이미 설정되어 있기 때문에 메인 프로세스에만 SIGNAL이 전파되고 task process는 계속 실행됩니다. 하지만\n<strong>scaleDown이 발생한다면, 실행 중이던 worker pod이 종료되기 때문에 pod 내에 있던 task process도 함께 강제 종료되면서 task가 실패</strong>하게 됩니다. 장시간 수행되는 task 일수록 이러한 문제를 마주칠 가능성이 높습니다.</p>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1MO0UQlSLa2mJvYyNZBajcfzKN14dhufr\" alt=\"learnk8s\"></p>\n<p>이를 해결하기 위해 task의 execution_timeout 시간까지 pod가 종료되지 않도록 <code class=\"language-text\">terminationGracePeriodSeconds</code>를 지정해주었습니다. 이제 각 컨테이너 내부의 프로세스 1에 <code class=\"language-text\">SIGTERM</code>이 전달되더라도 pod의 graceful shutdown 시간 동안 대기하므로 task process는 계속 실행됩니다. 시간이 모두 지나면 <code class=\"language-text\">SIGKILL</code>을 통해 모든 프로세스가 종료되고 pod도 삭제됩니다.</p>\n<br>\n<h2 id=\"적용-후기\" style=\"position:relative;\"><a href=\"#%EC%A0%81%EC%9A%A9-%ED%9B%84%EA%B8%B0\" aria-label=\"적용 후기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>적용 후기</h2>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=16q46FTUBbbJRrccrGFLihodJKODGDW1v\" alt=\"test\"></p>\n<p>위의 문제들을 모두 수정한 이후부터 안정적으로 worker의 확장, 축소가 이루어졌습니다.<br>\n위 그림과 같이 개발 환경에 동시성 테스트를 위한 DAG을 먼저 만들어서 slot 지표에 따라 replica count가 어떻게 변화하는지 확인해본다면 안정적으로 적용할 수 있습니다.</p>\n<br>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h2>\n<ul>\n<li><a href=\"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#flapping\">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#flapping</a></li>\n<li><a href=\"https://learnk8s.io/graceful-shutdown\">https://learnk8s.io/graceful-shutdown</a></li>\n</ul>","excerpt":"Airflow…"}}}},{"node":{"title":"컨테이너 환경을 위한 초기화 시스템 (tini, dumb-init)","id":"bf47ab9d-ffa5-5ef8-b122-4a731c1ccf6d","slug":"container-tini-dumb-init","publishDate":"May 27, 2022","heroImage":{"id":"dab22ea8-d54d-52a6-852a-278ba3b19a2b","title":"cover-dataengineering","fluid":{"aspectRatio":1.499531396438613,"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50 1600w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50&fm=webp 1600w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"c81d0d2f-c1fe-507d-951c-fa83f8ce2010","childMarkdownRemark":{"id":"0cadd438-2c43-5bca-a5d1-3abec3fb9b65","timeToRead":5,"html":"<p>쿠버네티스 기반의 데이터플랫폼을 운영하다보면 이미지의 <code class=\"language-text\">ENTRYPOINT</code>에 <code class=\"language-text\">tini</code>, <code class=\"language-text\">dumb-init</code>과 같은 명령어를 사용하는 경우가 많습니다. 예를 들어 Airflow에서는 dumb-init을, SparkOperator에서는 tini를 사용하고 있습니다. 이 글에서는 컨테이너 환경에서 왜 이러한 초기화 시스템이 필요한지 알아보려 합니다.</p>\n<p><br><br></p>\n<h2 id=\"pid-1의-역할\" style=\"position:relative;\"><a href=\"#pid-1%EC%9D%98-%EC%97%AD%ED%95%A0\" aria-label=\"pid 1의 역할 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>PID 1의 역할</h2>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1ki_LL8QtIXzXWjRjX91HXWbUaNfXqk5S\" alt=\"top\"></p>\n<p>리눅스에서 <strong>PID 1은 부팅 시 커널에 의해 최초로 실행되는 init 프로세스</strong>입니다.\ninit 프로세스는 SSH 데몬, Docker 데몬, Apache/Nginx 시작 등과 같은 시스템들의 시작을 담당합니다. 각 프로세스는 차례로 추가 하위 프로세스를 생성할 수 있습니다. PID 1은 결국 모든 프로세스의 최종 부모 프로세스 역할을 하게 됩니다. 현재 배포판들은 복잡한 init 대신 systemd가 초기화 시스템의 역할을 대신하고 있습니다.</p>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=16ONXGJUMijn6HUHt4h1tAepWSQw0vDup\" alt=\"zombie\"></p>\n<p>여기까지는 일반적인 상황입니다. 만약 예기치 못한 상황으로 인해 프로세스가 종료되면 어떻게 될까요? bash(PID 5) 프로세스가 종료된다고 가정해보겠습니다. 5번은 이제 <strong>좀비 프로세스</strong>로 변합니다.</p>\n<p>왜 이런 일이 발생할까요? Unix는 부모 프로세스가 종료 상태를 수집하기 위해 자식 프로세스 종료를 명시적으로 대기하는 방식으로 설계되었기 때문입니다. 좀비 프로세스는 부모 프로세스가 시스템 호출의 <code class=\"language-text\">waitpid()</code> 시스템 명령을 수행할 때까지 존재합니다. 좀비를 제거하기 위해 자식 프로세스에서 <code class=\"language-text\">waitpid()</code>를 호출하는 작업을 <strong>reaping</strong>이라고 합니다.</p>\n<p>대부분의 경우 이러한 상황이 큰 문제가 되지 않습니다. 많은 어플리케이션이 자식 프로세스를 올바르게 가져옵니다. sshd를 사용하는 위의 예시에서 bash가 종료되면 운영 체제는 <code class=\"language-text\">SIGCHLD</code> 신호를 sshd에 보내 깨우게 합니다. sshd는 신호를 통해 인지하고 자식 프로세스를 거둡니다.</p>\n<p>하지만 부모 프로세스가 의도적으로 종료되거나 사용자가 프로세스를 종료시켰다고 가정해보겠습니다. 그러면 그 자식 프로세스들은 어떻게 될까요? 더 이상 상위 프로세스가 없으므로 <strong>고아 상태(orphaned)</strong>가 됩니다.</p>\n<p>init 프로세스는 이를 해결하기 위한 작업을 수행합니다. 바로 <strong>고아 상태가 된 자식 프로세스를 거두는 것(adopt)</strong> 입니다. init 프로세스에 의해 생성된 적이 없지만 프로세스의 부모가 되어 좀비 프로세스가 되지 않도록 정리해주는 역할을 합니다.</p>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1jHWw-uO9qO1QrnlLhr2KuiPIP14HHaQV\" alt=\"adopt\"></p>\n<p>백그라운드에서 실행되는 nginx 프로세스를 예시로 들어보겠습니다. 먼저 nginx는 자식 프로세스를 만듭니다. 그리고 nginx 프로세스가 종료됩니다. 고아가 된 nginx 자식 프로세스는 init 프로세스가 거두어들입니다.</p>\n<p>이러한 init 프로세스의 역할 덕분에 우리는 어플리케이션을 개발할 때 크게 신경쓰지 않게 되었습니다. 하지만 쿠버네티스를 포함한 컨테이너 환경의 경우, 조금 다릅니다.</p>\n<br>\n<h2 id=\"컨테이너-내부에서의-프로세스-동작\" style=\"position:relative;\"><a href=\"#%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88-%EB%82%B4%EB%B6%80%EC%97%90%EC%84%9C%EC%9D%98-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4-%EB%8F%99%EC%9E%91\" aria-label=\"컨테이너 내부에서의 프로세스 동작 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>컨테이너 내부에서의 프로세스 동작</h2>\n<p>도커는 컨테이너 ENTRYPOINT(CMD)로 명시된 프로세스를 PID 1로써 새로운 PID 네임스페이스에 정의합니다. 그리고 컨테이너 내부에 있는 PID 1 프로세스에만 신호를 보내 종료할 수 있습니다. 이러한 이유로 컨테이너는 경량화 이미지를 기반으로 단일 프로세스만 실행하는 경우가 많습니다. 두 가지 예시를 살펴보겠습니다.</p>\n<p><strong>1. sh 프로세스가 PID 1인 경우</strong><br>\nDockerfile을 통해 다음과 같은 컨테이너 명령을 지정하면 실행을 위해 쉘에 전달됩니다. 그 결과 아래와 같은 프로세스 트리가 생성됩니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">- docker run (on the host machine)\n  - /bin/sh (PID 1, inside container)\n    - python my_server.py (PID 2, inside container)</code></pre></div>\n<p>쉘을 PID 1로 사용하면 실제로 2번 프로세스에 signal를 보내는 것이 거의 불가능합니다. 쉘로 보낸 신호는 하위 프로세스로 전달되지 않으며 프로세스가 완료될 때까지 셸이 종료되지 않습니다. 이 경우 컨테이너를 종료하기 위해 SIGKILL을 보내야 합니다.</p>\n<p><strong>2. 내 프로세스가 PID 1인 경우</strong><br>\nDockerfile에서 다음과 같이 정의하면 프로세스가 즉시 시작되고 컨테이너의 초기화 시스템으로써 작동하여 다음과 같은 프로세스 트리가 생성됩니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">- docker run (on the host machine)\n  - python my_server.py (PID 1, inside container)</code></pre></div>\n<p>이러한 구조가 1번 예시보다 나은 방법입니다. 프로세스는 이제 실제로 보내는 신호를 수신합니다. 그러나 PID 1이므로 예상대로 응답하지 않을 수 있습니다.</p>\n<br>\n<h2 id=\"pid-1의-signal-propagation-문제\" style=\"position:relative;\"><a href=\"#pid-1%EC%9D%98-signal-propagation-%EB%AC%B8%EC%A0%9C\" aria-label=\"pid 1의 signal propagation 문제 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>PID 1의 Signal Propagation 문제</h2>\n<p>컨테이너 환경도 마찬가지로 PID 1은 초기화 시스템의 책임이 있습니다.\n일반적인 프로세스는 <code class=\"language-text\">TERM</code>에 대한 자체 handler를 등록하여 종료하기 전 cleanup을 수행할 수 있습니다. 프로세스가 signal handler를 등록하지 않은 경우, 커널은 일반적으로 <code class=\"language-text\">TERM</code> 신호에 대한 기본 동작인 프로세스 종료를 수행합니다.</p>\n<p>반면 PID 1은 <code class=\"language-text\">TERM</code> 신호에 대해 기본 동작으로 실행되지 않습니다. 따라서 signal handler를 등록하지 않은 경우, <code class=\"language-text\">TERM</code>은 프로세스에 아무런 영향도 미치지 못합니다.\n만약 자식 프로세스가 하위 프로세스를 생성하고 먼저 죽었다면, 컨테이너 상에 좀비 프로세스가 계속 쌓일 수 있습니다.</p>\n<p>docker run이 <code class=\"language-text\">SIGTERM</code>을 수신하면 컨테이너 자체가 죽지 않더라도 신호를 컨테이너로 전달한 다음 종료됩니다. docker stop 명령을 사용해도 마찬가지입니다. <code class=\"language-text\">TERM</code> signal을 보내고 10초 동안 기다린 다음 프로세스가 여전히 중지되지 않으면 KILL이 전송되어 정리할 기회 없이 즉시 중지됩니다.</p>\n<br>\n<h2 id=\"dumb-init\" style=\"position:relative;\"><a href=\"#dumb-init\" aria-label=\"dumb init permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>dumb-init</h2>\n<p>dumb-init은 이러한 문제를 해결하고 컨테이너를 일반 프로세스와 같은 형태로 사용할 수 있도록 지원하기 위해 만들어졌습니다. systemd과 달리 컨테이너에서 사용하기 위해 경량화된 형태로 개발된 초기화 시스템입니다. dumb-init을 사용하면 다음과 같은 프로세스 트리가 생성됩니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">- docker run (on the host machine)\n  - dumb-init (PID 1, inside container)\n    - python my_server.py (PID 2, inside container)</code></pre></div>\n<p>dumb-init은 모든 signal에 대해 signal handler를 등록하고 해당 signal을 프로세스 세션으로 전달합니다. 파이썬 프로세스는 더 이상 PID 1로 실행되지 않기 때문에 dumb-init이 <code class=\"language-text\">TERM</code>과 같은 신호를 전달할 때 handler를 등록하지 않아도 프로세스 종료가 가능합니다. dumb-init은 signal propagation 뿐만 아니라 고아 상태가 된 자식 프로세스를 거두는 역할(adopt)도 수행합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"docker\"><pre class=\"language-docker\"><code class=\"language-docker\"><span class=\"token keyword\">RUN</span> apt install dumb<span class=\"token punctuation\">-</span>init\n<span class=\"token keyword\">ENTRYPOINT</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"/usr/bin/dumb-init\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"--\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"/my/script\"</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p>사용 방법은 정말 간단합니다. 이미지에 바이너리를 설치하고 명령어 실행할 때 추가하면 됩니다.</p>\n<br>\n<h2 id=\"airflow-이미지에서-dumb-init-사용\" style=\"position:relative;\"><a href=\"#airflow-%EC%9D%B4%EB%AF%B8%EC%A7%80%EC%97%90%EC%84%9C-dumb-init-%EC%82%AC%EC%9A%A9\" aria-label=\"airflow 이미지에서 dumb init 사용 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Airflow 이미지에서 dumb-init 사용</h2>\n<p>Airflow도 dumb-init를 ENTRYPOINT에서 사용하고 있습니다. webserver, worker, scheduler pod에서 <code class=\"language-text\">bash -c ENTRYPOINT</code>를 사용하는데 bash는 자식에게 signal을 전달 안하기 때문에 dumb-init 사용이 필요합니다. 컨테이너 내에서는 환경변수를 통해 다르게 설정할 수 있도록 지원하고 있습니다. 설정 값의 차이는 아래와 같습니다.</p>\n<ul>\n<li><code class=\"language-text\">DUMB_INIT_SETSID=1</code> : 메인 프로세스 그룹의 모든 프로세스에 SIGNAL 전파</li>\n<li><code class=\"language-text\">DUMB_INIT_SETSID=0</code> : 메인 프로세스에만 SIGNAL 전파</li>\n</ul>\n<p>공식 차트에서 worker pod은 0으로 나머지는 1로 설정되어 있습니다.<br>\n이유는 Celery Worker의 warm shutdown을 지원하기 위해서 입니다. 특히 Airflow on Kubernetes 구성에서 CeleryExecutor를 사용하는 경우, task의 정상적인 종료를 위해 필요합니다. 이 부분은 다음 포스트에 이어서 정리해보겠습니다.</p>\n<br>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h2>\n<ul>\n<li><a href=\"https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/\">https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/</a></li>\n<li><a href=\"https://engineeringblog.yelp.com/2016/01/dumb-init-an-init-for-docker.html\">https://engineeringblog.yelp.com/2016/01/dumb-init-an-init-for-docker.html</a></li>\n<li><a href=\"https://github.com/Yelp/dumb-init\">https://github.com/Yelp/dumb-init</a></li>\n<li><a href=\"https://airflow.apache.org/docs/docker-stack/entrypoint.html\">https://airflow.apache.org/docs/docker-stack/entrypoint.html</a></li>\n</ul>","excerpt":"쿠버네티스 기반의 데이터플랫폼을 운영하다보면 이미지의 에 , 과 같은 명령어를 사용하는 경우가 많습니다. 예를 들어 Airflow에서는 dumb…"}}}}]}},"pageContext":{"basePath":"","paginationPath":"","pageNumber":0,"humanPageNumber":1,"skip":0,"limit":7,"numberOfPages":16,"previousPagePath":"","nextPagePath":"/2"}}}