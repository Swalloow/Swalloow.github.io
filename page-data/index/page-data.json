{"componentChunkName":"component---src-templates-posts-js","path":"/","result":{"data":{"allContentfulPost":{"edges":[{"node":{"title":"Spark on Kubernetes: 스팟 인스턴스 사용을 위한 기능들","id":"1e8b83c6-ab58-5682-b4e9-2c995faf6103","slug":"spark-on-kubernetes-spot-instance","publishDate":"July 23, 2022","heroImage":{"id":"dab22ea8-d54d-52a6-852a-278ba3b19a2b","title":"cover-dataengineering","fluid":{"aspectRatio":1.499531396438613,"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50 1600w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50&fm=webp 1600w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"456f045e-0520-51cd-b056-4f330449b7a5","childMarkdownRemark":{"id":"dbcbd6f9-ba42-5df6-8d65-cab1263fa8e9","timeToRead":3,"html":"<p>스팟 인스턴스 유형을 사용하면 온디맨드에 비해 70~90%의 비용을 절감할 수 있습니다.\n하지만 스팟 인스턴스는 가격 입찰, 가용성 등 여러 이유로 중단될 수 있습니다.\n따라서 스팟 인스턴스를 사용한다면 노드가 중단되는 상황에 대비할 수 있어야 합니다.\n이 글에서는 Spark on Kubernetes를 스팟 인스턴스 위에서 안정적으로 운영하기 위해 필요한 설정들을 정리해보려 합니다.</p>\n<p><br><br></p>\n<h2 id=\"driver는-on-demand에-할당하기\" style=\"position:relative;\"><a href=\"#driver%EB%8A%94-on-demand%EC%97%90-%ED%95%A0%EB%8B%B9%ED%95%98%EA%B8%B0\" aria-label=\"driver는 on demand에 할당하기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>driver는 on-demand에 할당하기</h2>\n<p>중단된 노드에 있던 <code class=\"language-text\">driver pod</code>가 종료되는 경우, Spark 작업은 실패하게 됩니다. <code class=\"language-text\">executor pod</code>가 종료되는 경우, 캐시된 데이터 또는 셔플 파일을 잃게 되지만 새로운 executor를 통해 이를 다시 계산하기 때문에 전체 작업이 실패하지는 않습니다.</p>\n<p>위와 같은 이유로 <strong>driver는 온디맨드 인스턴스에 할당</strong>하는 것이 안전합니다.\n노드 그룹을 분리하고 <code class=\"language-text\">nodeSelector</code>를 활용한다면 driver는 온디맨드에서, executor는 스팟에서 실행하도록 설정할 수 있습니다.</p>\n<p><br><br></p>\n<h2 id=\"적절한-인스턴스-유형-선택하기\" style=\"position:relative;\"><a href=\"#%EC%A0%81%EC%A0%88%ED%95%9C-%EC%9D%B8%EC%8A%A4%ED%84%B4%EC%8A%A4-%EC%9C%A0%ED%98%95-%EC%84%A0%ED%83%9D%ED%95%98%EA%B8%B0\" aria-label=\"적절한 인스턴스 유형 선택하기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>적절한 인스턴스 유형 선택하기</h2>\n<p>일부 인스턴스 유형은 해당 시점의 spot market 상황에 따라 안정적으로 확보하지 못할 수도 있습니다. 확보를 못하게 되면 executor는 계속 pending 상태에 머무르게 되고 전체 수행시간도 지연됩니다.</p>\n<p>사용량에 비해 크기가 큰 인스턴스 유형을 선택했다면, 여러 <code class=\"language-text\">executor pod</code>가 하나의 노드에 할당됩니다. 이 때 해당 노드가 중단된다면 여러 executor가 종료되므로 재계산에 더 많은 시간이 소요됩니다.</p>\n<p>위와 같은 이유로 적절한 인스턴스 유형을 선택하는 것이 spot kill을 줄이는데 도움이 됩니다.\nKarpenter를 사용한다면, 여러 인스턴스 유형을 지정하여 Pod의 리소스 요청량에 가장 적합한 노드를 프로비저닝 할 수 있습니다. 또한 <code class=\"language-text\">Instance Fleet</code>의 <code class=\"language-text\">Allocation Strategy</code>에 따라 가장 안정적으로 확보 가능한 인스턴스 유형을 선택할 수 있습니다.</p>\n<p><br><br></p>\n<h2 id=\"spark-31-graceful-executor-decommissioning\" style=\"position:relative;\"><a href=\"#spark-31-graceful-executor-decommissioning\" aria-label=\"spark 31 graceful executor decommissioning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark 3.1: Graceful Executor Decommissioning</h2>\n<p><code class=\"language-text\">Graceful Executor Decommissioning</code>은 Spark 3.1 버전에 추가된 기능입니다.\n이 기능을 통해 <strong>노드가 중단되더라도 최소한의 손실로 Spark 작업이 지속되도록 설정</strong>할 수 있습니다. 이를 사용하려면 먼저 클러스터에 <code class=\"language-text\">Node Termination Handler</code>가 설치되어 있어야 합니다. <code class=\"language-text\">Node Termination Handler</code>는 클라우드에 따라 다르게 설치할 수 있도록 지원하고 있습니다.</p>\n<p>이제 노드가 중단되었을 때 과정을 아래 그림을 통해 확인해보겠습니다.</p>\n<p><img src=\"https://drive.google.com/uc?export=view&#x26;id=1HVzNyHmW0YRn9LzX-sBe_uCoYpH3iMse\" alt=\"spark-decom\"></p>\n<ol>\n<li>스팟 인스턴스가 중단되기 약 120초 전에 <code class=\"language-text\">Termination Handler</code>의 notice 발생</li>\n<li>driver가 해당 executor를 blacklist에 추가하고 신규 task의 스케줄링을 차단</li>\n<li>중단되는 노드에 있던 캐시된 데이터, 셔플 파일을 다른 노드로 복제</li>\n<li>실패 처리된 task를 이어서 수행 (복제한 파일을 그대로 활용)</li>\n</ol>\n<br>\n<p>위의 과정을 통해 노드가 중단되었을 때 재계산을 최소화 할 수 있습니다.<br>\n이 기능에는 다음과 같이 일부 제한 사항도 존재합니다.</p>\n<p>120초의 시간 제한이 있기 때문에 <strong>옮겨야할 파일이 아주 큰 경우, 일부 파일 손실이 발생</strong>할 수 있습니다. 일반적으로 non-SSD 볼륨은 분당 최대 15GB, SSD 볼륨은 35~40GB 까지 가능합니다. 동시에 많은 executor가 spot kill 당하는 경우, 동일한 이유로 파일 손실이 발생할 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">spark.decommission.enabled\nspark.storage.decommission.enabled\nspark.storage.decommission.rddBlocks.enabled\nspark.storage.decommission.shuffleBlocks.enabled</code></pre></div>\n<p><code class=\"language-text\">Graceful Executor Decommissioning</code>은 위의 설정을 통해 활성화 할 수 있습니다.</p>\n<p><br><br></p>\n<h2 id=\"spark-32-executor-pvc-reuse\" style=\"position:relative;\"><a href=\"#spark-32-executor-pvc-reuse\" aria-label=\"spark 32 executor pvc reuse permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark 3.2: Executor PVC Reuse</h2>\n<p><img src=\"https://drive.google.com/uc?export=view&#x26;id=1X7Ud-SEi0jwoZjuWtaqCrXTsK-5X97Zd\" alt=\"spark-reuse\"></p>\n<p><code class=\"language-text\">Executor PVC Reuse</code>는 Spark 3.2 버전에 추가된 기능입니다.\n이 기능을 통해 spot kill 이후에도 <strong>동일한 PVC 연결을 통해 셔플 파일을 재사용</strong>할 수 있습니다. 이를 사용하려면 먼저 클러스터에 <code class=\"language-text\">Dynamic PVC</code>에 대한 설정이 필요합니다.</p>\n<p>현재는 NVMe 기반의 SSD에서 사용이 어렵다는 제한 사항이 있습니다.<br>\n또한 PVC가 즉시 재사용 불가능한 상황이라면 race condition이 발생할 수도 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">spark.kubernetes.driver.reusePersistentVolumeClaim\nspark.kubernetes.driver.ownPersistentVolumeClaim\nspark.kubernetes.executor.volumes.persistentVolumeClaim.data.options.*\nspark.kubernetes.executor.volumes.persistentVolumeClaim.data.mount.*</code></pre></div>\n<p>Executor PVC Reuse는 위의 설정을 통해 활성화 할 수 있습니다.</p>\n<br>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h2>\n<ul>\n<li><a href=\"https://databricks.com/it/dataaisummit/session/how-make-apache-spark-kubernetes-run-reliably-spot-instances/\">https://databricks.com/it/dataaisummit/session/how-make-apache-spark-kubernetes-run-reliably-spot-instances/</a></li>\n<li><a href=\"https://issues.apache.org/jira/browse/SPARK-20624\">https://issues.apache.org/jira/browse/SPARK-20624</a></li>\n<li><a href=\"https://issues.apache.org/jira/browse/SPARK-35593\">https://issues.apache.org/jira/browse/SPARK-35593</a></li>\n</ul>","excerpt":"스팟 인스턴스 유형을 사용하면 온디맨드에 비해 70~9…"}}}},{"node":{"title":"쿠버네티스에서 GPU 리소스를 효율적으로 활용하는 방법","id":"4f44d330-2a1d-5f91-a35c-744c2ed60fc7","slug":"gpu-utilization","publishDate":"July 08, 2022","heroImage":{"id":"f36c235f-3e3e-517d-bd80-697bc6183072","title":"cover-devops","fluid":{"aspectRatio":1.5,"src":"//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=1080&h=720&q=50 1080w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=1080&h=720&q=50&fm=webp 1080w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"3bdf2497-323d-5fe7-900e-cda640c9fff6","childMarkdownRemark":{"id":"d6c114d7-5f57-51d4-ab3c-80b0d5b4125c","timeToRead":3,"html":"<p>GPU는 강력한 연산 기능을 제공하지만 비용이 많이 들기 때문에 제한된 리소스를 효율적으로 활용하는 것이 중요합니다. 이번 글에서는 NVIDIA GPU의 리소스 공유를 지원하기 위한 방법으로 <strong>Time Slicing</strong>과 <strong>MIG</strong>에 대해 정리해보려 합니다.</p>\n<br>\n<h2 id=\"gpu-리소스가-낭비되고-있다\" style=\"position:relative;\"><a href=\"#gpu-%EB%A6%AC%EC%86%8C%EC%8A%A4%EA%B0%80-%EB%82%AD%EB%B9%84%EB%90%98%EA%B3%A0-%EC%9E%88%EB%8B%A4\" aria-label=\"gpu 리소스가 낭비되고 있다 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GPU 리소스가 낭비되고 있다?</h2>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1OJV0IVyYU3NjbRYFbM2ZueKAMVZi5_4y\" alt=\"utilization\"></p>\n<p>여러 아키텍쳐(암페어, 파스칼 등)로 구성된 GPU들을 모아 쿠버네티스 노드 풀을 구성하고 사용자들은 GPU 리소스를 할당받아 사용하는 환경이라고 가정해보겠습니다. 사용자들은 GPU 할당을 못 받는 상황임에도 실제 GPU 사용량을 측정해보면 생각보다 낮게 유지되고 있는 경우가 있습니다. 워크로드에 따라 필요한 리소스가 다르기 때문입니다.</p>\n<p>노트북 환경은 항상 개발을 하는게 아니기 때문에 idle 상태로 대기하는 시간이 많습니다. 작은 배치 사이즈로 운영되는 인퍼런스의 경우, 트래픽에 따라 사용량이 달라질 수 있습니다.\n따라서 이런 상황에서는 <strong>항상 리소스를 점유하기 보다 필요할 때 bursting 가능한 방식으로 운영</strong>하는 것이 효율적입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Pod\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> cuda<span class=\"token punctuation\">-</span>vector<span class=\"token punctuation\">-</span>add\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">restartPolicy</span><span class=\"token punctuation\">:</span> OnFailure\n  <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> cuda<span class=\"token punctuation\">-</span>vector<span class=\"token punctuation\">-</span>add\n      <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"k8s.gcr.io/cuda-vector-add:v0.1\"</span>\n      <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">limits</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">nvidia.com/gpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1 </span><span class=\"token comment\"># GPU 1개 요청하기</span></code></pre></div>\n<p>쿠버네티스에서는 디바이스 플러그인을 통해 Pod가 GPU 리소스를 요청할 수 있습니다.\n하지만 <strong>Pod는 하나 이상의 GPU만 요청할 수 있으며 CPU와 달리 GPU의 일부(fraction)를 요청하는 것은 불가능</strong>합니다. 예를 들어 간단한 실험에 최신 버전의 고성능 GPU 1개를 온전히 할당 받는 것은 낭비입니다. NVIDIA 문서에서는 SW/HW 관점에서 GPU 리소스를 효율적으로 사용하기 위해 다양한 방법을 소개합니다. 그 중 <strong>Time Slicing</strong>과 <strong>MIG</strong>에 대해 알아보겠습니다.</p>\n<br>\n<h2 id=\"time-slicing\" style=\"position:relative;\"><a href=\"#time-slicing\" aria-label=\"time slicing permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Time Slicing</h2>\n<p>Time Slicing은 GPU의 <strong>시간 분할 스케줄러</strong>입니다.\n파스칼 아키텍쳐부터 지원하는 <strong>compute preemption</strong> 기능을 활용한 방법입니다.\n각 컨테이너는 공평하게 <code class=\"language-text\">timeslice</code>를 할당받게 되지만 전환할 때 <code class=\"language-text\">context switching</code> 비용이 발생합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> ConfigMap\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> time<span class=\"token punctuation\">-</span>slicing<span class=\"token punctuation\">-</span>config\n  <span class=\"token key atrule\">namespace</span><span class=\"token punctuation\">:</span> gpu<span class=\"token punctuation\">-</span>operator\n<span class=\"token key atrule\">data</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">a100-40gb</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span><span class=\"token punctuation\">-</span>\n    <span class=\"token key atrule\">version</span><span class=\"token punctuation\">:</span> v1\n    <span class=\"token key atrule\">sharing</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">timeSlicing</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n        <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> nvidia.com/gpu\n          <span class=\"token key atrule\">replicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">8</span>\n        <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> nvidia.com/mig<span class=\"token punctuation\">-</span>1g.5gb\n          <span class=\"token key atrule\">replicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n  <span class=\"token key atrule\">tesla-t4</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span><span class=\"token punctuation\">-</span>\n    <span class=\"token key atrule\">version</span><span class=\"token punctuation\">:</span> v1\n    <span class=\"token key atrule\">sharing</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">timeSlicing</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n        <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> nvidia.com/gpu\n          <span class=\"token key atrule\">replicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">4</span></code></pre></div>\n<p><strong>NVIDIA GPU Operator</strong>에서는 위와 같이 <code class=\"language-text\">ConfigMap</code>을 사용하거나 <code class=\"language-text\">node label</code>을 통해 설정할 수 있습니다. 설정한 이후에 노드를 확인해보면 아래와 같이 리소스에 값이 추가된 것을 확인할 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\">$ kubectl describe node $NODE\n\n<span class=\"token key atrule\">status</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">capacity</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">nvidia.com/gpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">8</span>\n  <span class=\"token key atrule\">allocatable</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">nvidia.com/gpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">8</span></code></pre></div>\n<p>최대 8개 컨테이너까지 <code class=\"language-text\">timeslice</code> 방식으로 shared GPU를 사용할 수 있다는 것을 의미합니다. 이 방법은 <strong>GPU 메모리 limit 설정을 강제하는 것이 아니기 때문에 OOM이 발생</strong>할 수도 있습니다. 이를 방지하려면 GPU를 사용하는 컨테이너 수를 모니터링하고 <code class=\"language-text\">Tensorflow</code>나 <code class=\"language-text\">PyTorch</code> 같은 프레임워크에서 총 GPU 메모리 제한 설정이 필요합니다.</p>\n<br>\n<h2 id=\"multi-instance-gpu-mig\" style=\"position:relative;\"><a href=\"#multi-instance-gpu-mig\" aria-label=\"multi instance gpu mig permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Multi instance GPU (MIG)</h2>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1bJYen4Q33jEa9yHcp4LR3NOPNNzTMZCR\" alt=\"mig\"></p>\n<p>MIG는 A100과 같은 <strong>암페어 아키텍처 기반 GPU를 최대 7개의 개별 GPU 인스턴스로 분할해서 사용</strong>할 수 있는 기능입니다.\n분할된 인스턴스를 <strong>파티션</strong>이라고 부르는데, 각 파티션은 물리적으로 격리되어 있기 때문에 안전하게 병렬로 사용할 수 있습니다.</p>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=12kdw7KZWuph3ADBsJO4FMD4m1fIxufAj\" alt=\"partition\"></p>\n<p>위의 표와 같이 설정을 통해 파티션 크기를 조정할 수 있습니다. 표에서 unit은 하나의 파티션에 몇 개가 할당되는지를 의미합니다.\nA100의 경우, 최대 7개의 <code class=\"language-text\">compute unit</code>과 8개의 <code class=\"language-text\">memory unit</code>을 가질 수 있습니다 (각 5GB 메모리). 파티션은 <code class=\"language-text\">&lt;compute&gt;g.&lt;memory&gt;gb</code> 형식을 따르고 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\">$ kubectl label nodes $NODE nvidia.com/mig.config=all<span class=\"token punctuation\">-</span>1g.5gb\n$ kubectl describe node $NODE\n\n<span class=\"token key atrule\">status</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">capacity</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">nvidia.com/gpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">7</span>\n  <span class=\"token key atrule\">allocatable</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">nvidia.com/gpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">7</span></code></pre></div>\n<p>이번에도 노드 설정 후, 값을 확인해보면 7이 들어가 있습니다.<br>\n<code class=\"language-text\">1g.5gb</code> 크기의 파티션을 7개까지 사용할 수 있다는 의미입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Deployment\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> cuda<span class=\"token punctuation\">-</span>vectoradd\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">replicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">7</span>\n  <span class=\"token key atrule\">template</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">nodeSelector</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">nvidia.com/gpu.product</span><span class=\"token punctuation\">:</span> A100<span class=\"token punctuation\">-</span>SXM4<span class=\"token punctuation\">-</span>40GB<span class=\"token punctuation\">-</span>MIG<span class=\"token punctuation\">-</span>1g.5gb\n    <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> vectoradd\n      <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> nvidia/samples<span class=\"token punctuation\">:</span>vectoradd<span class=\"token punctuation\">-</span>cuda11.2.1\n      <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">limits</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">nvidia.com/gpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span></code></pre></div>\n<p>위와 같이 MIG를 통해 Pod 마다 1개의 파티션을 갖도록 설정해서 7개의 replica 구성하는 것도 가능합니다. 이처럼 사용자는 <strong>MIG를 통해 GPU를 최대로 활용</strong>할 수 있습니다.</p>\n<br>\n<h2 id=\"time-slicing-vs-mig\" style=\"position:relative;\"><a href=\"#time-slicing-vs-mig\" aria-label=\"time slicing vs mig permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Time Slicing vs MIG</h2>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1NYVmMl0RyQVEnL5toybX8DH-AwpF1Bw2\" alt=\"compare\"></p>\n<p>두 방식을 비교해보면 위의 표와 같습니다.\nTime Slicing 방식은 7개 이상의 컨테이너를 사용할 수 있습니다. 따라서 <strong>bursting 워크로드에 적합한 방식</strong>이라고 볼 수 있습니다. 반면 <strong>MIG는 적은 양의 고정된 사용량을 가지는 워크로드에 적합</strong>합니다.\nA100은 MIG를 통해 분할하고 그 외의 GPU는 Time Slicing을 사용하는 방식으로 함께 사용할 수 있으니 워크로드에 맞는 방식을 선택하는 것이 중요합니다.</p>\n<br>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h2>\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=X876kr-LkPA\">Kubecon 2022 - Improving GPU Utilization using Kubernetes</a></li>\n<li><a href=\"https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/overview.html\">https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/overview.html</a></li>\n</ul>","excerpt":"GPU는 강력한 연산 기능을 제공하지만 비용이 많이 들기 때문에 제한된 리소스를 효율적으로 활용하는 것이 중요합니다. 이번 글에서는 NVIDIA…"}}}},{"node":{"title":"Airflow worker에 KEDA AutoScaler 적용한 후기","id":"b10d61e7-5adc-53e4-b2da-9142ae8bffb5","slug":"airflow-worker-keda-autoscaler","publishDate":"June 24, 2022","heroImage":{"id":"dab22ea8-d54d-52a6-852a-278ba3b19a2b","title":"cover-dataengineering","fluid":{"aspectRatio":1.499531396438613,"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50 1600w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50&fm=webp 1600w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"ea004cc8-b136-57cb-b9c0-22e9a7a65cf3","childMarkdownRemark":{"id":"2a2780a6-435b-5112-9cc3-94f01693acb8","timeToRead":4,"html":"<p>Airflow에서 실행되는 배치 작업들은 특정 시간 또는 야간에 많이 수행되고 이외의 시간은 상대적으로 여유로운 경우가 많습니다. 이러한 상황에서 오토스케일링을 적용한다면 효율적으로 리소스를 최적화하여 사용할 수 있습니다.</p>\n<p>만약 쿠버네티스 위에서 Celery Executor를 사용한다면 worker의 오토스케일링을 위해 KEDA를 고려해볼 수 있습니다. 이 글에서는 Airflow worker에 KEDA AutoScaler를 적용하면서 겪었던 여러 문제들과 해결 과정에 대해 정리해보려 합니다.</p>\n<p><br><br></p>\n<h2 id=\"keda-autoscaler\" style=\"position:relative;\"><a href=\"#keda-autoscaler\" aria-label=\"keda autoscaler permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>KEDA AutoScaler</h2>\n<p>KEDA는 쿠버네티스에서 이벤트 기반 오토스케일링을 쉽게 구현할 수 있도록 지원하는 컴포넌트입니다. 쿠버네티스의 HPA와 함께 동작하며 다양한 built-in scaler를 통해 유연하게 오토스케일링 조건을 설정할 수 있습니다.</p>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1ASRyxTdtpbFqt6qGdRhD98OBSbTa-i8k\" alt=\"keda\"></p>\n<p>만약 Airflow에 적용한다면 위의 그림과 같은 형태로 구성됩니다.\n사용자는 KEDA의 <code class=\"language-text\">ScaledObject</code> CRD를 생성하여 클러스터에 배포합니다.\nKEDA는 쿠버네티스의 API Server와 통신하며 Operator와 같은 형태로써 컨트롤 루프에 따라 동작합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> keda.sh/v1alpha1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> ScaledObject\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> airflow<span class=\"token punctuation\">-</span>worker\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">scaleTargetRef</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> airflow<span class=\"token punctuation\">-</span>worker\n  <span class=\"token key atrule\">pollingInterval</span><span class=\"token punctuation\">:</span> <span class=\"token number\">10</span>\n  <span class=\"token key atrule\">cooldownPeriod</span><span class=\"token punctuation\">:</span> <span class=\"token number\">30</span>\n  <span class=\"token key atrule\">minReplicaCount</span><span class=\"token punctuation\">:</span> <span class=\"token number\">3</span>\n  <span class=\"token key atrule\">maxReplicaCount</span><span class=\"token punctuation\">:</span> <span class=\"token number\">10</span>\n  <span class=\"token key atrule\">triggers</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> postgresql\n      <span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">connectionFromEnv</span><span class=\"token punctuation\">:</span> AIRFLOW_CONN_AIRFLOW_DB\n        <span class=\"token key atrule\">query</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"\"</span></code></pre></div>\n<p><code class=\"language-text\">ScaledObject</code>는 위와 같이 무엇을 기준으로 트리거할지, 스케일링 정책 등을 정의할 수 있습니다. KEDA는 <code class=\"language-text\">minReplicaCount</code>에 따라 다르게 동작하는데 <code class=\"language-text\">minReplicaCount</code>가 0인 경우, KEDA가 trigger 지표를 통해 직접 처리하지만 1 이상인 경우에는 KEDA가 Metrics Server에 전달만하고 HPA를 통해 처리됩니다. 각 옵션에 대한 자세한 설명은 <a href=\"https://keda.sh/docs/2.7/concepts/scaling-deployments/\">공식 문서</a>에서 확인할 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">SELECT</span> ceil<span class=\"token punctuation\">(</span><span class=\"token function\">COUNT</span><span class=\"token punctuation\">(</span><span class=\"token operator\">*</span><span class=\"token punctuation\">)</span>::<span class=\"token keyword\">decimal</span> <span class=\"token operator\">/</span> {{ celery<span class=\"token punctuation\">.</span>worker_concurrency }}<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">FROM</span> task_instance\n<span class=\"token keyword\">WHERE</span> state<span class=\"token operator\">=</span><span class=\"token string\">'running'</span> <span class=\"token operator\">OR</span> state<span class=\"token operator\">=</span><span class=\"token string\">'queued'</span></code></pre></div>\n<p>Airflow에서 사용하는 <code class=\"language-text\">ScaledObject</code>의 트리거 쿼리는 위와 같이<code class=\"language-text\">celery.worker_concurrency</code> 설정을 기준으로 하고 있습니다. 예를 들어 concurrency 설정이 12이며 running 또는 queued 상태의 task instance가 10에서 23으로 증가한 상황이라고 가정해보겠습니다. desired state가 1에서 2로 변경되었기 때문에 deployment의 replica 수는 2로 확장 됩니다. 스케줄이 모두 종료된 이후 다시 task instance가 10으로 줄어들면 replica 수는 1로 축소 됩니다.</p>\n<p>Airflow 공식 차트에서는 KEDA 관련 옵션을 지원하고 있기 때문에 <a href=\"https://airflow.apache.org/docs/helm-chart/stable/keda.html\">공식 문서</a>를 통해 쉽게 적용할 수 있습니다.<br>\n하지만 문제는 적용한 이후에 발생했습니다.</p>\n<br>\n<h2 id=\"적용-후에-발생한-문제\" style=\"position:relative;\"><a href=\"#%EC%A0%81%EC%9A%A9-%ED%9B%84%EC%97%90-%EB%B0%9C%EC%83%9D%ED%95%9C-%EB%AC%B8%EC%A0%9C\" aria-label=\"적용 후에 발생한 문제 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>적용 후에 발생한 문제</h2>\n<p>적용 후에 실행 중인 task의 로그가 갑자기 끊기면서 강제로 실패 처리되는 문제가 있었습니다.<br>\n시간을 보니 worker가 Scale-In 되는 시점에 발생했고 크게 두 가지 문제를 확인할 수 있었습니다.</p>\n<br>\n<h3 id=\"1-hpa의-replica-flapping-문제\" style=\"position:relative;\"><a href=\"#1-hpa%EC%9D%98-replica-flapping-%EB%AC%B8%EC%A0%9C\" aria-label=\"1 hpa의 replica flapping 문제 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. HPA의 replica flapping 문제</h3>\n<p>먼저 의도한 것보다 Scale-In/Out이 너무 빈번하게 발생했습니다.\n새로 노드가 뜨는데 시간이 소요되므로 배치가 많은 시간 대에도 잦은 스케일 조정이 발생하는 것은 비효율적입니다. 이러한 문제를 HPA에서는 <strong>replica flapping</strong> 이라고 말합니다.\nHPA는 이를 제어하기 위해 <strong>안정화 윈도우와 스케일링 정책</strong>을 지원하고 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">behavior</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">scaleDown</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">stabilizationWindowSeconds</span><span class=\"token punctuation\">:</span> <span class=\"token number\">600</span></code></pre></div>\n<p>위와 같이 <code class=\"language-text\">stabilizationWindowSeconds</code> 설정을 600으로 설정하면 이전 10분 동안의 모든 목표 상태를 고려해서 가장 높은 값으로 설정합니다. 현재 시점에 scaleDown 조건을 만족하더라도 즉시 수행되는게 아니라 10분이 지난 시점에 scaleDown이 수행됩니다. 이를 통해 잦은 스케일 조정을 제한할 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">behavior</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">scaleDown</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">policies</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> Pods\n      <span class=\"token key atrule\">value</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n      <span class=\"token key atrule\">periodSeconds</span><span class=\"token punctuation\">:</span> <span class=\"token number\">300</span></code></pre></div>\n<p><code class=\"language-text\">scaleDown.polices</code>를 통해 Scale-In 발생 시 replica 변경 허용에 대한 정책을 지정할 수 있습니다. 위의 예시는 5분 내에 최대 1개의 replica를 scaleDown 하도록 허용하는 정책입니다. 이를 통해 계단식으로 천천히 pod를 축소할 수 있습니다.</p>\n<p>현재 Airflow 공식 차트에서는 KEDA의 advanced 옵션을 지원하지 않아 <a href=\"https://github.com/apache/airflow/pull/24220\">PR</a>을 추가했습니다.<br>\n차트 1.7 버전부터 사용하실 수 있습니다.</p>\n<br>\n<h3 id=\"2-worker-warm-shutdown-문제\" style=\"position:relative;\"><a href=\"#2-worker-warm-shutdown-%EB%AC%B8%EC%A0%9C\" aria-label=\"2 worker warm shutdown 문제 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Worker Warm Shutdown 문제</h3>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=10C7umomf0yxDRGav3STo5-uX3i5JUmdA\" alt=\"celery\"></p>\n<p>celery worker의 warm shutdown이 제대로 이루어지지 않았기 때문에 task의 로그가 갑자기 끊기면서 강제로 실패 했습니다. Airflow의 CeleryExecutor는 위와 같이 여러 프로세스를 통해 수행됩니다. 이 때 실제로 task를 실행하는 프로세스는 main 프로세스가 아니라 subprocess 입니다. celery에서는 실행 중인 task가 처리된 이후에 종료할 수 있도록 <strong>warm shutdown</strong>을 지원하고 있습니다. worker의 main process가 <code class=\"language-text\">SIGTERM</code>을 받으면 task가 종료될때까지 기다리게 됩니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"># warm shutdown log\nworker: Warm shutdown (MainProcess)\n\n -------------- celery@fcd56490a11f v4.4.7 (cliffs)\n--- ***** -----\n-- ******* ---- Linux-5.4.0-1045-aws-x86_64-with-debian-10.8\n- *** --- * ---\n- ** ---------- [config]\n- ** ---------- .&gt; app:         airflow.executors.celery_executor:0x7f95\n- ** ---------- .&gt; transport:   redis://redis:6379/0\n- ** ---------- .&gt; results:     postgresql://airflow:**@postgres/airflow\n- *** --- * --- .&gt; concurrency: 16 (prefork)\n-- ******* ---- .&gt; task events: OFF (enable -E to monitor tasks in this worker)\n--- ***** -----\n -------------- [queues]\n                .&gt; default          exchange=default(direct) key=default\n\n[tasks]\n  . airflow.executors.celery_executor.execute_command</code></pre></div>\n<p><a href=\"https://swalloow.github.io/container-tini-dumb-init/\">이전 글</a>에서 설명한 것처럼 Airflow 공식 차트에서 worker pod은 <code class=\"language-text\">DUMB_INIT_SETSID=0</code>으로 이미 설정되어 있기 때문에 메인 프로세스에만 SIGNAL이 전파되고 task process는 계속 실행됩니다. 하지만\n<strong>scaleDown이 발생한다면, 실행 중이던 worker pod이 종료되기 때문에 pod 내에 있던 task process도 함께 강제 종료되면서 task가 실패</strong>하게 됩니다. 장시간 수행되는 task 일수록 이러한 문제를 마주칠 가능성이 높습니다.</p>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1MO0UQlSLa2mJvYyNZBajcfzKN14dhufr\" alt=\"learnk8s\"></p>\n<p>이를 해결하기 위해 task의 execution_timeout 시간까지 pod가 종료되지 않도록 <code class=\"language-text\">terminationGracePeriodSeconds</code>를 지정해주었습니다. 이제 각 컨테이너 내부의 프로세스 1에 <code class=\"language-text\">SIGTERM</code>이 전달되더라도 pod의 graceful shutdown 시간 동안 대기하므로 task process는 계속 실행됩니다. 시간이 모두 지나면 <code class=\"language-text\">SIGKILL</code>을 통해 모든 프로세스가 종료되고 pod도 삭제됩니다.</p>\n<br>\n<h2 id=\"적용-후기\" style=\"position:relative;\"><a href=\"#%EC%A0%81%EC%9A%A9-%ED%9B%84%EA%B8%B0\" aria-label=\"적용 후기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>적용 후기</h2>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=16q46FTUBbbJRrccrGFLihodJKODGDW1v\" alt=\"test\"></p>\n<p>위의 문제들을 모두 수정한 이후부터 안정적으로 worker의 확장, 축소가 이루어졌습니다.<br>\n위 그림과 같이 개발 환경에 동시성 테스트를 위한 DAG을 먼저 만들어서 slot 지표에 따라 replica count가 어떻게 변화하는지 확인해본다면 안정적으로 적용할 수 있습니다.</p>\n<br>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h2>\n<ul>\n<li><a href=\"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#flapping\">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#flapping</a></li>\n<li><a href=\"https://learnk8s.io/graceful-shutdown\">https://learnk8s.io/graceful-shutdown</a></li>\n</ul>","excerpt":"Airflow…"}}}},{"node":{"title":"컨테이너 환경을 위한 초기화 시스템 (tini, dumb-init)","id":"bf47ab9d-ffa5-5ef8-b122-4a731c1ccf6d","slug":"container-tini-dumb-init","publishDate":"May 27, 2022","heroImage":{"id":"dab22ea8-d54d-52a6-852a-278ba3b19a2b","title":"cover-dataengineering","fluid":{"aspectRatio":1.499531396438613,"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50 1600w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50&fm=webp 1600w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"c81d0d2f-c1fe-507d-951c-fa83f8ce2010","childMarkdownRemark":{"id":"0cadd438-2c43-5bca-a5d1-3abec3fb9b65","timeToRead":5,"html":"<p>쿠버네티스 기반의 데이터플랫폼을 운영하다보면 이미지의 <code class=\"language-text\">ENTRYPOINT</code>에 <code class=\"language-text\">tini</code>, <code class=\"language-text\">dumb-init</code>과 같은 명령어를 사용하는 경우가 많습니다. 예를 들어 Airflow에서는 dumb-init을, SparkOperator에서는 tini를 사용하고 있습니다. 이 글에서는 컨테이너 환경에서 왜 이러한 초기화 시스템이 필요한지 알아보려 합니다.</p>\n<p><br><br></p>\n<h2 id=\"pid-1의-역할\" style=\"position:relative;\"><a href=\"#pid-1%EC%9D%98-%EC%97%AD%ED%95%A0\" aria-label=\"pid 1의 역할 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>PID 1의 역할</h2>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1ki_LL8QtIXzXWjRjX91HXWbUaNfXqk5S\" alt=\"top\"></p>\n<p>리눅스에서 <strong>PID 1은 부팅 시 커널에 의해 최초로 실행되는 init 프로세스</strong>입니다.\ninit 프로세스는 SSH 데몬, Docker 데몬, Apache/Nginx 시작 등과 같은 시스템들의 시작을 담당합니다. 각 프로세스는 차례로 추가 하위 프로세스를 생성할 수 있습니다. PID 1은 결국 모든 프로세스의 최종 부모 프로세스 역할을 하게 됩니다. 현재 배포판들은 복잡한 init 대신 systemd가 초기화 시스템의 역할을 대신하고 있습니다.</p>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=16ONXGJUMijn6HUHt4h1tAepWSQw0vDup\" alt=\"zombie\"></p>\n<p>여기까지는 일반적인 상황입니다. 만약 예기치 못한 상황으로 인해 프로세스가 종료되면 어떻게 될까요? bash(PID 5) 프로세스가 종료된다고 가정해보겠습니다. 5번은 이제 <strong>좀비 프로세스</strong>로 변합니다.</p>\n<p>왜 이런 일이 발생할까요? Unix는 부모 프로세스가 종료 상태를 수집하기 위해 자식 프로세스 종료를 명시적으로 대기하는 방식으로 설계되었기 때문입니다. 좀비 프로세스는 부모 프로세스가 시스템 호출의 <code class=\"language-text\">waitpid()</code> 시스템 명령을 수행할 때까지 존재합니다. 좀비를 제거하기 위해 자식 프로세스에서 <code class=\"language-text\">waitpid()</code>를 호출하는 작업을 <strong>reaping</strong>이라고 합니다.</p>\n<p>대부분의 경우 이러한 상황이 큰 문제가 되지 않습니다. 많은 어플리케이션이 자식 프로세스를 올바르게 가져옵니다. sshd를 사용하는 위의 예시에서 bash가 종료되면 운영 체제는 <code class=\"language-text\">SIGCHLD</code> 신호를 sshd에 보내 깨우게 합니다. sshd는 신호를 통해 인지하고 자식 프로세스를 거둡니다.</p>\n<p>하지만 부모 프로세스가 의도적으로 종료되거나 사용자가 프로세스를 종료시켰다고 가정해보겠습니다. 그러면 그 자식 프로세스들은 어떻게 될까요? 더 이상 상위 프로세스가 없으므로 <strong>고아 상태(orphaned)</strong>가 됩니다.</p>\n<p>init 프로세스는 이를 해결하기 위한 작업을 수행합니다. 바로 <strong>고아 상태가 된 자식 프로세스를 거두는 것(adopt)</strong> 입니다. init 프로세스에 의해 생성된 적이 없지만 프로세스의 부모가 되어 좀비 프로세스가 되지 않도록 정리해주는 역할을 합니다.</p>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1jHWw-uO9qO1QrnlLhr2KuiPIP14HHaQV\" alt=\"adopt\"></p>\n<p>백그라운드에서 실행되는 nginx 프로세스를 예시로 들어보겠습니다. 먼저 nginx는 자식 프로세스를 만듭니다. 그리고 nginx 프로세스가 종료됩니다. 고아가 된 nginx 자식 프로세스는 init 프로세스가 거두어들입니다.</p>\n<p>이러한 init 프로세스의 역할 덕분에 우리는 어플리케이션을 개발할 때 크게 신경쓰지 않게 되었습니다. 하지만 쿠버네티스를 포함한 컨테이너 환경의 경우, 조금 다릅니다.</p>\n<br>\n<h2 id=\"컨테이너-내부에서의-프로세스-동작\" style=\"position:relative;\"><a href=\"#%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88-%EB%82%B4%EB%B6%80%EC%97%90%EC%84%9C%EC%9D%98-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4-%EB%8F%99%EC%9E%91\" aria-label=\"컨테이너 내부에서의 프로세스 동작 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>컨테이너 내부에서의 프로세스 동작</h2>\n<p>도커는 컨테이너 ENTRYPOINT(CMD)로 명시된 프로세스를 PID 1로써 새로운 PID 네임스페이스에 정의합니다. 그리고 컨테이너 내부에 있는 PID 1 프로세스에만 신호를 보내 종료할 수 있습니다. 이러한 이유로 컨테이너는 경량화 이미지를 기반으로 단일 프로세스만 실행하는 경우가 많습니다. 두 가지 예시를 살펴보겠습니다.</p>\n<p><strong>1. sh 프로세스가 PID 1인 경우</strong><br>\nDockerfile을 통해 다음과 같은 컨테이너 명령을 지정하면 실행을 위해 쉘에 전달됩니다. 그 결과 아래와 같은 프로세스 트리가 생성됩니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">- docker run (on the host machine)\n  - /bin/sh (PID 1, inside container)\n    - python my_server.py (PID 2, inside container)</code></pre></div>\n<p>쉘을 PID 1로 사용하면 실제로 2번 프로세스에 signal를 보내는 것이 거의 불가능합니다. 쉘로 보낸 신호는 하위 프로세스로 전달되지 않으며 프로세스가 완료될 때까지 셸이 종료되지 않습니다. 이 경우 컨테이너를 종료하기 위해 SIGKILL을 보내야 합니다.</p>\n<p><strong>2. 내 프로세스가 PID 1인 경우</strong><br>\nDockerfile에서 다음과 같이 정의하면 프로세스가 즉시 시작되고 컨테이너의 초기화 시스템으로써 작동하여 다음과 같은 프로세스 트리가 생성됩니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">- docker run (on the host machine)\n  - python my_server.py (PID 1, inside container)</code></pre></div>\n<p>이러한 구조가 1번 예시보다 나은 방법입니다. 프로세스는 이제 실제로 보내는 신호를 수신합니다. 그러나 PID 1이므로 예상대로 응답하지 않을 수 있습니다.</p>\n<br>\n<h2 id=\"pid-1의-signal-propagation-문제\" style=\"position:relative;\"><a href=\"#pid-1%EC%9D%98-signal-propagation-%EB%AC%B8%EC%A0%9C\" aria-label=\"pid 1의 signal propagation 문제 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>PID 1의 Signal Propagation 문제</h2>\n<p>컨테이너 환경도 마찬가지로 PID 1은 초기화 시스템의 책임이 있습니다.\n일반적인 프로세스는 <code class=\"language-text\">TERM</code>에 대한 자체 handler를 등록하여 종료하기 전 cleanup을 수행할 수 있습니다. 프로세스가 signal handler를 등록하지 않은 경우, 커널은 일반적으로 <code class=\"language-text\">TERM</code> 신호에 대한 기본 동작인 프로세스 종료를 수행합니다.</p>\n<p>반면 PID 1은 <code class=\"language-text\">TERM</code> 신호에 대해 기본 동작으로 실행되지 않습니다. 따라서 signal handler를 등록하지 않은 경우, <code class=\"language-text\">TERM</code>은 프로세스에 아무런 영향도 미치지 못합니다.\n만약 자식 프로세스가 하위 프로세스를 생성하고 먼저 죽었다면, 컨테이너 상에 좀비 프로세스가 계속 쌓일 수 있습니다.</p>\n<p>docker run이 <code class=\"language-text\">SIGTERM</code>을 수신하면 컨테이너 자체가 죽지 않더라도 신호를 컨테이너로 전달한 다음 종료됩니다. docker stop 명령을 사용해도 마찬가지입니다. <code class=\"language-text\">TERM</code> signal을 보내고 10초 동안 기다린 다음 프로세스가 여전히 중지되지 않으면 KILL이 전송되어 정리할 기회 없이 즉시 중지됩니다.</p>\n<br>\n<h2 id=\"dumb-init\" style=\"position:relative;\"><a href=\"#dumb-init\" aria-label=\"dumb init permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>dumb-init</h2>\n<p>dumb-init은 이러한 문제를 해결하고 컨테이너를 일반 프로세스와 같은 형태로 사용할 수 있도록 지원하기 위해 만들어졌습니다. systemd과 달리 컨테이너에서 사용하기 위해 경량화된 형태로 개발된 초기화 시스템입니다. dumb-init을 사용하면 다음과 같은 프로세스 트리가 생성됩니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">- docker run (on the host machine)\n  - dumb-init (PID 1, inside container)\n    - python my_server.py (PID 2, inside container)</code></pre></div>\n<p>dumb-init은 모든 signal에 대해 signal handler를 등록하고 해당 signal을 프로세스 세션으로 전달합니다. 파이썬 프로세스는 더 이상 PID 1로 실행되지 않기 때문에 dumb-init이 <code class=\"language-text\">TERM</code>과 같은 신호를 전달할 때 handler를 등록하지 않아도 프로세스 종료가 가능합니다. dumb-init은 signal propagation 뿐만 아니라 고아 상태가 된 자식 프로세스를 거두는 역할(adopt)도 수행합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"docker\"><pre class=\"language-docker\"><code class=\"language-docker\"><span class=\"token keyword\">RUN</span> apt install dumb<span class=\"token punctuation\">-</span>init\n<span class=\"token keyword\">ENTRYPOINT</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"/usr/bin/dumb-init\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"--\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"/my/script\"</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p>사용 방법은 정말 간단합니다. 이미지에 바이너리를 설치하고 명령어 실행할 때 추가하면 됩니다.</p>\n<br>\n<h2 id=\"airflow-이미지에서-dumb-init-사용\" style=\"position:relative;\"><a href=\"#airflow-%EC%9D%B4%EB%AF%B8%EC%A7%80%EC%97%90%EC%84%9C-dumb-init-%EC%82%AC%EC%9A%A9\" aria-label=\"airflow 이미지에서 dumb init 사용 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Airflow 이미지에서 dumb-init 사용</h2>\n<p>Airflow도 dumb-init를 ENTRYPOINT에서 사용하고 있습니다. webserver, worker, scheduler pod에서 <code class=\"language-text\">bash -c ENTRYPOINT</code>를 사용하는데 bash는 자식에게 signal을 전달 안하기 때문에 dumb-init 사용이 필요합니다. 컨테이너 내에서는 환경변수를 통해 다르게 설정할 수 있도록 지원하고 있습니다. 설정 값의 차이는 아래와 같습니다.</p>\n<ul>\n<li><code class=\"language-text\">DUMB_INIT_SETSID=1</code> : 메인 프로세스 그룹의 모든 프로세스에 SIGNAL 전파</li>\n<li><code class=\"language-text\">DUMB_INIT_SETSID=0</code> : 메인 프로세스에만 SIGNAL 전파</li>\n</ul>\n<p>공식 차트에서 worker pod은 0으로 나머지는 1로 설정되어 있습니다.<br>\n이유는 Celery Worker의 warm shutdown을 지원하기 위해서 입니다. 특히 Airflow on Kubernetes 구성에서 CeleryExecutor를 사용하는 경우, task의 정상적인 종료를 위해 필요합니다. 이 부분은 다음 포스트에 이어서 정리해보겠습니다.</p>\n<br>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h2>\n<ul>\n<li><a href=\"https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/\">https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/</a></li>\n<li><a href=\"https://engineeringblog.yelp.com/2016/01/dumb-init-an-init-for-docker.html\">https://engineeringblog.yelp.com/2016/01/dumb-init-an-init-for-docker.html</a></li>\n<li><a href=\"https://github.com/Yelp/dumb-init\">https://github.com/Yelp/dumb-init</a></li>\n<li><a href=\"https://airflow.apache.org/docs/docker-stack/entrypoint.html\">https://airflow.apache.org/docs/docker-stack/entrypoint.html</a></li>\n</ul>","excerpt":"쿠버네티스 기반의 데이터플랫폼을 운영하다보면 이미지의 에 , 과 같은 명령어를 사용하는 경우가 많습니다. 예를 들어 Airflow에서는 dumb…"}}}},{"node":{"title":"EKS Karpenter를 활용한 Groupless AutoScaling","id":"573e9f5f-beea-54d9-aff8-918f477ae03a","slug":"eks-karpenter-groupless-autoscaling","publishDate":"May 13, 2022","heroImage":{"id":"f36c235f-3e3e-517d-bd80-697bc6183072","title":"cover-devops","fluid":{"aspectRatio":1.5,"src":"//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=1080&h=720&q=50 1080w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=1080&h=720&q=50&fm=webp 1080w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"ac3a803f-1cdb-5459-9902-a5c10a366151","childMarkdownRemark":{"id":"b1939eae-8e3c-597c-8e20-a18f9cad131d","timeToRead":5,"html":"<p>21년 12월 EKS에서 새로운 쿠버네티스 클러스터 오토스케일러인 <a href=\"https://aws.amazon.com/ko/blogs/korea/introducing-karpenter-an-open-source-high-performance-kubernetes-cluster-autoscaler/\">Karpenter</a>를 발표했습니다.<br>\n이후로 많은 사용자들이 오픈소스에 참여하면서 버전도 많이 올라갔고 안정적으로 사용하고 있습니다. 이 글에서는 Karpenter와 기존에 사용하던 Cluster AutoScaler를 비교하고 이관할 때 알아두면 좋은 내용에 대해 정리해보려 합니다.</p>\n<br>\n<h2 id=\"cluster-autoscaler가-가진-한계점\" style=\"position:relative;\"><a href=\"#cluster-autoscaler%EA%B0%80-%EA%B0%80%EC%A7%84-%ED%95%9C%EA%B3%84%EC%A0%90\" aria-label=\"cluster autoscaler가 가진 한계점 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Cluster AutoScaler가 가진 한계점</h2>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1qGi_2q3niqMYxOuKSUauMHzDE7JQEB_5\" alt=\"eksasg\"></p>\n<p>그 동안 EKS의 Cluster AutoScaler는 <strong>AWS의 AutoScaling Group(ASG)</strong>을 활용하고 있었습니다. ASG는 주기적으로 현재 상태를 확인하고 Desired State로 변화하는 방식으로 동작합니다. 사용자는 목적에 맞게 노드 그룹을 나누고 ASG의 Min, Max 설정을 통해 클러스터 노드 수를 제한할 수 있습니다. 이를 통해 기존 AWS 사용자가 직관적인 구조를 그대로 활용할 수 있었습니다. 하지만 클러스터의 규모가 커질수록 ASG 활용으로 인해 불편한 점이 존재했습니다.</p>\n<p><strong>1. 번거로운 ASG 노드 그룹 관리</strong><br>\nK8S 클러스터는 여러 조직이 함께 사용할 수 있는 멀티테넌트 구조를 지원합니다. 두 조직이 서비스의 안정적인 운영을 위해 노드 그룹을 격리해야 하는 요구사항이 생기면 EKS 운영자는 새로운 ASG 노드 그룹을 생성하고 관리해주어야 합니다. 많은 운영자가 EKS의 IaC 구현을 위해 <a href=\"https://github.com/terraform-aws-modules/terraform-aws-eks\">terraform-aws-eks</a> 모듈을 사용하는데 여기에 매번 설정을 업데이트하고 반영하는 일은 번거롭고 각 조직에게 역할을 위임하기도 애매합니다.</p>\n<p>또 다른 예시는 리소스 활용 목적에 따라 노드 그룹을 분리할 때 입니다. 많은 CPU가 필요한 워크로드는 컴퓨팅 최적화 인스턴스 유형을 사용하고 메모리가 필요한 워크로드는 메모리 최적화 인스턴스 유형을 사용하는 것이 효율적입니다. 그리고 비용 최적화를 위해 spot 인스턴스 유형을 사용할 수도 있습니다. 이를 구현하기 위해 ASG에서는 c타입, r타입, spot 인스턴스를 가지는 각 노드 그룹을 만들어주어야 합니다.</p>\n<p><strong>2. ASG로 인한 노드 프로비저닝 시간 지연</strong><br>\nEKS Cluster AutoScaler는 K8S의 Cluster AutoScaler에 ASG를 활용하여 AWS cloud provider를 구현한 형태입니다. 클러스터 내에서 어플리케이션 로드를 감지한 이후, 중간에 AWS 리소스 요청을 거치기 때문에 즉시 처리되기가 어렵습니다.</p>\n<br>\n<h2 id=\"karpenter-소개\" style=\"position:relative;\"><a href=\"#karpenter-%EC%86%8C%EA%B0%9C\" aria-label=\"karpenter 소개 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Karpenter 소개</h2>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1boQKLPL8qFLzZtkTwriGRH00PqjX1f3N\" alt=\"karpenter\"></p>\n<p>Karpenter는 다음과 같이 세 가지 컴포넌트로 구성되어 있습니다.</p>\n<ul>\n<li><strong>Controller</strong>: K8S controller 형태로 구현되어 pod 상태를 감시하고 노드 확장 및 축소</li>\n<li><strong>Webhook</strong>: Provisioner CRD에 대한 유효성 검사 및 기본값을 지정</li>\n<li><strong>Provisioner</strong>: Karpenter에 의해 생성되는 노드와 Pod에 대한 제약조건을 지정</li>\n</ul>\n<p><a href=\"https://github.com/aws/karpenter/tree/main/charts/karpenter\">Karpenter Helm Chart</a>를 통해 설치하면 controller와 webhook pod가 생성됩니다. 이후에 <a href=\"https://karpenter.sh/v0.10.0/provisioner/\">provisioner CRD</a>를 정의하고 클러스터에 배포하면 사용할 수 있습니다. provisioner는 ASG 노드 그룹과 유사한 개념입니다. 따라서 default를 사용하는게 아니라 기존에 사용하던 설정에 맞게 새로 만들어야 합니다. Scale In/Out 관련된 내용은 다음과 같습니다.</p>\n<h3 id=\"scale-out-기준\" style=\"position:relative;\"><a href=\"#scale-out-%EA%B8%B0%EC%A4%80\" aria-label=\"scale out 기준 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Scale Out 기준</h3>\n<ul>\n<li>pending 상태의 pod 수, 리소스 요청량에 따라 수행</li>\n<li>신규 노드가 15분 동안 NotReady 상태라면 종료하고 새로 생성</li>\n<li>kubernetes well-known label 설정 가능</li>\n</ul>\n<h3 id=\"scale-in-기준\" style=\"position:relative;\"><a href=\"#scale-in-%EA%B8%B0%EC%A4%80\" aria-label=\"scale in 기준 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Scale In 기준</h3>\n<ul>\n<li>노드에 예약된 pod가 없는 경우</li>\n<li>해당 노드에 대해 cordon, drain을 수행하고 삭제</li>\n<li><code class=\"language-text\">karpenter.sh/do-not-evict</code> 설정을 통해 보호 가능</li>\n</ul>\n<br>\n<h2 id=\"karpenter-vs-autoscaler\" style=\"position:relative;\"><a href=\"#karpenter-vs-autoscaler\" aria-label=\"karpenter vs autoscaler permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Karpenter vs AutoScaler</h2>\n<p>앞서 언급했던 Cluster AutoScaler와 Karpenter를 비교해보면 다음과 같습니다.</p>\n<p><strong>1. Provisioner API를 통해 간편한 노드 관리</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">requirements:\n  - key: &quot;node.kubernetes.io/instance-type&quot;\n    operator: In\n    values: [&quot;m5.large&quot;, &quot;m5.2xlarge&quot;]\n  - key: &quot;topology.kubernetes.io/zone&quot;\n    operator: In\n    values: [&quot;ap-northeast-2a&quot;, &quot;ap-northeast-2c&quot;]\n  - key: &quot;karpenter.sh/capacity-type&quot;\n    operator: In\n    values: [&quot;spot&quot;, &quot;on-demand&quot;]</code></pre></div>\n<p>Karpenter는 노드 프로비저닝을 위해 ASG 노드 그룹을 생성할 필요가 없습니다. 대신 yaml을 통해 Provisioner CRD만 생성하면 됩니다. 현재 노드 프로비저닝을 위한 instance type, subnet, volume, SG 등 대부분의 설정을 지원하고 있습니다.</p>\n<p><strong>2. 수 많은 인스턴스 유형에 대해 유연하게 처리</strong><br>\nKarpenter는 노드 프로비저닝을 위해 EC2 Fleet API를 사용합니다. 사용자는\n여러 유형의 인스턴스를 지정할 수 있으며 어떤 유형의 인스턴스를 생성할지는 Karpenter가 결정합니다. 예를 들어 pending 상태의 pod가 1CPU, 4GB 리소스를 요청한다면 m5.large 인스턴스를 생성합니다. spot 인스턴스의 경우, Fleet API의 최저 입찰 경쟁에 따라 저렴한 비용으로 사용할 수 있습니다.</p>\n<p><strong>3. 노드 프로비저닝 시간 단축</strong><br>\nKarpenter는 Cluster AutoScaler와 동일한 역할을 하지만 자체 구현된 오픈소스로 JIT(Just-In-Time)을 지원합니다. 적용한 이후 실제로 약 2배 정도 프로비저닝 시간이 단축되었습니다. Karpenter를 통해 생성된 노드는 pre-pulling을 통해 이미지를 미리 받아올 수 있으며 빠른 컨테이너 런타임 준비를 통해 pod를 즉시 바인딩할 수 있습니다.</p>\n<p>두 가지 AutoScaler는 여러 장단점이 존재하기 때문에 적절하게 선택할 필요가 있습니다. 데이터 영역에서 활용하는 클러스터는 다양한 인스턴스 유형을 사용하고 빈번하게 스케일 조정이 일어나는 경우가 많습니다. 따라서 Karpenter가 가지는 장점을 최대로 활용할 수 있습니다.</p>\n<br>\n<h2 id=\"karpenter-이관-가이드\" style=\"position:relative;\"><a href=\"#karpenter-%EC%9D%B4%EA%B4%80-%EA%B0%80%EC%9D%B4%EB%93%9C\" aria-label=\"karpenter 이관 가이드 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Karpenter 이관 가이드</h2>\n<p>최근에 <a href=\"https://karpenter.sh/v0.10.0/getting-started/migrating-from-cas/\">공식 이관 가이드</a>가 나와서 제가 사용했던 이관 방법들과 주의사항 위주로 정리해보았습니다.</p>\n<h3 id=\"이관-방법\" style=\"position:relative;\"><a href=\"#%EC%9D%B4%EA%B4%80-%EB%B0%A9%EB%B2%95\" aria-label=\"이관 방법 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>이관 방법</h3>\n<ul>\n<li>기존에 사용하던 설정들과 Scale In/Out에 대한 테스트는 karpenter 문서에서 안내하는 inflate pod을 통해 진행할 수 있습니다.</li>\n<li>Cluster AutoScaler의 일부 노드 그룹을 Provisioner로 이관하는 방식으로 진행하면 점진적으로 옮겨갈 수 있습니다.</li>\n<li>Provisioner yaml 설정에 익숙하지 않다면 <a href=\"https://karpenter.sh/v0.10.0/aws/launch-templates/\">launch template을 만들어 정의하는 방법</a>도 있습니다. 하지만 동일 설정이 있다면 Karpenter에서는 Provisioner yaml을 우선시하기 때문에 launch template 사용하는 방법을 권장하지 않습니다.</li>\n<li>Scale In에서 노드가 종료되는 시간을 조정하기 위해 TTL 설정을 사용하는 것이 좋습니다. TTL 설정이 너무 작으면 잠시 재시작하는 상황에서도 Scale In/Out이 빈번하게 발생할 수 있습니다.</li>\n<li>karpenter 관련 pod는 karpenter에 의해 생성된 노드에 띄울 수 없습니다. 따라서 ASG 노드 그룹이 적어도 하나는 존재해야 합니다.</li>\n</ul>\n<br>\n<h3 id=\"karpenter가-가지는-제한-사항\" style=\"position:relative;\"><a href=\"#karpenter%EA%B0%80-%EA%B0%80%EC%A7%80%EB%8A%94-%EC%A0%9C%ED%95%9C-%EC%82%AC%ED%95%AD\" aria-label=\"karpenter가 가지는 제한 사항 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Karpenter가 가지는 제한 사항</h3>\n<ul>\n<li>Karpenter에 의해 생성되는 노드는 현재 ASG max 설정과 같은 클러스터 상한선 기준이 없습니다. 따라서 노드에 대한 모니터링과 알림이 필요합니다. Karpenter에서는 프로메테우스 메트릭을 제공하고 있습니다.</li>\n<li>Karpenter의 Binpacking 로직은 VPC CNI 네트워크 사용을 가정하기 때문에 커스텀 CNI를 사용한다면 제대로 동작하지 않을 수 있습니다.</li>\n<li>0.10 이전 버전에서는 <code class=\"language-text\">podAffinity</code>, <code class=\"language-text\">podAntiAffinity</code>를 지원하지 않습니다. 따라서 하위 버전을 사용한다면 <code class=\"language-text\">nodeSelector</code>, <code class=\"language-text\">topologySpreadConstraints</code>를 활용하셔야 합니다.</li>\n</ul>\n<br>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h2>\n<ul>\n<li><a href=\"https://karpenter.sh/\">https://karpenter.sh/</a></li>\n<li><a href=\"https://aws.github.io/aws-eks-best-practices/karpenter/\">https://aws.github.io/aws-eks-best-practices/karpenter/</a></li>\n</ul>","excerpt":"21년 12월 EKS에서 새로운 쿠버네티스 클러스터 오토스케일러인 Karpenter…"}}}},{"node":{"title":"개발자가 의사결정을 기록하는 방법 (feat. ADR)","id":"89a1d9f0-062b-52b6-8ad6-66b6275cd043","slug":"feat-adr","publishDate":"December 04, 2021","heroImage":{"id":"1563c3af-a4e8-5db4-acb2-9bfd9fdb294d","title":"cover-develop","fluid":{"aspectRatio":1.5,"src":"//images.ctfassets.net/tushy4jlcik7/4W9SzEIJpHuwsUBnxSSypH/3a18765095ea5756c742b7adb83a0518/cover_develop.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/4W9SzEIJpHuwsUBnxSSypH/3a18765095ea5756c742b7adb83a0518/cover_develop.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/4W9SzEIJpHuwsUBnxSSypH/3a18765095ea5756c742b7adb83a0518/cover_develop.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/4W9SzEIJpHuwsUBnxSSypH/3a18765095ea5756c742b7adb83a0518/cover_develop.jpg?w=1800&h=1200&q=50 1800w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/4W9SzEIJpHuwsUBnxSSypH/3a18765095ea5756c742b7adb83a0518/cover_develop.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/4W9SzEIJpHuwsUBnxSSypH/3a18765095ea5756c742b7adb83a0518/cover_develop.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/4W9SzEIJpHuwsUBnxSSypH/3a18765095ea5756c742b7adb83a0518/cover_develop.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/4W9SzEIJpHuwsUBnxSSypH/3a18765095ea5756c742b7adb83a0518/cover_develop.jpg?w=1800&h=1200&q=50&fm=webp 1800w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/4W9SzEIJpHuwsUBnxSSypH/3a18765095ea5756c742b7adb83a0518/cover_develop.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"61cf49e6-420d-569a-85c0-4425ba1b08fe","childMarkdownRemark":{"id":"716ac876-fcb8-585f-ad39-b9244afecf29","timeToRead":2,"html":"<p>개발자들에게는 항상 다양한 선택지 중에 하나를 골라야 하는 상황이 주어집니다.<br>\n가장 간단한 예시로는 어떤 언어/프레임워크를 사용할지, 어느 버전을 사용할지에 대한 결정입니다.<br>\n오늘은 위와 같은 의사결정을 기록하기 위한 ADR에 대해 소개하려고 합니다.</p>\n<br>\n<h2 id=\"adr이란\" style=\"position:relative;\"><a href=\"#adr%EC%9D%B4%EB%9E%80\" aria-label=\"adr이란 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ADR이란?</h2>\n<p>ADR은 Architectural Decision Records의 약자로 <strong>아키텍쳐와 관련된 결정을 내렸을 때 그 과정을 기록해 두는 문서</strong>를 말합니다. 아마 ADR이라는 단어를 몰라도 큰 규모의 오픈소스를 사용하다보면 많이 접해보셨을거라 생각합니다. 예를 들어 Kubernetes 프로젝트에서는 개선 과제를 제안할 때 <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps\">KEP 템플릿</a>을 사용하여 문서를 작성하도록 가이드하고 있습니다.</p>\n<p>ADR은 간단한 양식을 통해 마크다운 형식으로 작성되며 문제 정의, 결정에 영향을 주는 기본 요구사항, 설계 결정 등의 내용이 포함되어 있습니다. GitHub, Spotify, Google 등 다양한 tech 기업들이 ADR형식을 사용하고 있습니다.</p>\n<br>\n<h2 id=\"adr이-필요한-이유\" style=\"position:relative;\"><a href=\"#adr%EC%9D%B4-%ED%95%84%EC%9A%94%ED%95%9C-%EC%9D%B4%EC%9C%A0\" aria-label=\"adr이 필요한 이유 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ADR이 필요한 이유</h2>\n<p>우리는 어떤 방식으로든 팀원들과 함께 의사결정하고 공유하게 됩니다. 그런데 새로운 팀원이 들어오고 히스토리에 대해 묻는다면 기억을 더듬어 당시에 왜 그렇게 했는지 설명합니다. 이러한 과정을 반복하며 시간을 낭비하고 있다면 ADR을 통해 해결할 수 있습니다. 많은 사람들이 말하는 ADR 도입의 장점은 아래와 같습니다.</p>\n<p><strong>명확하고 합리적인 의사결정을 내릴 수 있습니다</strong><br>\n정의된 ADR 템플릿에 따라 문서화하면 일관된 방식으로 의사결정할 수 있습니다.\n저자는 문서를 작성하는 과정에서 더 합리적인 결론을 도출해낼 수 있으며 독자는 문제에 대해 쉽게 이해할 수 있습니다.</p>\n<p><strong>새로운 팀원이 적응하는데 많은 도움이 됩니다</strong><br>\n새로운 팀원이 들어오면 새로운 개발환경, 아키텍쳐를 이해하기까지 많은 시간을 할애합니다.\n만약 ADR을 통해 과거 의사결정 과정까지 알게 된다면 더 쉽게 이해할 수 있습니다.</p>\n<br>\n<h2 id=\"adr-template\" style=\"position:relative;\"><a href=\"#adr-template\" aria-label=\"adr template permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ADR template</h2>\n<p>ADR 형식은 정하기 나름이지만 이 글에서는 가장 알려진 템플릿을 기준으로 설명하겠습니다.\n<a href=\"https://github.com/joelparkerhenderson/architecture-decision-record\">architecture-decision-record GitHub</a>에서 더 다양한 템플릿과 예시 문서를 확인할 수 있습니다.</p>\n<p><strong>1. Status</strong><br>\n<img src=\"http://drive.google.com/uc?export=view&#x26;id=1urCNDlBKQav3MhkzZzJX6kwwFEJMSjgj\" alt=\"adr-status\"></p>\n<p>먼저 status는 위와 같은 상태 다이어그램으로 표현되며 현재 문서의 상태를 나타냅니다.</p>\n<p><strong>2. Context</strong><br>\nContext는 해결하고자 하는 문제를 정의하는 목차입니다.</p>\n<p><strong>3. Decision</strong><br>\n<img src=\"http://drive.google.com/uc?export=view&#x26;id=1CnhoR26bTnQfpaarLFjmSrs8-7mbjWLP\" alt=\"adt-table\"></p>\n<p>Decision에서는 제안하고자 하는 내용 및 해당 결정의 이유에 대해 설명합니다.<br>\n의사결정 과정에서 고려했던 대안들과 장단점에 대한 내용도 포함됩니다.<br>\n위와 같이 간단히 비교하는 표를 추가한다면 읽는 사람들이 더 쉽게 이해할 수 있습니다.</p>\n<p><strong>4. Consequences</strong><br>\nConsequences에서는 결정을 통해 사용자가 받는 영향에 대해 정의합니다.\n예를 들어 이 결정이 도입된다면 어떤 효과가 나타날 수 있는지, 마이그레이션 과제라면 다운타임이 발생하는지, 사용자들의 코드 변경이 필요한지 등을 작성합니다.</p>\n<br>\n<p>새로 도입할 때는 ADR이 부담스러운 업무가 되지 않도록 가능한 가볍게 유지할 수 있어야 합니다.<br>\nADR은 나를 위한 것이 아니라 현재 그리고 미래의 팀원들을 위한 것이라고 합니다.<br>\n그 동안 문서로 작성 안했다면 아주 간단하게 시작해보는건 어떨까요?</p>\n<br>\n<h2 id=\"참고-자료\" style=\"position:relative;\"><a href=\"#%EC%B0%B8%EA%B3%A0-%EC%9E%90%EB%A3%8C\" aria-label=\"참고 자료 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>참고 자료</h2>\n<ul>\n<li><a href=\"https://github.blog/2020-08-13-why-write-adrs/\">GitHub Blog: Why write ADRs?</a></li>\n<li><a href=\"https://cloud.google.com/architecture/architecture-decision-records\">Google Cloud Architecture ADR Docs</a></li>\n<li><a href=\"https://github.com/alphagov/govuk-aws/tree/master/docs/architecture/decisions\">govuk-aws ADR docs example</a></li>\n</ul>","excerpt":"…"}}}},{"node":{"title":"JupyterHub on Kubernetes","id":"784b5b9c-9d53-5ffd-9cb1-e61df96893dd","slug":"jupyterhub-on-kubernetes","publishDate":"October 23, 2021","heroImage":{"id":"dab22ea8-d54d-52a6-852a-278ba3b19a2b","title":"cover-dataengineering","fluid":{"aspectRatio":1.499531396438613,"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50 1600w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50&fm=webp 1600w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"861d6065-30d6-5b6e-9d02-e2e7e4f7ddea","childMarkdownRemark":{"id":"ccc6a49d-08cc-55e8-90e0-5dece449d643","timeToRead":4,"html":"<p>일반적으로 JupyterHub를 Kubernetes 환경에 배포할 때 Helm Chart를 많이 사용합니다.<br>\n이 글에서는 <a href=\"https://github.com/jupyterhub/zero-to-jupyterhub-k8s\">zero-to-jupyterhub-k8s Helm Chart</a>에 포함된 다양한 기능들에 대해 소개해보려 합니다.</p>\n<p><strong>목차</strong></p>\n<ul>\n<li><a href=\"https://swalloow.github.io/jupyterhub-on-kubernetes/#kubespawner\">kubespawner</a></li>\n<li><a href=\"https://swalloow.github.io/jupyterhub-on-kubernetes/#zero-to-jupyterhub-k8s-chart\">zero-to-jupyterhub-k8s chart</a></li>\n<li><a href=\"https://swalloow.github.io/jupyterhub-on-kubernetes/#proxy\">proxy</a></li>\n<li><a href=\"https://swalloow.github.io/jupyterhub-on-kubernetes/#singleuser-profile\">singleuser, profile</a></li>\n<li><a href=\"https://swalloow.github.io/jupyterhub-on-kubernetes/#idle-culler\">idle-culler</a></li>\n<li><a href=\"https://swalloow.github.io/jupyterhub-on-kubernetes/#user-scheduler\">user-scheduler</a></li>\n<li><a href=\"https://swalloow.github.io/jupyterhub-on-kubernetes/#image-pre-puller\">image-pre-puller</a></li>\n<li><a href=\"https://swalloow.github.io/jupyterhub-on-kubernetes/#monitoring\">monitoring</a></li>\n</ul>\n<p><br><br></p>\n<h2 id=\"kubespawner\" style=\"position:relative;\"><a href=\"#kubespawner\" aria-label=\"kubespawner permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>KubeSpawner</h2>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1OaKRO1t73IJoFz267WhZGspKHIvzztFB\" alt=\"jhub\"></p>\n<p>먼저 JupyterHub의 기본 아키텍쳐에 대해 간단히 짚고 넘어가보겠습니다.\nJupyterHub에는 노트북 서버를 다양한 방법을 통해 프로비저닝하기 위해 <strong>Spawner</strong>라는 인터페이스가 존재합니다. K8S 환경이라면 KubeSpawner를 사용하게 됩니다. 이를 통해 프로비저닝 단계에서 kube-scheduler 기반으로 다양한 K8S 리소스를 노트북 서버와 함께 사용할 수 있습니다. 또한 KubeSpawner는 <strong>사용자에게 격리된 환경과 컴퓨팅 리소스를 제공</strong>할 수 있습니다.</p>\n<p><strong>노트북 Pod 생성 이벤트</strong><br>\n노트북이 생성되는 과정은 다음과 같습니다.</p>\n<ul>\n<li>할당 가능한 노드 탐색 (NodeSelector, Affinity)</li>\n<li>없으면 CA에 의해 노드 추가, 노드가 Ready 상태가 될 때까지 대기</li>\n<li>Pod을 추가된 노드에 할당</li>\n<li>노트북 이미지 pull</li>\n<li>노트북 컨테이너 실행</li>\n</ul>\n<br>\n<h2 id=\"zero-to-jupyterhub-k8s-chart\" style=\"position:relative;\"><a href=\"#zero-to-jupyterhub-k8s-chart\" aria-label=\"zero to jupyterhub k8s chart permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>zero-to-jupyterhub-k8s Chart</h2>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1j1PF9_mQPo3pic6ywAsG3wxOABVxshf-\" alt=\"jhub-chart-arc\"></p>\n<p><a href=\"https://github.com/jupyterhub/zero-to-jupyterhub-k8s\">zero-to-jupyterhub-k8s Helm Chart</a> 의 아키텍쳐는 위의 그림과 같습니다. 기존 JupyterHub와 달리 hook-image-awaiter, jupyterhub-idle-culler 등의 컴포넌트가 추가된 모습을 확인하실 수 있습니다. 이제 대략적으로 어떤 기능을 제공하는지 알아보겠습니다.</p>\n<br>\n<h2 id=\"proxy\" style=\"position:relative;\"><a href=\"#proxy\" aria-label=\"proxy permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Proxy</h2>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">proxy</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">service</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> ClusterIP\n  <span class=\"token key atrule\">chp</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">networkPolicy</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">enabled</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">false</span></code></pre></div>\n<p>먼저 CHP(configurable-http-proxy) 설정 부분입니다. JupyterHub에서 <strong>Proxy는 인증, 사용자 노트북 라우팅, 헬스 체크 등 다양한 역할을 수행</strong>합니다. 차트에서는 유연한 Proxy 설정을 위해 CHP, Traefik 등 다양한 옵션을 지원합니다. 아키텍쳐는 aws-load-balancer-controller를 사용한다는 가정하에 구성한 예시입니다. 위 그림과 같이 사용자는 중간의 Proxy 컴포넌트를 거쳐 JupyterHub에 접속하게 됩니다.</p>\n<br>\n<h2 id=\"singleuser-profile\" style=\"position:relative;\"><a href=\"#singleuser-profile\" aria-label=\"singleuser profile permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>SingleUser, Profile</h2>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1C60z5F13WvN3HzsFkv2A09Ja8yMzIGtL\" alt=\"spawner-op\"></p>\n<p>singleUser는 사용자의 노트북 환경을 의미하며 사용자는 미리 정의된 프로필(이미지)을 선택하여 원하는 노트북 환경을 생성할 수 있습니다. 위 아키텍쳐에서는 <strong>PV, PVC를 통해 사용자에게 개인, 공용 볼륨을 할당</strong>해주었습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">profileList</span><span class=\"token punctuation\">:</span>\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">display_name</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"Python Notebook\"</span>\n    <span class=\"token key atrule\">description</span><span class=\"token punctuation\">:</span> <span class=\"token key atrule\">\"Spec</span><span class=\"token punctuation\">:</span> CPU 2<span class=\"token punctuation\">,</span> Memory 4G / Spark 3.1\"\n    <span class=\"token key atrule\">kubespawner_override</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> jupyter/python<span class=\"token punctuation\">-</span>notebook<span class=\"token punctuation\">:</span>hub<span class=\"token punctuation\">-</span>1.4.2\n      <span class=\"token key atrule\">cpu_limit</span><span class=\"token punctuation\">:</span> <span class=\"token number\">2</span>\n      <span class=\"token key atrule\">mem_limit</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"4G\"</span>\n      <span class=\"token key atrule\">cpu_guarantee</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n      <span class=\"token key atrule\">mem_guarantee</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"2G\"</span>\n      <span class=\"token key atrule\">environment</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">TZ</span><span class=\"token punctuation\">:</span> Asia/Seoul\n      <span class=\"token key atrule\">lifecycle_hooks</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">postStart</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">exec</span><span class=\"token punctuation\">:</span>\n            command<span class=\"token punctuation\">:</span></code></pre></div>\n<p>프로필에는 리소스 뿐만 아니라 lifecycle_hook, environment 등 K8S의 다양한 리소스를 함께 정의하여 유연하게 구성할 수 있습니다. 노트북 기본 이미지는 <a href=\"https://github.com/jupyter/docker-stacks\">jupyter/docker-stacks</a> 저장소로부터 생성한다면 편하게 패키지 의존성을 관리할 수 있습니다.</p>\n<p><strong>resource guarantee</strong><br>\nresource guarantee는 모든 사용자가 최소한 <code class=\"language-text\">_guarantee</code> 만큼의 리소스를 사용할 수 있으며 최대 <code class=\"language-text\">_limit</code> 만큼의 리소스를 제공받을 수 있음을 의미합니다. 예를 들어 사용자에게 2G의 RAM이 보장되는 경우, 사용자는 2G 이상의 RAM을 사용할 수 있습니다. 문서에서는 guarantee 값을 limit의 반으로 설정하는 것을 권장하고 있습니다.</p>\n<br>\n<h2 id=\"idle-culler\" style=\"position:relative;\"><a href=\"#idle-culler\" aria-label=\"idle culler permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Idle Culler</h2>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">cull</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">enabled</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n  <span class=\"token key atrule\">timeout</span><span class=\"token punctuation\">:</span> <span class=\"token number\">86400</span>\n  <span class=\"token key atrule\">every</span><span class=\"token punctuation\">:</span> <span class=\"token number\">600</span>\n  <span class=\"token key atrule\">concurrency</span><span class=\"token punctuation\">:</span> <span class=\"token number\">10</span></code></pre></div>\n<p>idle-culler는 일정 주기 동안 미사용된 노트북 리소스를 정리합니다.\n이를 통해 노트북 리소스를 최적화하여 운영할 수 있습니다.\nidle-culler를 활성화하면 JupyterHub Service에 등록되며 이후 JupyterHub API를 통해 사용자 활동을 주기적으로 확인합니다.</p>\n<br>\n<h2 id=\"user-scheduler\" style=\"position:relative;\"><a href=\"#user-scheduler\" aria-label=\"user scheduler permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>User Scheduler</h2>\n<p>user scheduler는 노트북 리소스를 적절한 노드에 할당하기 위해 추가되었습니다.\n기본 K8S 스케줄러는 여러 노드에 분산하여 리소스를 할당하지만, user scheduler는 가장 리소스를 많이 점유하고 있는 노드에 리소스를 할당합니다. 이를 통해 <strong>Cluster AutoScaler, idle-culler와 연계하여 노트북 리소스를 최적화하여 운영</strong>할 수 있습니다.</p>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1XIgeSDAIxdXAvM0V2y1wQJvlwGoQOnwB\" alt=\"user-scheduler\"></p>\n<p>예를 들어 일반적인 설정이라면, pod가 다양한 노드에 분산되어 클러스터 scale-in 조건까지 도달하기가 어렵습니다. 하지만 user-scheduler를 사용한다면, 위 그림과 같이 노드에 할당된 pod의 수가 점진적으로 줄어들게 됩니다.</p>\n<br>\n<h2 id=\"image-pre-puller\" style=\"position:relative;\"><a href=\"#image-pre-puller\" aria-label=\"image pre puller permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Image Pre Puller</h2>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">prePuller</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">requests</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> 10m\n      <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> 8Mi\n  <span class=\"token key atrule\">hook</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">enabled</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n    <span class=\"token key atrule\">pullOnlyOnChanges</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span></code></pre></div>\n<p>Image prePuller는 사용자가 노트북을 실행하기 전에 노드에 미리 이미지를 준비하여 노트북 환경 생성 시간을 단축시켜 줍니다. 예를 들어 CA에 의해 노드가 새로 추가된다거나 새로운 이미지가 프로필에 등록된 경우, 미리 노드에 프로필 이미지를 pull 하게 됩니다.</p>\n<br>\n<h2 id=\"monitoring\" style=\"position:relative;\"><a href=\"#monitoring\" aria-label=\"monitoring permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Monitoring</h2>\n<p>JupyterHub는 <code class=\"language-text\">/metrics</code> endpoint를 통해 prometheus 메트릭을 지원합니다. 주요 지표로는 활성 사용자 수, 노트북 서버 생성까지 소요되는 시간 등이 있습니다. 사용 가능한 전체 메트릭은 <a href=\"https://jupyterhub.readthedocs.io/en/stable/reference/metrics.html\">JupyterHub 문서</a>에서 확인하실 수 있습니다.\n또한  <a href=\"https://github.com/jupyterhub/grafana-dashboards\">jupyterhub/grafana-dashboards</a> 저장소를 통해 미리 정의된 운영 대시보드를 제공합니다. 이를 통해 쉽게 모니터링을 구성할 수 있습니다.</p>\n<br>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h2>\n<ul>\n<li><a href=\"https://zero-to-jupyterhub.readthedocs.io/en/latest/index.html\">https://zero-to-jupyterhub.readthedocs.io/en/latest/index.html</a></li>\n<li><a href=\"https://jupyterhub.readthedocs.io/en/stable/index.html\">https://jupyterhub.readthedocs.io/en/stable/index.html</a></li>\n</ul>","excerpt":"일반적으로 JupyterHub를 Kubernetes 환경에 배포할 때 Helm Chart를 많이 사용합니다.\n이 글에서는 zero-to…"}}}}]}},"pageContext":{"basePath":"","paginationPath":"","pageNumber":0,"humanPageNumber":1,"skip":0,"limit":7,"numberOfPages":16,"previousPagePath":"","nextPagePath":"/2"}}}