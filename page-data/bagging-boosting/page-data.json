{"componentChunkName":"component---src-templates-post-js","path":"/bagging-boosting/","result":{"data":{"contentfulPost":{"title":"Bagging과 Boosting 그리고 Stacking","slug":"bagging-boosting","metaDescription":null,"publishDate":"July 19, 2017","publishDateISO":"2017-07-19","tags":[{"title":"DataScience","id":"82931dd3-d22b-528e-8a9b-ddbb200bb401","slug":"datascience"}],"heroImage":{"title":"cover-datascience","gatsbyImageData":{"images":{"sources":[{"srcSet":"https://images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=450&h=300&q=50&fm=webp 450w,\nhttps://images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=900&h=600&q=50&fm=webp 900w,\nhttps://images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&h=1200&q=50&fm=webp 1800w","sizes":"(min-width: 1800px) 1800px, 100vw","type":"image/webp"}],"fallback":{"src":"https://images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&h=1200&fl=progressive&q=50&fm=jpg","srcSet":"https://images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=450&h=300&fl=progressive&q=50&fm=jpg 450w,\nhttps://images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=900&h=600&fl=progressive&q=50&fm=jpg 900w,\nhttps://images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&h=1200&fl=progressive&q=50&fm=jpg 1800w","sizes":"(min-width: 1800px) 1800px, 100vw"}},"layout":"constrained","width":1800,"height":1200,"placeholder":{"fallback":"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEASABIAAD/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wAARCAANABQDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAQBAwUG/8QAHxAAAgEEAgMAAAAAAAAAAAAAAQIAAwQRIQVCExSR/8QAFgEBAQEAAAAAAAAAAAAAAAAAAQAC/8QAFhEBAQEAAAAAAAAAAAAAAAAAAQAR/9oADAMBAAIRAxEAPwCthE7ioFIBOznEy05C8uWI8wpjHVZCF3uFSpUd99m18kpIXSYyTqES96ouiFMJnScb/9k="}},"ogimg":{"src":"https://images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&q=50"}},"body":{"childMarkdownRemark":{"timeToRead":2,"html":"<p>오늘은 머신러닝 성능을 최대로 끌어올릴 수 있는 앙상블 기법에 대해 정리해보았습니다.</p>\n<br>\n<h2 id=\"ensemble-hybrid-method\" style=\"position:relative;\"><a href=\"#ensemble-hybrid-method\" aria-label=\"ensemble hybrid method permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Ensemble, Hybrid Method</h2>\n<p>앙상블 기법은 동일한 학습 알고리즘을 사용해서 여러 모델을 학습하는 개념입니다.\nWeak learner를 결합한다면, Single learner보다 더 나은 성능을 얻을 수 있다는 아이디어입니다.\n<strong>Bagging</strong> 과 <strong>Boosting</strong> 이 이에 해당합니다.</p>\n<p>동일한 학습 알고리즘을 사용하는 방법을 앙상블이라고 한다면,\n서로 다른 모델을 결합하여 새로운 모델을 만들어내는 방법도 있습니다.\n대표적으로 <strong>Stacking</strong> 이 있으며, 최근 Kaggle 에서 많이 소개된 바 있습니다.</p>\n<br>\n<h2 id=\"bagging\" style=\"position:relative;\"><a href=\"#bagging\" aria-label=\"bagging permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Bagging</h2>\n<p>Bagging은 샘플을 여러 번 뽑아 각 모델을 학습시켜 결과를 <strong>집계(Aggregating)</strong> 하는 방법입니다. 아래의 그림을 통해 자세히 알아보겠습니다.</p>\n<p><span\n        class=\"gatsby-resp-image-wrapper\"\n        style=\"position: relative; display: block; ; max-width: 650px; margin-left: auto; margin-right: auto;\"\n      >\n        <span\n          class=\"gatsby-resp-image-background-image\"\n          style=\"padding-bottom: 67.3768308921438%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAAbCAMAAAA5zj1cAAACVVBMVEX////q8vrW5vT4+/2tzepbm9Xl7/h5rd1zqdt3rNx5rt16rt1tptpvp9pqpNlyqdtlode00exppNno8fn2+v39/v/6/P75+/33+v3v9fv2+f38/f71+f37/f7y9/z0+Pzx9/v+/v76+vrt8vf7+/v9/f2IiIiBgYF/f39+fn7X19eAgIB/f4B8fHyFhYXOzs6QkJCXl5eSkpKWlpaLi4uDg4OVlZWTk5OJiYmEhISUlJSZmZmMjIyRkZGNjY2Pj4+CgoKYmJh9fX3R0dHNzc3w8PDx8fHU1NTy8vLv7+/b29v/3nj//fX/227/1FL/3Xb/9tv/++3/4ID/1FD/+eb///7/0UP//PT//vz/1FP//vn/3XX/9NT//PH/1lj/993/1llfzfZu0vZr0fZYy/XK7/xUyvVx0/do0PZs0vZWy/Vy0/fI7vxNyPVmz/Zjz/ZGxvTG7fxQyfVkz/ZezfZSyvW66vtSyfVlz/ZazPUcufIlu/IrvfMtvvMXt/Gx5/oNtPEmvPIvvvMStvEPtfElvPIxv/MsvvMWt/Eku/IywPMZuPEju/I0wPMqvfIeufKg4fkdufIhuvI1wfMovfIYt/EfuvILtPEiu/IVt/EOtPEguvITtvELs/EiuvIVtvEbufIauPIRtfFPyfVdzfXJ7vzM7/zN8PzC7PtczfV00/b9/v7w9vvstLTaaWnXZ2j55+fff3/LKyvKKSnIICDKJyfHHBz119fWWVnQQ0PONzfPPDzHHh7EERHQQUHMMDDPPT3KKirDCwv8/Pzqs7PYamr25eX9WfNmAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAAB3RJTUUH6AEUDTMrhNfVFAAAAbFJREFUGBmNwb1u01AAhuH3Oz5O7PyUliYgKFLLUi6CASgjoRSkCoTEwAUxcB+wMrIAM2wMMFSqqBAVFEVtfhzbh+OkIPkoA88j6iQqKghY6iJRUUHAEjCqZIQsgQZyRhkhQ11XsnJll//QYxkBfeFJLEgsZFSa36hYoCk8TTgXY0d4jSO8HnMCtg7wtg5YuC7kcELOqEDOfAXEtoQklEuFpUQyuaWQya00MaZsfgIstCSNWlIeDy8WRWnPZj3Goygt0/g47asYOzxDEqudcWpdJ2FoV6I2V/NpZ4MsnkUJw+m4TUXcFSCHJ4dnhCcHHG6WQPSGhXsDvPtUdvEe7FHZxXv4iDkDJAmwnz4GnrSfAp0O3rNVvJUV/hlQGbDcgDnxXEILM8UTooak4QVJeSwPjV4Clis6lzXzWNLUpVI3mXX01wjPsiHy6enlSVpGR5fGSnXYP+7/Xv9+tvlzXSdr/PiV4Fk+3pLDlB05tsv07Q43ymsfbrJa8PnO+9s5X3aEJ3oETtaoab3YZ6lXLCPqXgs5MHsELHVWUhlJhCyhWN6MkCXQVmVGSO+okaioIPAHGSR3NgT0ZroAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n        >\n          <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\"\n        alt=\"boosting\"\n        title=\"\"\n        src=\"https://images.ctfassets.net/tushy4jlcik7/2Qi9V8VS1XzgWdLT3UiSpk/fe2d857d275ee85b2122f350bc18c718/boosting.png\"\n        srcset=\"https://images.ctfassets.net/tushy4jlcik7/2Qi9V8VS1XzgWdLT3UiSpk/fe2d857d275ee85b2122f350bc18c718/boosting.png?w=188 188w,\nhttps://images.ctfassets.net/tushy4jlcik7/2Qi9V8VS1XzgWdLT3UiSpk/fe2d857d275ee85b2122f350bc18c718/boosting.png?w=376 376w,\nhttps://images.ctfassets.net/tushy4jlcik7/2Qi9V8VS1XzgWdLT3UiSpk/fe2d857d275ee85b2122f350bc18c718/boosting.png?w=751 751w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        loading=\"lazy\"\n      />\n        </span>\n      </span></p>\n<p>먼저 대상 데이터로부터 복원 랜덤 샘플링을 합니다.\n이렇게 추출한 데이터가 일종의 표본 집단이 됩니다.\n이제 여기에 동일한 모델을 학습시킵니다.\n그리고 학습된 모델의 예측변수들을 집계하여 그 결과로 모델을 생성해냅니다.</p>\n<p>이러한 방식을 <strong>Bootstrap Aggregating</strong> 이라고 부릅니다.</p>\n<p>이렇게 하는 이유는 \"알고리즘의 안정성과 정확성을 향상시키기 위해서\" 입니다.\n대부분 학습에서 나타나는 오류는 다음과 같습니다.</p>\n<ol>\n<li>높은 bias로 인한 Underfitting</li>\n<li>높은 Variance로 인한 Overfitting</li>\n</ol>\n<p>앙상블 기법은 이러한 오류를 최소화하는데 도움이 됩니다.\n특히 Bagging은 각 샘플에서 나타난 결과를 일종의 중간값으로 맞추어 주기 때문에,\nOverfitting을 피할 수 있습니다.</p>\n<p>일반적으로 Categorical Data인 경우, 투표 방식 (Voting)으로 집계하며\nContinuous Data인 경우, 평균 (Average)으로 집계합니다.</p>\n<p>대표적인 Bagging 알고리즘으로 <code class=\"language-text\">RandomForest</code> 모델이 있습니다.\n원래 단일 DecisionTree 모델은 boundary가 discrete 한 모양일 수 밖에 없지만,\nRandomForest는 여러 트리 모델을 결합하여 이를 넘어설 수 있게 되었습니다.</p>\n<p>결과는 아래와 같습니다.</p>\n<p><span\n        class=\"gatsby-resp-image-wrapper\"\n        style=\"position: relative; display: block; ; max-width: 650px; margin-left: auto; margin-right: auto;\"\n      >\n        <span\n          class=\"gatsby-resp-image-background-image\"\n          style=\"padding-bottom: 110.00000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEAYABgAAD/2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4+JS5ESUM8SDc9Pjv/2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv/wAARCAAsACgDASIAAhEBAxEB/8QAGgAAAwADAQAAAAAAAAAAAAAAAAMEAQIFB//EAC0QAAIBAwIFAgUFAQAAAAAAAAECAwAEERIhEyIxQWEFUTKBkbHwFBVicaHx/8QAFwEBAQEBAAAAAAAAAAAAAAAAAAECA//EABoRAQEAAgMAAAAAAAAAAAAAAAABESECMVH/2gAMAwEAAhEDEQA/APUTcTJxDKHCqX3DfyOn/MVhZ53VtLFgHA1KxO3/AHFVzenxTsWZnz2wenv96XF6cYhIOJyuc8igEHOc71WcbSyXrIV30k5AHEOx8/UGnPNKsoIVmQqunmIG/U9d/nWsvp8XFEcUsiOyndl1bbdvmPpVD+nrLEscjyNpAGcgZ6+PNZnbpyss0ntblnulhfPxMMhyenTv4opsPpfAuUljuGAQYK6Rv7/mKK3b4y6FJu2kS1kaLOsLtinVpLpMZV20hhjNZEUt26zISApBK48fgFWQSGSIM2M98VzZrZf1EaRuTG6trkLLynG2R3zk1dZx8KIxmfjEHOcAY8VMxbFFFFFVBSLxWa0lVCASvf270+sMAylTuCMGg5kt04YZc5Dn4Qdhg1dbMXt1ZjqJ7+9L/b4N8lztgc52/qmwQJbx8OPVjOeZi33oG0UUUH//2Q=='); background-size: cover; display: block;\"\n        >\n          <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\"\n        alt=\"agg result\"\n        title=\"\"\n        src=\"https://images.ctfassets.net/tushy4jlcik7/24IAgyNrQgeMlBCt81CroN/3d82eca18a1289219478efdbd24fbe2f/agg_result.png\"\n        srcset=\"https://images.ctfassets.net/tushy4jlcik7/24IAgyNrQgeMlBCt81CroN/3d82eca18a1289219478efdbd24fbe2f/agg_result.png?w=240 240w,\nhttps://images.ctfassets.net/tushy4jlcik7/24IAgyNrQgeMlBCt81CroN/3d82eca18a1289219478efdbd24fbe2f/agg_result.png?w=480 480w,\nhttps://images.ctfassets.net/tushy4jlcik7/24IAgyNrQgeMlBCt81CroN/3d82eca18a1289219478efdbd24fbe2f/agg_result.png?w=960 960w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        loading=\"lazy\"\n      />\n        </span>\n      </span></p>\n<br>\n<h2 id=\"boosting\" style=\"position:relative;\"><a href=\"#boosting\" aria-label=\"boosting permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Boosting</h2>\n<p>Bagging이 일반적인 모델을 만드는데 집중되어있다면,\nBoosting은 맞추기 어려운 문제를 맞추는데 초점이 맞춰져 있습니다.</p>\n<p>수학 문제를 푸는데 9번 문제가 엄청 어려워서 계속 틀렸다고 가정해보겠습니다.\nBoosting 방식은 9번 문제에 가중치를 부여해서 9번 문제를 잘 맞춘 모델을 최종 모델로 선정합니다.\n아래 그림을 통해 자세히 알아보겠습니다.</p>\n<p><img src=\"https://quantdare.com/wp-content/uploads/2016/04/bb3.png\" alt=\"\"></p>\n<p>Boosting도 Bagging과 동일하게 복원 랜덤 샘플링을 하지만, 가중치를 부여한다는 차이점이 있습니다.\nBagging이 병렬로 학습하는 반면, Boosting은 순차적으로 학습시킵니다.\n학습이 끝나면 나온 결과에 따라 가중치가 재분배됩니다.</p>\n<p>오답에 대해 높은 가중치를 부여하고, 정답에 대해 낮은 가중치를 부여하기 때문에\n오답에 더욱 집중할 수 있게 되는 것 입니다.\nBoosting 기법의 경우, 정확도가 높게 나타납니다.\n하지만, 그만큼 Outlier에 취약하기도 합니다.</p>\n<p>AdaBoost, XGBoost, GradientBoost 등 다양한 모델이 있습니다.\n그 중에서도 XGBoost 모델은 강력한 성능을 보여줍니다. 최근 대부분의 Kaggle 대회 우승 알고리즘이기도 합니다.</p>\n<br>\n<h2 id=\"stacking\" style=\"position:relative;\"><a href=\"#stacking\" aria-label=\"stacking permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Stacking</h2>\n<p><strong>Meta Modeling</strong> 이라고 불리기도 하는 이 방법은 위의 2가지 방식과는 조금 다릅니다.\n“Two heads are better than one” 이라는 아이디어에서 출발합니다.</p>\n<p>Stacking은 서로 다른 모델들을 조합해서 최고의 성능을 내는 모델을 생성합니다.\n여기에서 사용되는 모델은 SVM, RandomForest, KNN 등 다양한 알고리즘을 사용할 수 있습니다.\n이러한 조합을 통해 서로의 장점은 취하고 약점을 보완할 수 있게 되는 것 입니다.</p>\n<p>Stacking은 이미 느끼셨겠지만 필요한 연산량이 어마어마합니다.\n적용해보고 싶다면 아래의 StackNet을 사용하는 방법을 추천합니다.</p>\n<p><a href=\"https://github.com/kaz-Anova/StackNet\">https://github.com/kaz-Anova/StackNet</a></p>\n<p>문제에 따라 정확도를 요구하기도 하지만, 안정성을 요구하기도 합니다.\n따라서, 주어진 문제에 적절한 모델을 선택하는 것이 중요합니다.</p>\n<br>","excerpt":"오늘은 머신러닝 성능을 최대로 끌어올릴 수 있는 앙상블 기법에 대해 정리해보았습니다. Ensemble, Hybrid Method 앙상블 기법은 동일한 학습 알고리즘을 사용해서 여러 모델을 학습하는 개념입니다.\nWeak learner를 결합한다면, Single learner보다 더 나은 성능을 얻을 수 있다는 아이디어입니다.\nBagging 과 Boosting 이 이에 해당합니다. 동일한 학습 알고리즘을 사용하는 방법을 앙상블이라고 한다면,\n서로 다른 모델을 결합하여 새로운 모델을 만들어내는 방법도 있습니다.\n대표적으로 Stacking 이 있으며, 최근 Kaggle…"}}}},"pageContext":{"slug":"bagging-boosting","basePath":"","prev":{"slug":"hive-metastore-issue","publishDate":"2017-08-11"},"next":{"slug":"spark-df-mysql","publishDate":"2017-07-17"}}},"staticQueryHashes":["1946181227","2744905544","3732430097"]}