{"componentChunkName":"component---src-templates-posts-js","path":"/8","result":{"data":{"allContentfulPost":{"edges":[{"node":{"title":"Bagging과 Boosting 그리고 Stacking","id":"2a1f51a0-65ac-59d9-bb86-370d6b8ac4a8","slug":"bagging-boosting","publishDate":"July 19, 2017","heroImage":{"id":"434fa86e-ac5b-52f4-8bd1-6ac1432526e2","title":"cover-datascience","fluid":{"aspectRatio":1.5,"src":"//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&h=1200&q=50 1800w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&h=1200&q=50&fm=webp 1800w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"ac8ad07c-6470-59fe-a551-b037f7ed51c7","childMarkdownRemark":{"id":"5f6e217c-527c-531d-bcd0-66f2d122662e","timeToRead":2,"html":"<p>오늘은 머신러닝 성능을 최대로 끌어올릴 수 있는 앙상블 기법에 대해 정리해보았습니다.</p>\n<br>\n<h2 id=\"ensemble-hybrid-method\" style=\"position:relative;\"><a href=\"#ensemble-hybrid-method\" aria-label=\"ensemble hybrid method permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Ensemble, Hybrid Method</h2>\n<p>앙상블 기법은 동일한 학습 알고리즘을 사용해서 여러 모델을 학습하는 개념입니다.\nWeak learner를 결합한다면, Single learner보다 더 나은 성능을 얻을 수 있다는 아이디어입니다.\n<strong>Bagging</strong> 과 <strong>Boosting</strong> 이 이에 해당합니다.</p>\n<p>동일한 학습 알고리즘을 사용하는 방법을 앙상블이라고 한다면,\n서로 다른 모델을 결합하여 새로운 모델을 만들어내는 방법도 있습니다.\n대표적으로 <strong>Stacking</strong> 이 있으며, 최근 Kaggle 에서 많이 소개된 바 있습니다.</p>\n<br>\n<h2 id=\"bagging\" style=\"position:relative;\"><a href=\"#bagging\" aria-label=\"bagging permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Bagging</h2>\n<p>Bagging은 샘플을 여러 번 뽑아 각 모델을 학습시켜 결과를 <strong>집계(Aggregating)</strong> 하는 방법입니다. 아래의 그림을 통해 자세히 알아보겠습니다.</p>\n<p><img src=\"/assets/images/boosting.png\"></p>\n<p>먼저 대상 데이터로부터 복원 랜덤 샘플링을 합니다.\n이렇게 추출한 데이터가 일종의 표본 집단이 됩니다.\n이제 여기에 동일한 모델을 학습시킵니다.\n그리고 학습된 모델의 예측변수들을 집계하여 그 결과로 모델을 생성해냅니다.</p>\n<p>이러한 방식을 <strong>Bootstrap Aggregating</strong> 이라고 부릅니다.</p>\n<p>이렇게 하는 이유는 \"알고리즘의 안정성과 정확성을 향상시키기 위해서\" 입니다.\n대부분 학습에서 나타나는 오류는 다음과 같습니다.</p>\n<ol>\n<li>높은 bias로 인한 Underfitting</li>\n<li>높은 Variance로 인한 Overfitting</li>\n</ol>\n<p>앙상블 기법은 이러한 오류를 최소화하는데 도움이 됩니다.\n특히 Bagging은 각 샘플에서 나타난 결과를 일종의 중간값으로 맞추어 주기 때문에,\nOverfitting을 피할 수 있습니다.</p>\n<p>일반적으로 Categorical Data인 경우, 투표 방식 (Voting)으로 집계하며\nContinuous Data인 경우, 평균 (Average)으로 집계합니다.</p>\n<p>대표적인 Bagging 알고리즘으로 <code class=\"language-text\">RandomForest</code> 모델이 있습니다.\n원래 단일 DecisionTree 모델은 boundary가 discrete 한 모양일 수 밖에 없지만,\nRandomForest는 여러 트리 모델을 결합하여 이를 넘어설 수 있게 되었습니다.</p>\n<p>결과는 아래와 같습니다.</p>\n<p><img src=\"/assets/images/agg_result.png\"></p>\n<br>\n<h2 id=\"boosting\" style=\"position:relative;\"><a href=\"#boosting\" aria-label=\"boosting permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Boosting</h2>\n<p>Bagging이 일반적인 모델을 만드는데 집중되어있다면,\nBoosting은 맞추기 어려운 문제를 맞추는데 초점이 맞춰져 있습니다.</p>\n<p>수학 문제를 푸는데 9번 문제가 엄청 어려워서 계속 틀렸다고 가정해보겠습니다.\nBoosting 방식은 9번 문제에 가중치를 부여해서 9번 문제를 잘 맞춘 모델을 최종 모델로 선정합니다.\n아래 그림을 통해 자세히 알아보겠습니다.</p>\n<p><img src=\"https://quantdare.com/wp-content/uploads/2016/04/bb3.png\"></p>\n<p>Boosting도 Bagging과 동일하게 복원 랜덤 샘플링을 하지만, 가중치를 부여한다는 차이점이 있습니다.\nBagging이 병렬로 학습하는 반면, Boosting은 순차적으로 학습시킵니다.\n학습이 끝나면 나온 결과에 따라 가중치가 재분배됩니다.</p>\n<p>오답에 대해 높은 가중치를 부여하고, 정답에 대해 낮은 가중치를 부여하기 때문에\n오답에 더욱 집중할 수 있게 되는 것 입니다.\nBoosting 기법의 경우, 정확도가 높게 나타납니다.\n하지만, 그만큼 Outlier에 취약하기도 합니다.</p>\n<p>AdaBoost, XGBoost, GradientBoost 등 다양한 모델이 있습니다.\n그 중에서도 XGBoost 모델은 강력한 성능을 보여줍니다. 최근 대부분의 Kaggle 대회 우승 알고리즘이기도 합니다.</p>\n<br>\n<h2 id=\"stacking\" style=\"position:relative;\"><a href=\"#stacking\" aria-label=\"stacking permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Stacking</h2>\n<p><strong>Meta Modeling</strong> 이라고 불리기도 하는 이 방법은 위의 2가지 방식과는 조금 다릅니다.\n“Two heads are better than one” 이라는 아이디어에서 출발합니다.</p>\n<p>Stacking은 서로 다른 모델들을 조합해서 최고의 성능을 내는 모델을 생성합니다.\n여기에서 사용되는 모델은 SVM, RandomForest, KNN 등 다양한 알고리즘을 사용할 수 있습니다.\n이러한 조합을 통해 서로의 장점은 취하고 약점을 보완할 수 있게 되는 것 입니다.</p>\n<p>Stacking은 이미 느끼셨겠지만 필요한 연산량이 어마어마합니다.\n적용해보고 싶다면 아래의 StackNet을 사용하는 방법을 추천합니다.</p>\n<p><a href=\"https://github.com/kaz-Anova/StackNet\">https://github.com/kaz-Anova/StackNet</a></p>\n<p>문제에 따라 정확도를 요구하기도 하지만, 안정성을 요구하기도 합니다.\n따라서, 주어진 문제에 적절한 모델을 선택하는 것이 중요합니다.</p>\n<br>","excerpt":"오늘은 머신러닝 성능을 최대로 끌어올릴 수 있는 앙상블 기법에 대해 정리해보았습니다. Ensemble, Hybrid Method…"}}}},{"node":{"title":"Spark DataFrame을 MySQL에 저장하는 방법","id":"521ef7ff-15d1-5f53-b269-e5e9a75838d1","slug":"spark-df-mysql","publishDate":"July 17, 2017","heroImage":{"id":"dab22ea8-d54d-52a6-852a-278ba3b19a2b","title":"cover-dataengineering","fluid":{"aspectRatio":1.499531396438613,"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50 1600w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50&fm=webp 1600w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"11db5a37-d4e3-5555-90b2-7f2f8aae6d79","childMarkdownRemark":{"id":"02bdf521-be03-59df-87f6-df899fda1460","timeToRead":1,"html":"<p>Spark에서 MySQL에 접근하고 DataFrame을 read, write 하는 방법에 대해 정리해보았습니다.\n참고로 저는 Spark 2.1.0 버전을 사용 중 입니다.</p>\n<br>\n<h2 id=\"mysql-jdbc-driver\" style=\"position:relative;\"><a href=\"#mysql-jdbc-driver\" aria-label=\"mysql jdbc driver permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>MySQL JDBC Driver</h2>\n<p>JDBC를 통해 접근하기 때문에 드라이버가 필요합니다.\n만일 SBT를 사용하신다면, build.sbt에 maven의 <code class=\"language-text\">mysql-connector-java</code> 를 추가하시면 됩니다.</p>\n<p>직접 jar 파일을 사용해야하는 상황이라면, 다음 링크를 통해 다운받으시면 됩니다.\n<a href=\"https://dev.mysql.com/downloads/connector/j/\">https://dev.mysql.com/downloads/connector/j/</a></p>\n<p>그리고 받으신 jar 파일을 -jars 옵션으로 추가해주셔야 합니다.</p>\n<p><code class=\"language-text\">–jars /home/example/jars/mysql-connector-java-5.1.26.jar</code></p>\n<p>마지막으로 spark-submit 을 사용하신다면, --packages 옵션을 추가해주시면 됩니다.</p>\n<p><code class=\"language-text\">--packages mysql:mysql-connector-java:5.1.39</code></p>\n<br>\n<h2 id=\"spark-dataframe-mysql\" style=\"position:relative;\"><a href=\"#spark-dataframe-mysql\" aria-label=\"spark dataframe mysql permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark DataFrame MySQL</h2>\n<p>Spark의 DataFrame은 read, write 함수를 통해 쉽게 데이터를 가져오거나 저장할 수 있습니다.\n아래 예시는 Scala 언어로 작성했습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token keyword\">import</span> <span class=\"token namespace\">org<span class=\"token punctuation\">.</span>apache<span class=\"token punctuation\">.</span>spark<span class=\"token punctuation\">.</span>sql</span><span class=\"token punctuation\">.</span>SaveMode\n<span class=\"token keyword\">import</span> <span class=\"token namespace\">java<span class=\"token punctuation\">.</span>util</span><span class=\"token punctuation\">.</span>Properties\n\n<span class=\"token keyword\">val</span> tempDF <span class=\"token operator\">=</span> List<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"1\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"2017-06-01\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"2017-06-03\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>toDF<span class=\"token punctuation\">(</span><span class=\"token string\">\"id\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"start\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"end\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">val</span> properties <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> Properties<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nproperties<span class=\"token punctuation\">.</span>put<span class=\"token punctuation\">(</span><span class=\"token string\">\"user\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"userId\"</span><span class=\"token punctuation\">)</span>\nproperties<span class=\"token punctuation\">.</span>put<span class=\"token punctuation\">(</span><span class=\"token string\">\"password\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"password\"</span><span class=\"token punctuation\">)</span>\ntempDF<span class=\"token punctuation\">.</span>write<span class=\"token punctuation\">.</span>mode<span class=\"token punctuation\">(</span>SaveMode<span class=\"token punctuation\">.</span>Append<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>jdbc<span class=\"token punctuation\">(</span><span class=\"token string\">\"jdbc:mysql://url/database\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"table\"</span><span class=\"token punctuation\">,</span> properties<span class=\"token punctuation\">)</span></code></pre></div>\n<p>위 예제에서는 Properties를 통해 설정값을 넣어주었습니다.\n유저 정보나 주소는 맞게 변경해주시면 됩니다.</p>\n<p>mode 라는 것이 있는데 <code class=\"language-text\">SaveMode.Append</code>는 기존의 테이블에 추가하는 방식이고\n<code class=\"language-text\">SaveMode.Overwrite</code>의 경우 기존의 테이블을 새로운 데이터로 대체하는 방식입니다.</p>\n<br>","excerpt":"Spark에서 MySQL에 접근하고 DataFrame을 read, write 하는 방법에 대해 정리해보았습니다.\n참고로 저는 Spark 2.…"}}}},{"node":{"title":"Spark 2.2.0 릴리즈 업데이트 정리","id":"17cb7e89-7826-5409-9b13-766bb5f1bc0c","slug":"spark22","publishDate":"July 14, 2017","heroImage":{"id":"dab22ea8-d54d-52a6-852a-278ba3b19a2b","title":"cover-dataengineering","fluid":{"aspectRatio":1.499531396438613,"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50 1600w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50&fm=webp 1600w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"fc24bdaa-b9ce-57d9-9c46-64c73c8f6553","childMarkdownRemark":{"id":"6296bebe-9af9-58d6-95b9-2a9c9c039b50","timeToRead":2,"html":"<p>7월 11일 약 2개월 만에 Spark 2.2.0이 릴리즈 되었습니다.\n어떤 변경 사항들이 있었는지 릴리즈 노트를 통해 간략하게 정리해보았습니다.</p>\n<br>\n<h2 id=\"pypi-를-통한-pyspark-설치\" style=\"position:relative;\"><a href=\"#pypi-%EB%A5%BC-%ED%86%B5%ED%95%9C-pyspark-%EC%84%A4%EC%B9%98\" aria-label=\"pypi 를 통한 pyspark 설치 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>pypi 를 통한 PySpark 설치</h2>\n<p>드디어 PySpark이 <code class=\"language-text\">pip</code>을 지원하게 되었습니다.\n<code class=\"language-text\">pip install pyspark</code> 명령어를 통해 쉽게 설치 가능합니다.\n설치된 버전은 Spark 2.2.0 버전 입니다.</p>\n<p><img src=\"/assets/images/pyspark-install.png\"></p>\n<p><code class=\"language-text\">numpy, pandas</code> 파이썬 패키지에 dependency가 있으며,\n자세한 사항은 <a href=\"https://pypi.python.org/pypi/pyspark\">pypi 패키지 링크</a>를 통해 확인하실 수 있습니다.\n이번 업데이트를 통해 standalone cluster에서 누구나 쉽게 사용해 볼 수 있을 듯 합니다.</p>\n<br>\n<h2 id=\"structured-streaming\" style=\"position:relative;\"><a href=\"#structured-streaming\" aria-label=\"structured streaming permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Structured Streaming</h2>\n<p>이번 버전부터 Structured Streaming이 새로 추가 되었습니다.\nStructured Streaming은 스트리밍 어플리케이션을 더 빠르고 쉽게 개발하기 위해 만들어진 패키지입니다.</p>\n<p>Spark Streaming이 내부적으로 RDD API를 지원하는 반면, Structured Streaming은 DataFrame, Dataset API를 지원합니다.\n언어는 Scala, Java, Python 모두 지원하며, <code class=\"language-text\">readStream</code> 이라는 메서드를 통해 다양한 저장소로부터 데이터를 읽을 수 있습니다.\n특히 이번 업데이트를 통해 Apache Kafka 스트리밍 지원이 추가되었습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Subscribe to 1 topic</span>\ndf <span class=\"token operator\">=</span> spark \\\n  <span class=\"token punctuation\">.</span>readStream \\\n  <span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"kafka\"</span><span class=\"token punctuation\">)</span> \\\n  <span class=\"token punctuation\">.</span>option<span class=\"token punctuation\">(</span><span class=\"token string\">\"kafka.bootstrap.servers\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"host1:port1,host2:port2\"</span><span class=\"token punctuation\">)</span> \\\n  <span class=\"token punctuation\">.</span>option<span class=\"token punctuation\">(</span><span class=\"token string\">\"subscribe\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"topic1\"</span><span class=\"token punctuation\">)</span> \\\n  <span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\ndf<span class=\"token punctuation\">.</span>selectExpr<span class=\"token punctuation\">(</span><span class=\"token string\">\"CAST(key AS STRING)\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"CAST(value AS STRING)\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Structured Streaming에 대한 자세한 내용은 <a href=\"http://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html\">http://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html</a> 에서 확인하실 수 있습니다.</p>\n<br>\n<h2 id=\"mllib\" style=\"position:relative;\"><a href=\"#mllib\" aria-label=\"mllib permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>MLlib</h2>\n<p>예상했던 대로 MLlib에도 많은 변화가 생겼습니다.\nRDD-based MLlib이 아니라 DataFrame-based MLlib을 확인하시면 됩니다.</p>\n<ul>\n<li>기존에 scala API만 지원하던 모델들에 <code class=\"language-text\">python, R API</code>가 추가되었습니다.</li>\n<li>지원이 추가된 모델은 <strong>Gradient Boosted Trees, Bisecting K-Means, LSH, Distributed PCA, SVD</strong> 입니다.</li>\n<li>DataFreame-based MLlib에 새로운 모델이 추가되었습니다.</li>\n<li>추가된 모델은 <strong>LinearSVC (Linear SVM Classifier), ChiSquare test, Correlation,\nImputer feature transformer, Tweedie distribution, FPGrowth frequent pattern mining, AssociationRules</strong> 입니다.</li>\n</ul>\n<br>\n<h2 id=\"sparkr\" style=\"position:relative;\"><a href=\"#sparkr\" aria-label=\"sparkr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>SparkR</h2>\n<p>이번 업데이트를 통해 SparkR에서 Spark SQL API가 확대되었습니다.</p>\n<ul>\n<li>R API에 Structured Streaming, Catalog가 추가되었습니다.</li>\n<li>to<em>json, from</em>json 메서드가 추가되었습니다.</li>\n<li>Coalesce, DataFrame checkpointing, Multi-column approxQuantile 기능이 추가되었습니다.</li>\n</ul>\n<br>\n<h2 id=\"graphx\" style=\"position:relative;\"><a href=\"#graphx\" aria-label=\"graphx permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GraphX</h2>\n<p>GraphX는 버그 수정, 최적화 업데이트가 추가되었습니다.\n이번 Structured Steaming이 메인에 추가된 것으로 보아,\n추후에 DataFrame, DataSet API 기반의 GraphFrame이 추가될 수도 있다고 예상합니다.</p>\n<ul>\n<li>PageRank, vertexRDD/EdgeRDD checkpoint 버그를 수정했습니다.</li>\n<li>PageRank, Pregel API가 개선되었습니다.</li>\n</ul>\n<br>\n<h2 id=\"core-and-sparksql-deprecations\" style=\"position:relative;\"><a href=\"#core-and-sparksql-deprecations\" aria-label=\"core and sparksql deprecations permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Core and SparkSQL, Deprecations</h2>\n<p>마지막으로 Core, SparkSQL 그리고 Deprecation 업데이트 입니다.\n전체 업데이트 및 기타 자세한 내용은 맨 아래의 링크를 참고하시면 됩니다.</p>\n<ul>\n<li>Python 2.6, Java 7, Hadoop 2.5 지원이 종료되었습니다.</li>\n<li><code class=\"language-text\">ALTER TABLE table_name ADD COLUMNS</code> 구문이 추가되었습니다.</li>\n<li>Cost-Based Optimizer 성능이 개선되었습니다.</li>\n<li>CSV, JSON 포멧의 File listing/IO 성능이 개선되었습니다.</li>\n</ul>\n<br>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h2>\n<ul>\n<li><a href=\"http://spark.apache.org/releases/spark-release-2-2-0.html\">http://spark.apache.org/releases/spark-release-2-2-0.html</a></li>\n</ul>\n<br>","excerpt":"7월 11일 약 2개월 만에 Spark 2.2.…"}}}},{"node":{"title":"Scala의 빌드 도구 SBT","id":"3ed6deed-a421-5296-88ac-00ad5a803025","slug":"scala-sbt","publishDate":"July 08, 2017","heroImage":{"id":"1563c3af-a4e8-5db4-acb2-9bfd9fdb294d","title":"cover-develop","fluid":{"aspectRatio":1.5,"src":"//images.ctfassets.net/tushy4jlcik7/4W9SzEIJpHuwsUBnxSSypH/3a18765095ea5756c742b7adb83a0518/cover_develop.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/4W9SzEIJpHuwsUBnxSSypH/3a18765095ea5756c742b7adb83a0518/cover_develop.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/4W9SzEIJpHuwsUBnxSSypH/3a18765095ea5756c742b7adb83a0518/cover_develop.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/4W9SzEIJpHuwsUBnxSSypH/3a18765095ea5756c742b7adb83a0518/cover_develop.jpg?w=1800&h=1200&q=50 1800w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/4W9SzEIJpHuwsUBnxSSypH/3a18765095ea5756c742b7adb83a0518/cover_develop.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/4W9SzEIJpHuwsUBnxSSypH/3a18765095ea5756c742b7adb83a0518/cover_develop.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/4W9SzEIJpHuwsUBnxSSypH/3a18765095ea5756c742b7adb83a0518/cover_develop.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/4W9SzEIJpHuwsUBnxSSypH/3a18765095ea5756c742b7adb83a0518/cover_develop.jpg?w=1800&h=1200&q=50&fm=webp 1800w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/4W9SzEIJpHuwsUBnxSSypH/3a18765095ea5756c742b7adb83a0518/cover_develop.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"b97ee9c6-52df-5812-8883-c00385807e05","childMarkdownRemark":{"id":"4b58c422-2d4b-5fc1-b279-6fc1416dece2","timeToRead":1,"html":"<p>Scala에는 SBT라는 빌드 도구가 있습니다.\nSBT는 의존성 관리에 Apache ivy를 사용합니다.\n앞으로 계속 내용을 추가할 예정입니다.</p>\n<br>\n<h2 id=\"sbt\" style=\"position:relative;\"><a href=\"#sbt\" aria-label=\"sbt permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>SBT</h2>\n<p>SBT로 생성한 프로젝트의 기본 디렉토리를 보면 <code class=\"language-text\">build.sbt</code>가 있습니다.\n<code class=\"language-text\">sbt</code>라는 명령어를 통해 <code class=\"language-text\">sbt-shell</code>로 이동할 수 있습니다.</p>\n<br>\n<h2 id=\"자주-사용하는-sbt-명령어\" style=\"position:relative;\"><a href=\"#%EC%9E%90%EC%A3%BC-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-sbt-%EB%AA%85%EB%A0%B9%EC%96%B4\" aria-label=\"자주 사용하는 sbt 명령어 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>자주 사용하는 SBT 명령어</h2>\n<ul>\n<li><code class=\"language-text\">actions</code> : 사용 가능한 명령 확인</li>\n<li><code class=\"language-text\">clean</code> : target 디렉토리의 생성된 모든 파일을 삭제</li>\n<li><code class=\"language-text\">update</code> : 프로젝트가 사용하는 라이브러리 다운로드</li>\n<li><code class=\"language-text\">compile</code> : 소스코드 컴파일</li>\n<li><code class=\"language-text\">test</code> : 테스트 실행</li>\n<li><code class=\"language-text\">run</code> : 메인 함수를 통해 코드를 실행</li>\n<li><code class=\"language-text\">reload</code> : 빌드 정의 변경 후 재실행</li>\n<li><code class=\"language-text\">console</code> : 스칼라 인터프리터를 실행</li>\n<li><code class=\"language-text\">package</code> : 배포 가능한 jar파일 생성</li>\n<li><code class=\"language-text\">publish-local</code> : 만들어진 jar를 로컬 ivy 캐시에 설치</li>\n<li><code class=\"language-text\">publish</code> : jar를 원격 저장소에 배포 (원격 저장소 설정 필요)</li>\n</ul>\n<br>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h2>\n<p><a href=\"http://www.scala-sbt.org/0.13/docs/index.html\">http://www.scala-sbt.org/0.13/docs/index.html</a></p>\n<br>","excerpt":"Scala에는 SBT라는 빌드 도구가 있습니다.\nSBT는 의존성 관리에 Apache ivy…"}}}},{"node":{"title":"AWS EMR step을 이용한 Spark Batch 작업","id":"23238ff5-8f3e-5b04-951c-d1971d69b5a2","slug":"emr-step","publishDate":"July 02, 2017","heroImage":{"id":"dab22ea8-d54d-52a6-852a-278ba3b19a2b","title":"cover-dataengineering","fluid":{"aspectRatio":1.499531396438613,"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50 1600w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50&fm=webp 1600w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"9b14c883-75a8-5767-9605-b31d2fc92488","childMarkdownRemark":{"id":"d4d37967-d3d8-5a21-9dab-b655511c629a","timeToRead":1,"html":"<p>AWS EMR은 특정 작업을 등록할 수 있는 <strong>step</strong> 이라는 기능을 제공합니다.\n예를 들어 매일 새벽에 클러스터에서 돌려야하는 Batch 작업이 있다면 step과 스케줄러를 통해 쉽게 해결할 수 있습니다.</p>\n<br>\n<h2 id=\"emr-step\" style=\"position:relative;\"><a href=\"#emr-step\" aria-label=\"emr step permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EMR Step</h2>\n<p>Step은 AWS console 내에서 추가해도 되지만, AWS-Cli를 이용해서 등록해보도록 하겠습니다.\nAWS-Cli로 등록하면 이후에 스크립트로 활용할 수도 있다는 편리함이 있습니다.</p>\n<p>AWS EMR step을 등록하는 방법은 아래와 같습니다.\n가독성을 위해 줄바꿈, 띄어쓰기를 했지만 실제로 등록할 때는 전부 붙이셔야 합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">$ aws emr add-steps\n    --cluster-id <span class=\"token variable\">$CLUSTERID</span>,\n    --steps <span class=\"token assign-left variable\">Name</span><span class=\"token operator\">=</span><span class=\"token variable\">$JOBNAME</span>,\n    <span class=\"token assign-left variable\">Jar</span><span class=\"token operator\">=</span><span class=\"token variable\">$JARFILE</span>,\n    <span class=\"token assign-left variable\">Args</span><span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>\n        /usr/lib/spark/bin/spark-submit,\n        --deploy-mode,client,\n        --properties-file,/etc/spark/conf/spark-defaults.conf,\n        --conf,spark.yarn.executor.memoryOverhead<span class=\"token operator\">=</span><span class=\"token number\">2048</span>,\n        --conf,spark.executor.memory<span class=\"token operator\">=</span>4g,\n        --packages,<span class=\"token variable\">$SPARK_PACKAGES</span>\n    <span class=\"token punctuation\">]</span>,\n    <span class=\"token assign-left variable\">ActionOnFailure</span><span class=\"token operator\">=</span><span class=\"token variable\">${ACTION_ON_FAIL}</span>'</code></pre></div>\n<p>Spark 작업 실행은 <code class=\"language-text\">Spark-submit</code>을 이용하여 클라이언트에 배포하는 형식입니다.\n이를 위해 jar 파일이 클라이언트의 로컬 경로에 포함되어 있어야 합니다.\nActionOnFailure를 통해 실패 시 Terminate, Stop 등의 옵션을 지정할 수 있습니다.</p>\n<p>만약 등록한 작업을 취소하고 싶다면, <code class=\"language-text\">cancel-steps</code>를 이용하시면 됩니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">$ aws emr cancel-steps <span class=\"token punctuation\">..</span>.</code></pre></div>\n<p>Spark 작업이 주기적으로 실행되어야 한다면,\n가장 간단한 방법은 위의 EMR step 등록 스크립트를 crontab으로 등록하는 것 입니다.\n만약 작업이 다양하고 복잡하다면, <strong>AWS Data Pipeline</strong> 이라는 제품을 고려해보는 것도 방법입니다.\n<a href=\"https://aws.amazon.com/ko/datapipeline/details/\">https://aws.amazon.com/ko/datapipeline/details/</a></p>\n<br>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h2>\n<ul>\n<li><a href=\"http://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-submit-step.html\">http://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-submit-step.html</a></li>\n<li><a href=\"http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-work-with-steps.html\">http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-work-with-steps.html</a></li>\n</ul>\n<br>","excerpt":"AWS EMR은 특정 작업을 등록할 수 있는 step 이라는 기능을 제공합니다.\n예를 들어 매일 새벽에 클러스터에서 돌려야하는 Batch…"}}}},{"node":{"title":"AWS 환경에서 Cloudera Manager 설치하기","id":"0d2128fb-e03c-513c-a13e-65512e32e50f","slug":"cloudera-install","publishDate":"June 23, 2017","heroImage":{"id":"dab22ea8-d54d-52a6-852a-278ba3b19a2b","title":"cover-dataengineering","fluid":{"aspectRatio":1.499531396438613,"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50","srcSet":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50 1600w","srcWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&q=50&fm=webp","srcSetWebp":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=450&h=300&q=50&fm=webp 450w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=900&h=600&q=50&fm=webp 900w,\n//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1600&h=1067&q=50&fm=webp 1600w","sizes":"(max-width: 1800px) 100vw, 1800px"},"ogimg":{"src":"//images.ctfassets.net/tushy4jlcik7/7uo9TsqFN9EBsDBqDJ5vXl/4c58a9f94babb15d8fd996c247737656/cover_dataengineering.jpg?w=1800&fl=progressive&q=50"}},"body":{"id":"cdc4d71a-e492-55c0-b955-f40f79e66e7d","childMarkdownRemark":{"id":"2e871a94-f412-52b4-a05e-d7e994b02bcc","timeToRead":1,"html":"<p>클라우데라 매니저는 하둡 에코시스템을 쉽게 설치하고 관리할 수 있도록 도와주는 도구입니다.\n이를 이용하여 AWS EC2 인스턴스에 하둡 클러스터를 설치하고 실행시키는 방법에 대해 정리해보았습니다.\n내 노트북에 가상머신 여러 개를 띄우기 힘든 경우에 좋은 대안이 될 수 있습니다.</p>\n<br>\n<h2 id=\"cloudera-manager\" style=\"position:relative;\"><a href=\"#cloudera-manager\" aria-label=\"cloudera manager permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Cloudera Manager</h2>\n<p>클라우데라는 데이터 인프라 관련 솔루션과 컨설팅을 하는 하둡 전문 기업입니다.\n하둡의 창시자 \"더그 커팅\"도 클라우데라의 수석 아키텍트로 일하고 있습니다.\nImpala, Hue 등 여러 오픈소스도 개발하고 있으며,\n오픈소스 하둡 프로젝트로부터 사람들이 쉽게 설치할 수 있도록 배포판(CDH)을 만들어서 제공해줍니다.\nCloudera Manager에는 유료, 무료버전이 있는데 여기에서는 무료버전(Express)을 설치하겠습니다.</p>\n<br>\n<h2 id=\"install\" style=\"position:relative;\"><a href=\"#install\" aria-label=\"install permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Install</h2>\n<p><img src=\"https://hadoopabcd.files.wordpress.com/2015/01/2.png?w=1378&#x26;h=510\"></p>\n<p>설치는 AWS EC2 m4.large / CentOS / 8GiB 에서 진행하였습니다.\n프리티어인 t2.micro 의 경우 내려가거나 설치가 안될 수 있기 때문에 제외하고 아무거나 쓰셔도 상관없습니다.</p>\n<p><img src=\"https://hadoopabcd.files.wordpress.com/2015/01/3.png\"></p>\n<p>클러스터의 수에 따라 인스턴스를 생성해줍니다. 그리고 Elastic IP로 public IP를 할당시켜줍니다.\nCloudera Manager는 기본으로 7180 포트를 사용하기 때문에 Inbound에서 열어주어야 합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">$ yum <span class=\"token function\">install</span> <span class=\"token function\">wget</span>\n$ <span class=\"token function\">wget</span> http://archive.cloudera.com/cm4/installer/latest/cloudera-manager-installer.bin\n$ <span class=\"token function\">chmod</span> +x cloudera-manager-installer.bin\n$ ./cloudera-manager-installer.bin</code></pre></div>\n<p>먼저 JDK를 설치한 다음, 위의 명령어를 통해 cloudera-manager-installer를 설치하고 실행시켜줍니다.</p>\n<p><img src=\"/assets/images/cloudera-login.png\"></p>\n<p>설치가 완료되고 나서, <code class=\"language-text\">localhost:7180</code>으로 접속하면 다음과 같은 로그인 화면이 나타납니다.\n초기 유저 아이디와 패스워드는 admin 입니다. 이후에는 본인이 원하는 환경에 맞추어 설치를 진행하면 됩니다.</p>\n<br>\n<h2 id=\"trouble-shooting\" style=\"position:relative;\"><a href=\"#trouble-shooting\" aria-label=\"trouble shooting permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Trouble Shooting</h2>\n<p><img src=\"/assets/images/error.png\" alt=\"Error1\"></p>\n<p><code class=\"language-text\">Fatal Error: SELinux is enabled. It must be disabled to install and use this product.</code>\n보안 관련된 문제인데 <code class=\"language-text\">vi /etc/selinux/config</code>로 들어가서 SELinux를 해제시켜주면 해결 됩니다.</p>\n<br>\n<p><img src=\"/assets/images/fail.png\" alt=\"Error2\"></p>\n<p>CDH를 설치하기 위해 <code class=\"language-text\">sudo vi /etc/ssh/sshd_config</code>에 들어가\n임시로 루트 계정의 SSH 접속을 허용해주어야 합니다.\n설치후에는 다시 잠금 설정해주셔도 됩니다.</p>\n<br>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h2>\n<ul>\n<li><a href=\"http://www.cloudera.com/documentation/manager/5-1-x/Cloudera-Manager-Installation-Guide/cm5ig_install_on_ec2.html\">http://www.cloudera.com/documentation/manager/5-1-x/Cloudera-Manager-Installation-Guide/cm5ig<em>install</em>on_ec2.html</a></li>\n</ul>\n<br>","excerpt":"클라우데라 매니저는 하둡 에코시스템을 쉽게 설치하고 관리할 수 있도록 도와주는 도구입니다.\n이를 이용하여 AWS EC…"}}}}]}},"pageContext":{"basePath":"","paginationPath":"","pageNumber":7,"humanPageNumber":8,"skip":43,"limit":6,"numberOfPages":16,"previousPagePath":"/7","nextPagePath":"/9"}}}