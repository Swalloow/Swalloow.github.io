{"componentChunkName":"component---src-templates-post-js","path":"/gpu-utilization/","result":{"data":{"contentfulPost":{"title":"쿠버네티스에서 GPU 리소스를 효율적으로 활용하는 방법","slug":"gpu-utilization","metaDescription":null,"publishDate":"July 08, 2022","publishDateISO":"2022-07-08","tags":[{"title":"DevOps","id":"5a31222a-01e6-5627-8680-b50849114510","slug":"devops"}],"heroImage":{"title":"cover-devops","gatsbyImageData":{"images":{"sources":[{"srcSet":"https://images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=270&h=180&q=50&fm=webp 270w,\nhttps://images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=540&h=360&q=50&fm=webp 540w,\nhttps://images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=1080&h=720&q=50&fm=webp 1080w","sizes":"(min-width: 1080px) 1080px, 100vw","type":"image/webp"}],"fallback":{"src":"https://images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=1080&h=720&fl=progressive&q=50&fm=jpg","srcSet":"https://images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=270&h=180&fl=progressive&q=50&fm=jpg 270w,\nhttps://images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=540&h=360&fl=progressive&q=50&fm=jpg 540w,\nhttps://images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=1080&h=720&fl=progressive&q=50&fm=jpg 1080w","sizes":"(min-width: 1080px) 1080px, 100vw"}},"layout":"constrained","width":1800,"height":1200,"placeholder":{"fallback":"data:image/jpeg;base64,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"}},"ogimg":{"src":"https://images.ctfassets.net/tushy4jlcik7/7KaSTt3mdmrYq2ZK1RiJku/dafd981ff3686217ac151b562e8b1412/cover_devops.jpg?w=1800&q=50"}},"body":{"childMarkdownRemark":{"timeToRead":3,"html":"<p>GPU는 강력한 연산 기능을 제공하지만 비용이 많이 들기 때문에 제한된 리소스를 효율적으로 활용하는 것이 중요합니다. 이번 글에서는 NVIDIA GPU의 리소스 공유를 지원하기 위한 방법으로 <strong>Time Slicing</strong>과 <strong>MIG</strong>에 대해 정리해보려 합니다.</p>\n<br>\n<h2 id=\"gpu-리소스가-낭비되고-있다\" style=\"position:relative;\"><a href=\"#gpu-%EB%A6%AC%EC%86%8C%EC%8A%A4%EA%B0%80-%EB%82%AD%EB%B9%84%EB%90%98%EA%B3%A0-%EC%9E%88%EB%8B%A4\" aria-label=\"gpu 리소스가 낭비되고 있다 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GPU 리소스가 낭비되고 있다?</h2>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1OJV0IVyYU3NjbRYFbM2ZueKAMVZi5_4y\" alt=\"utilization\"></p>\n<p>여러 아키텍쳐(암페어, 파스칼 등)로 구성된 GPU들을 모아 쿠버네티스 노드 풀을 구성하고 사용자들은 GPU 리소스를 할당받아 사용하는 환경이라고 가정해보겠습니다. 사용자들은 GPU 할당을 못 받는 상황임에도 실제 GPU 사용량을 측정해보면 생각보다 낮게 유지되고 있는 경우가 있습니다. 워크로드에 따라 필요한 리소스가 다르기 때문입니다.</p>\n<p>노트북 환경은 항상 개발을 하는게 아니기 때문에 idle 상태로 대기하는 시간이 많습니다. 작은 배치 사이즈로 운영되는 인퍼런스의 경우, 트래픽에 따라 사용량이 달라질 수 있습니다.\n따라서 이런 상황에서는 <strong>항상 리소스를 점유하기 보다 필요할 때 bursting 가능한 방식으로 운영</strong>하는 것이 효율적입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Pod\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> cuda<span class=\"token punctuation\">-</span>vector<span class=\"token punctuation\">-</span>add\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">restartPolicy</span><span class=\"token punctuation\">:</span> OnFailure\n  <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> cuda<span class=\"token punctuation\">-</span>vector<span class=\"token punctuation\">-</span>add\n      <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"k8s.gcr.io/cuda-vector-add:v0.1\"</span>\n      <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">limits</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">nvidia.com/gpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span> <span class=\"token comment\"># GPU 1개 요청하기</span></code></pre></div>\n<p>쿠버네티스에서는 디바이스 플러그인을 통해 Pod가 GPU 리소스를 요청할 수 있습니다.\n하지만 <strong>Pod는 하나 이상의 GPU만 요청할 수 있으며 CPU와 달리 GPU의 일부(fraction)를 요청하는 것은 불가능</strong>합니다. 예를 들어 간단한 실험에 최신 버전의 고성능 GPU 1개를 온전히 할당 받는 것은 낭비입니다. NVIDIA 문서에서는 SW/HW 관점에서 GPU 리소스를 효율적으로 사용하기 위해 다양한 방법을 소개합니다. 그 중 <strong>Time Slicing</strong>과 <strong>MIG</strong>에 대해 알아보겠습니다.</p>\n<br>\n<h2 id=\"time-slicing\" style=\"position:relative;\"><a href=\"#time-slicing\" aria-label=\"time slicing permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Time Slicing</h2>\n<p>Time Slicing은 GPU의 <strong>시간 분할 스케줄러</strong>입니다.\n파스칼 아키텍쳐부터 지원하는 <strong>compute preemption</strong> 기능을 활용한 방법입니다.\n각 컨테이너는 공평하게 <code class=\"language-text\">timeslice</code>를 할당받게 되지만 전환할 때 <code class=\"language-text\">context switching</code> 비용이 발생합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> ConfigMap\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> time<span class=\"token punctuation\">-</span>slicing<span class=\"token punctuation\">-</span>config\n  <span class=\"token key atrule\">namespace</span><span class=\"token punctuation\">:</span> gpu<span class=\"token punctuation\">-</span>operator\n<span class=\"token key atrule\">data</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">a100-40gb</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span><span class=\"token punctuation\">-</span>\n    <span class=\"token key atrule\">version</span><span class=\"token punctuation\">:</span> v1\n    <span class=\"token key atrule\">sharing</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">timeSlicing</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n        <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> nvidia.com/gpu\n          <span class=\"token key atrule\">replicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">8</span>\n        <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> nvidia.com/mig<span class=\"token punctuation\">-</span>1g.5gb\n          <span class=\"token key atrule\">replicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n  <span class=\"token key atrule\">tesla-t4</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span><span class=\"token punctuation\">-</span>\n    <span class=\"token key atrule\">version</span><span class=\"token punctuation\">:</span> v1\n    <span class=\"token key atrule\">sharing</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">timeSlicing</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n        <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> nvidia.com/gpu\n          <span class=\"token key atrule\">replicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">4</span></code></pre></div>\n<p><strong>NVIDIA GPU Operator</strong>에서는 위와 같이 <code class=\"language-text\">ConfigMap</code>을 사용하거나 <code class=\"language-text\">node label</code>을 통해 설정할 수 있습니다. 설정한 이후에 노드를 확인해보면 아래와 같이 리소스에 값이 추가된 것을 확인할 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\">$ kubectl describe node $NODE\n\n<span class=\"token key atrule\">status</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">capacity</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">nvidia.com/gpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">8</span>\n  <span class=\"token key atrule\">allocatable</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">nvidia.com/gpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">8</span></code></pre></div>\n<p>최대 8개 컨테이너까지 <code class=\"language-text\">timeslice</code> 방식으로 shared GPU를 사용할 수 있다는 것을 의미합니다. 이 방법은 <strong>GPU 메모리 limit 설정을 강제하는 것이 아니기 때문에 OOM이 발생</strong>할 수도 있습니다. 이를 방지하려면 GPU를 사용하는 컨테이너 수를 모니터링하고 <code class=\"language-text\">Tensorflow</code>나 <code class=\"language-text\">PyTorch</code> 같은 프레임워크에서 총 GPU 메모리 제한 설정이 필요합니다.</p>\n<br>\n<h2 id=\"multi-instance-gpu-mig\" style=\"position:relative;\"><a href=\"#multi-instance-gpu-mig\" aria-label=\"multi instance gpu mig permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Multi instance GPU (MIG)</h2>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1bJYen4Q33jEa9yHcp4LR3NOPNNzTMZCR\" alt=\"mig\"></p>\n<p>MIG는 A100과 같은 <strong>암페어 아키텍처 기반 GPU를 최대 7개의 개별 GPU 인스턴스로 분할해서 사용</strong>할 수 있는 기능입니다.\n분할된 인스턴스를 <strong>파티션</strong>이라고 부르는데, 각 파티션은 물리적으로 격리되어 있기 때문에 안전하게 병렬로 사용할 수 있습니다.</p>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=12kdw7KZWuph3ADBsJO4FMD4m1fIxufAj\" alt=\"partition\"></p>\n<p>위의 표와 같이 설정을 통해 파티션 크기를 조정할 수 있습니다. 표에서 unit은 하나의 파티션에 몇 개가 할당되는지를 의미합니다.\nA100의 경우, 최대 7개의 <code class=\"language-text\">compute unit</code>과 8개의 <code class=\"language-text\">memory unit</code>을 가질 수 있습니다 (각 5GB 메모리). 파티션은 <code class=\"language-text\">&lt;compute>g.&lt;memory>gb</code> 형식을 따르고 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\">$ kubectl label nodes $NODE nvidia.com/mig.config=all<span class=\"token punctuation\">-</span>1g.5gb\n$ kubectl describe node $NODE\n\n<span class=\"token key atrule\">status</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">capacity</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">nvidia.com/gpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">7</span>\n  <span class=\"token key atrule\">allocatable</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">nvidia.com/gpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">7</span></code></pre></div>\n<p>이번에도 노드 설정 후, 값을 확인해보면 7이 들어가 있습니다.<br>\n<code class=\"language-text\">1g.5gb</code> 크기의 파티션을 7개까지 사용할 수 있다는 의미입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Deployment\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> cuda<span class=\"token punctuation\">-</span>vectoradd\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">replicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">7</span>\n  <span class=\"token key atrule\">template</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">nodeSelector</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">nvidia.com/gpu.product</span><span class=\"token punctuation\">:</span> A100<span class=\"token punctuation\">-</span>SXM4<span class=\"token punctuation\">-</span>40GB<span class=\"token punctuation\">-</span>MIG<span class=\"token punctuation\">-</span>1g.5gb\n    <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> vectoradd\n      <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> nvidia/samples<span class=\"token punctuation\">:</span>vectoradd<span class=\"token punctuation\">-</span>cuda11.2.1\n      <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">limits</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">nvidia.com/gpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span></code></pre></div>\n<p>위와 같이 MIG를 통해 Pod 마다 1개의 파티션을 갖도록 설정해서 7개의 replica 구성하는 것도 가능합니다. 이처럼 사용자는 <strong>MIG를 통해 GPU를 최대로 활용</strong>할 수 있습니다.</p>\n<br>\n<h2 id=\"time-slicing-vs-mig\" style=\"position:relative;\"><a href=\"#time-slicing-vs-mig\" aria-label=\"time slicing vs mig permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Time Slicing vs MIG</h2>\n<p><img src=\"http://drive.google.com/uc?export=view&#x26;id=1NYVmMl0RyQVEnL5toybX8DH-AwpF1Bw2\" alt=\"compare\"></p>\n<p>두 방식을 비교해보면 위의 표와 같습니다.\nTime Slicing 방식은 7개 이상의 컨테이너를 사용할 수 있습니다. 따라서 <strong>bursting 워크로드에 적합한 방식</strong>이라고 볼 수 있습니다. 반면 <strong>MIG는 적은 양의 고정된 사용량을 가지는 워크로드에 적합</strong>합니다.\nA100은 MIG를 통해 분할하고 그 외의 GPU는 Time Slicing을 사용하는 방식으로 함께 사용할 수 있으니 워크로드에 맞는 방식을 선택하는 것이 중요합니다.</p>\n<br>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h2>\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=X876kr-LkPA\">Kubecon 2022 - Improving GPU Utilization using Kubernetes</a></li>\n<li><a href=\"https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/overview.html\">https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/overview.html</a></li>\n</ul>","excerpt":"GPU는 강력한 연산 기능을 제공하지만 비용이 많이 들기 때문에 제한된 리소스를 효율적으로 활용하는 것이 중요합니다. 이번 글에서는 NVIDIA GPU의 리소스 공유를 지원하기 위한 방법으로 Time Slicing과 MIG에 대해 정리해보려 합니다. GPU 리소스가 낭비되고 있다? utilization 여러 아키텍쳐(암페어, 파스칼 등)로 구성된 GPU들을 모아 쿠버네티스 노드 풀을 구성하고 사용자들은 GPU 리소스를 할당받아 사용하는 환경이라고 가정해보겠습니다. 사용자들은 GPU 할당을 못 받는 상황임에도 실제 GPU…"}}}},"pageContext":{"slug":"gpu-utilization","basePath":"","prev":{"slug":"spark-on-kubernetes-spot-instance","publishDate":"2022-07-23"},"next":{"slug":"airflow-worker-keda-autoscaler","publishDate":"2022-06-24"}}},"staticQueryHashes":["1946181227","2744905544","3732430097"]}