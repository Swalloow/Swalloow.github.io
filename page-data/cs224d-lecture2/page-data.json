{"componentChunkName":"component---src-templates-post-js","path":"/cs224d-lecture2/","result":{"data":{"contentfulPost":{"title":"[CS224d] Lecture2. Word Vectors","slug":"cs224d-lecture2","metaDescription":null,"publishDate":"January 21, 2017","publishDateISO":"2017-01-21","tags":[{"title":"DataScience","id":"82931dd3-d22b-528e-8a9b-ddbb200bb401","slug":"datascience"}],"heroImage":{"title":"cover-datascience","gatsbyImageData":{"images":{"sources":[{"srcSet":"https://images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=450&h=300&q=50&fm=webp 450w,\nhttps://images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=900&h=600&q=50&fm=webp 900w,\nhttps://images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&h=1200&q=50&fm=webp 1800w","sizes":"(min-width: 1800px) 1800px, 100vw","type":"image/webp"}],"fallback":{"src":"https://images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&h=1200&fl=progressive&q=50&fm=jpg","srcSet":"https://images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=450&h=300&fl=progressive&q=50&fm=jpg 450w,\nhttps://images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=900&h=600&fl=progressive&q=50&fm=jpg 900w,\nhttps://images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&h=1200&fl=progressive&q=50&fm=jpg 1800w","sizes":"(min-width: 1800px) 1800px, 100vw"}},"layout":"constrained","width":1800,"height":1200,"placeholder":{"fallback":"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEASABIAAD/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wAARCAANABQDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAQBAwUG/8QAHxAAAgEEAgMAAAAAAAAAAAAAAQIAAwQRIQVCExSR/8QAFgEBAQEAAAAAAAAAAAAAAAAAAQAC/8QAFhEBAQEAAAAAAAAAAAAAAAAAAQAR/9oADAMBAAIRAxEAPwCthE7ioFIBOznEy05C8uWI8wpjHVZCF3uFSpUd99m18kpIXSYyTqES96ouiFMJnScb/9k="}},"ogimg":{"src":"https://images.ctfassets.net/tushy4jlcik7/5l0PQJpz5C5IDFjHYigWJI/389fe4852b9cb39e9ada4938db33e6ca/cover_datascience.jpg?w=1800&q=50"}},"body":{"childMarkdownRemark":{"timeToRead":5,"html":"<h3 id=\"how-do-we-represent-the-meaning-of-a-word\" style=\"position:relative;\"><a href=\"#how-do-we-represent-the-meaning-of-a-word\" aria-label=\"how do we represent the meaning of a word permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>How do we represent the meaning of a word?</h3>\n<p>\"단어의 의미를 어떻게 표현해야 할까?\"에 대한 고민은 예전부터 지속되어 왔다. 결국 사람이 단어나 몸짓으로 표현하는 것은 쉽지만 컴퓨터가 이를 이해하기는 어렵다.\n마찬가지로 글쓰기, 미술 등으로 표현하는 것 또한 마찬가지이다.</p>\n<p>과거에는 WordNet과 같은 방법을 사용했다. WordNet이란, 각 단어끼리의 관계(상위단어, 동의어) 가 나타나 있는 트리구조의 그래프 모형이다.\n물론 이를 구축하기 위한 작업은 전부 사람이 했다. 그러다보니 주관적이고 유지하는데 있어 많은 노동이 필요하다는 한계가 존재했다.</p>\n<br>\n<h3 id=\"problems-with-this-discrete-representation\" style=\"position:relative;\"><a href=\"#problems-with-this-discrete-representation\" aria-label=\"problems with this discrete representation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Problems with this discrete representation</h3>\n<p>기존의 discrete representation에는 다음과 같은 문제점들이 존재한다.\n우선, 아까와 같은 경우 리소스로는 충분하지만 뉘앙스가 부족하다. 비슷한 예로 동의어 문제가 있다.\n<code class=\"language-text\">he's good / he's very proficient</code> 와 같은 문장을 비교해보면,\nproficient는 보통 말을 능숙하게 하는 사람을 표현하듯이 문맥에 따라 의미가 달라질 것이다.</p>\n<br>\n<p>두번째 이유는 매번 신조어가 나타나는데 이를 최신화하기 어렵다.\nwicked, badass, ninjia... (미국에서 쓰는 신조어인듯 싶다.)</p>\n<br>\n<p>세번째 이유는 주관적이고 유지하는데 사람의 노동이 필요하다.\n사람들이 직접 구축한 것이기 때문에 주관적이다.\n특히 WordNet은 영어 이외의 언어에서 잘 구축된 경우가 별로 없다.\n(국내의 몇몇 대학원에서 구축된 것이 있지만 절대 공개하지 않는다...)</p>\n<br>\n<p>네번째, 단어 간의 유사도를 계산하기 어렵다.\n어떤 단어가 어느정도 동의어인지 아닌지 계산하기도 어렵다는 말이다.</p>\n<br>\n<p>그래서 보통 Rule-based 와 통계적 NLP를 사용하는 모델들은 단어를 <code class=\"language-text\">atomic symbol</code>로써 사용한다.\n예를 들어, 2만 개의 단어 중에 <code class=\"language-text\">hotel</code> 이라는 단어를 vector space로 나타낸다면,\n<code class=\"language-text\">[0, 0, 0, 0, 0, 0, ..., 0, 0, 1, 0, 0, 0]</code> 이런 식이다. (2만 개의 차원)\n이러한 경우, <code class=\"language-text\">motel</code> 이라는 단어와 AND operation을 통해 유사도를 계산한다면\n무조건 0이 나오게 될 것이다. (별로 좋지 않다)</p>\n<br>\n<p>이러한 벡터를 <code class=\"language-text\">One-Hot Vector</code> 라고 부르며,\n이러한 NLP 방법론을 <code class=\"language-text\">Bag of Words Representation</code> 이라고 부른다.</p>\n<br>\n<h3 id=\"distributional-similarity-based-representations\" style=\"position:relative;\"><a href=\"#distributional-similarity-based-representations\" aria-label=\"distributional similarity based representations permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Distributional similarity based representations</h3>\n<p>이 방법은 현대 통계적 NLP 방법론 중에서 가장 좋게 평가받는 모델 중 하나이다.</p>\n<p><img src=\"https://drive.google.com/uc?export=view&#x26;id=1escsCa2d0KNOKx6FQk_rmgKhpvgYTiAs\" alt=\"distributional\"></p>\n<p>위 그림을 예로 들면, banking 이라는 단어를 표현할 때,\n단어의 왼쪽과 오른쪽으로부터 banking이라는 단어의 정보를 얻는다. (dept, crises...)\n이러한 방법을 통해 문맥으로부터 뭔가 더 얻을 수 있지 않을까? 라는 생각에서 출발한 모델이다.</p>\n<br>\n<h3 id=\"how-to-make-neighbors-represent-words\" style=\"position:relative;\"><a href=\"#how-to-make-neighbors-represent-words\" aria-label=\"how to make neighbors represent words permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>How to make neighbors represent words?</h3>\n<p>그렇다면 어떻게 이웃된 정보를 표현할 수 있을까? <code class=\"language-text\">cooccurrence matrix X</code>를 통해 표현한다.\n여기에는 2가지 옵션이 있는데 full document 와 window 이다.</p>\n<br>\n<ol>\n<li>\n<p>Full document matrix</p>\n<p>전체 문서의 matrix X를 통해 일반적인 토픽을 얻는다.\n예를 들면, <code class=\"language-text\">swimming / wet / boat / ship</code> 은 문맥에 의해 어떤 하나의 공통된 토픽을 갖게 될 것이다.\n대표적으로 <code class=\"language-text\">Latent Semantic Analysis</code> 와 같은 모델이 있다. (이 강의에서는 크게 다루지 않음)</p>\n</li>\n</ol>\n<br>\n<ol start=\"2\">\n<li>\n<p>Window based matrix</p>\n<p>이 방법은 Window의 길이 (일반적으로 5 - 10) 에 따라 대칭적으로 이동하면서 확인하는 방법이다.</p>\n<ul>\n<li>I like deep learning.</li>\n<li>Iike NLP.</li>\n<li>I enjoy flying</li>\n</ul>\n<p>위와 같은 corpus가 있을 때, 이를 matrix로 표현하면 다음과 같다.\n간단히 보면 각 단어의 빈도 수를 체크한 것이다.</p>\n<p><img src=\"https://drive.google.com/uc?export=view&#x26;id=1BPyaD4IN-Gq-uMxkNkydhS_lxNCI9QuH\" alt=\"matrix\"></p>\n</li>\n</ol>\n<br>\n<h3 id=\"problems-with-simple-cooccurrence-vectors\" style=\"position:relative;\"><a href=\"#problems-with-simple-cooccurrence-vectors\" aria-label=\"problems with simple cooccurrence vectors permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Problems with simple cooccurrence vectors</h3>\n<p>이 방법의 문제점은 단어의 크기가 커지면 matrix의 차원이 엄청나게 커진다는 점이다.\n여기에서 sparsity matrix란, 행렬의 요소가 대부분이 0인 행렬을 말한다.\n이 문제를 해결하는 방법은 Low dimensional vector를 사용하는 것이다.</p>\n<br>\n<h3 id=\"method-1-dimensionality-reduction-on-x\" style=\"position:relative;\"><a href=\"#method-1-dimensionality-reduction-on-x\" aria-label=\"method 1 dimensionality reduction on x permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Method 1: Dimensionality Reduction on X</h3>\n<p>그렇다면 차원은 어떻게 낮출 수 있을까? 이에 대한 방법을 소개한다.</p>\n<br>\n<h4 id=\"singular-value-decomposition-svd\" style=\"position:relative;\"><a href=\"#singular-value-decomposition-svd\" aria-label=\"singular value decomposition svd permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Singular Value Decomposition (SVD)</h4>\n<p><img src=\"https://drive.google.com/uc?export=view&#x26;id=1zW80xzOpymNd2EPoJkGigJmE2HAAfUCy\" alt=\"svd\"></p>\n<p>​\tSVD는 원래의 matrix X를 3개의 matrix로 분해한다.\n​\t여기에서 n보다 훨씬 작은 값 k를 설정해서 top - k만 가져가고,\n​\t그 이상의 차원은 날려버리는 방법으로 차원 축소를 해버린다.\n​\t보통 이러한 decomposition 방식을 행렬 내의 dependancy를 풀어주기 위한 방법이라고 이해하면 쉽다.\n​\tdense matrix란, 앞에서 언급했던 sparse matrix와 반대되는 개념이다. (가득찬, 꽉찬)</p>\n<br>\n<p>몇 가지 질문을 통해 나온 내용을 정리하자면, SVD의 dimension은 실험을 통해 최적의 값을 찾아야 한다. 어떤 데이터셋이냐에 따라 다르다.\n일반적으로 25에서 1000 사이의 차원으로 축소한다.</p>\n<br>\n<h3 id=\"hacks-to-x\" style=\"position:relative;\"><a href=\"#hacks-to-x\" aria-label=\"hacks to x permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Hacks to X</h3>\n<p>아직 몇 가지 문제가 남아있다.\nfunction words (the, he, has)가 너무 빈번하게 나타나는데,\n실제로는 이러한 단어가 큰 의미를 갖지 않는다.\n하지만, count를 세게 되면 위와 같은 단어가 큰 영향을 미친다.</p>\n<br>\n<p>이를 해결하기 위한 방법은 다음과 같다.</p>\n<ul>\n<li>corpus 자체에서 저런 단어를 모두 지운다.</li>\n<li>min(X, t), with t~100 : min을 씌워 빈번하게 발생하는 단어의 영향력을 낮춘다. (100번이 넘으면 무조건 100으로 고정)</li>\n<li>Ramped windows : window가 맞물려 있을 때 가까운 단어에 더 가중치를 준다.</li>\n<li>Pearson correlations</li>\n</ul>\n<br>\n<h3 id=\"problems-with-svd\" style=\"position:relative;\"><a href=\"#problems-with-svd\" aria-label=\"problems with svd permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Problems with SVD</h3>\n<p>SVD는 다음과 같은 문제가 있다.\n우선 O(<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>m</mi><msup><mi>n</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">mn^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">m</span><span class=\"mord\"><span class=\"mord mathdefault\">n</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span></span>) 의 복잡도를 갖기 때문에 Computational cost가 심하다.\n따라서, 수 백만의 단어나 문서를 사용하는 경우 좋지 않다.\n그리고 새로운 단어를 새로 적용하기가 어렵다.\n매번 새로 돌려야 하며, 다른 딥러닝 모델과 학습체계가 다르다.</p>\n<br>\n<h2 id=\"word2vec\" style=\"position:relative;\"><a href=\"#word2vec\" aria-label=\"word2vec permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Word2Vec</h2>\n<p>Word2Vec은 처음부터 바로 낮은 차원의 word vectors를 학습시키자! 라는 아이디어에서 출발한다.\n이전에는 cooccurrence를 직접 count 했다면,\nWord2Vec은 모든 단어에 대해 주변의 단어를 예측한다.\n더 빠르고 쉽게 새로운 문장과 어울리며, 새로운 단어를 추가할 수도 있다.\nWord2Vec과 GloVe는 상당히 유사한데, Word2Vec은 구글에서 GloVe는 스탠포드에서 만들었다.\n특히 강의자가 논문에 참여했기 때문에 강의 중간에 자랑이 많이 나온다.</p>\n<p>초기논문 참고 : \"Pennington 2014, Glove : Global Vectors for Word Representation\"</p>\n<br>\n<p>window 크기만큼 양옆으로 확장해가며 sliding 방식으로 이동해서 전체 단어로 확장한다.\nObjective Function : 중심단어를 기준으로 log probability가 최대가 되도록 한다.\nwt가 나왔을 때, wt 주변의 단어가 나올 확률이 얼마나 되는지가 J(0)이다.</p>\n<br>\n<p><img src=\"https://drive.google.com/uc?export=view&#x26;id=1SL6F_jcod9d6TKXlPHya1qkUyGORdtHf\" alt=\"word2vec cost\"></p>\n<ul>\n<li>window length : n</li>\n<li>T token의 아주 큰 corpus</li>\n<li>m만큼 좌우를 확인</li>\n<li>하나의 중심 단어로 주변의 단어를 찾는 probability를 최대화하는 것이 목표</li>\n</ul>\n<br>\n<p><img src=\"https://drive.google.com/uc?export=view&#x26;id=1zbyzRCUnxMFaoeWgNtC6hYdp8eKyfRp0\" alt=\"word2vec cost\"></p>\n<ul>\n<li>o : outside word id</li>\n<li>c : center word id</li>\n<li>u : outside vectors of o</li>\n<li>v : center vectors of c</li>\n</ul>\n<br>\n<p>내적한 값은 단어 간의 유사도를 말하며,\n분자가 최대한 높고 분모가 최대한 작은 것이 좋다.\n항상 두개의 벡터가 나오는데 하나의 벡터는 outside word를 표현하고,\n다른 하나의 벡터는 outside words를 예측하는데 사용한다.</p>\n<br>\n<p>50분부터 Chain Rule에 대한 설명 참조,\n59분부터 log probability를 적용한 설명 참조</p>\n<br>\n<h3 id=\"negative-sampling\" style=\"position:relative;\"><a href=\"#negative-sampling\" aria-label=\"negative sampling permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Negative Sampling</h3>\n<p>추가로 중간에 빼먹은 내용 (Slide 29) 중에 잠깐 Negative Sampling에 대한 내용이 나온다.\n간단히 짚고 넘어가자면, 전체 데이터에서 샘플링 했을 떄 어차피 대부분은 연관이 없고(negative), 확실히 연관이 있는 것(positive)은 적다.\n그래서 negative는 버리고 positive만 골라서 해도 비슷한 결과가 나온다.\n실제로 속도는 빠르고 성능도 괜찮다고 한다.</p>\n<br>\n<h3 id=\"linear-relationships-in-word2vec\" style=\"position:relative;\"><a href=\"#linear-relationships-in-word2vec\" aria-label=\"linear relationships in word2vec permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Linear Relationships in word2vec</h3>\n<p>이러한 표현 방식은 유사도를 계산하는데 아주 좋은 결과를 보인다.\n단순히 vector subtraction으로도 연산 가능하다.</p>\n<p>한국어 word2vec 데모 : <a href=\"http://w.elnn.kr/search/\">http://w.elnn.kr/search/</a></p>\n<br>\n<h3 id=\"count-based-vs-direct-prediction\" style=\"position:relative;\"><a href=\"#count-based-vs-direct-prediction\" aria-label=\"count based vs direct prediction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Count based vs Direct prediction</h3>\n<ol>\n<li>Count based</li>\n</ol>\n<ul>\n<li>LSA, HAL, COALS, PCA</li>\n<li>Training 속도가 빠름</li>\n<li>통계기법을 효율적으로 사용</li>\n<li>(단점) 단어의 유사도를 계산하는데만 사용 (Relationship 계산 불가)</li>\n<li>(단점) count가 큰 경우에 불균등한 중요성이 주어짐 (?)</li>\n</ul>\n<br>\n<ol start=\"2\">\n<li>Direct prediction</li>\n</ol>\n<ul>\n<li>NNLM, HLBL, RNN, Skip-gram/CBOW</li>\n<li>(단점) 말뭉치의 크기에 따라 Scale 가능</li>\n<li>(단점) 통계기법을 활용하지 않음</li>\n<li>대부분의 영역에서 좋은 성능을 내는 모델 생성</li>\n<li>단어 유사도에 대해 복잡한 패턴도 잡아냄</li>\n</ul>\n<br>","excerpt":"How do we represent the meaning of a word? \"단어의 의미를 어떻게 표현해야 할까?\"에 대한 고민은 예전부터 지속되어 왔다. 결국 사람이 단어나 몸짓으로 표현하는 것은 쉽지만 컴퓨터가 이를 이해하기는 어렵다.\n마찬가지로 글쓰기, 미술 등으로 표현하는 것 또한 마찬가지이다. 과거에는 WordNet과 같은 방법을 사용했다. WordNet…"}}}},"pageContext":{"slug":"cs224d-lecture2","basePath":"","prev":{"slug":"open-api-guide","publishDate":"2017-01-25"},"next":{"slug":"db-to-dataframe","publishDate":"2017-01-14"}}},"staticQueryHashes":["1946181227","2744905544","3732430097"]}