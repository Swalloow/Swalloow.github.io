<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>swalloow.github.io/</title>
   
   <link>http://swalloow.github.io/</link>
   <description>About Data Science, Data Engineering</description>
   <language>ko-KO</language>
   <managingEditor> Swalloow</managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>EKS 클러스터에 VPC CIDR 추가하기</title>
	  <link>//eks-cidr</link>
	  <author>Swalloow</author>
	  <pubDate>2020-03-14T19:18:00+09:00</pubDate>
	  <guid>//eks-cidr</guid>
	  <description><![CDATA[
	     <p>​<br /></p>

<p>앞서 정리했던 <a href="https://swalloow.github.io/eks-vpc-cni">EKS의 VPC 네트워크 구성 이해하기</a> 글에서 언급한 바와 같이 내 요구사항보다 적은 범위의 대역으로 클러스터를 생성한다면 VPC CNI로 인한 IP 제한에 마주할 수 있습니다.</p>

<p>처음부터 넓은 범위의 대역으로 생성하면 좋겠지만 이미 클러스터를 생성한 이후에는 어떻게 하면 좋을까요? 
이러한 경우에는 VPC에 새로운 CIDR을 할당하고 이를 클러스터에 추가할 수 있습니다.</p>

<p>​<br /></p>

<h3 id="vpc-cidr">VPC CIDR</h3>

<p><img src="http://drive.google.com/uc?export=view&amp;id=1i0LURjsrJbJWwqaVikpEDg3DQtmPhUp-" alt="" /></p>

<p>먼저 확장하고자 하는 VPC를 선택한 후 새로운 CIDR 범위를 추가해주어야 합니다. 저는 위의 그림과 같이 10.X.X.X와 100.X.X.X 범위를 추가해주었습니다. 이후 새로운 CIDR 범위를 사용하여 서브넷을 생성한 다음 라우팅 테이블을 연결해줍니다.
EKS에서 서브넷을 검색 가능하도록 하기 위해 아래와 같은 태그를 추가해줍니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">Key</span><span class="o">=</span>kubernetes.io/cluster/yourClusterName,Value<span class="o">=</span>shared</code></pre></figure>

<p><br /></p>

<h3 id="custom-eni-config">Custom ENI Config</h3>

<p>이제 새로운 CIDR 범위를 사용하도록 CNI 플러그인 설정을 변경해주어야 합니다. 먼저 클러스터에서 사용 중인 CNI 플러그인 버전을 확인하고 만일 1.5.3 미만인 경우 최신 버전으로 업데이트 합니다. (2020년 3월 기준, 최신 버전은 1.6 입니다)</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>kubectl describe daemonset aws-node --namespace kube-system | grep Image | cut -d <span class="s2">"/"</span> -f 2
<span class="gp">$ </span>kubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/master/config/v1.6/aws-k8s-cni.yaml</code></pre></figure>

<p>설치한 이후에 <code class="highlighter-rouge">kubectl get crd</code>를 통해 <strong>ENIConfig CRD</strong>가 제대로 설치되었는지 확인해줍니다. 만약 설치가 안되었다면 아래의 파일을 통해 CRD를 정의해주시면 됩니다.</p>

<figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="s">apiVersion</span><span class="pi">:</span> <span class="s">apiextensions.k8s.io/v1beta1</span>
<span class="s">kind</span><span class="pi">:</span> <span class="s">CustomResourceDefinition</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">eniconfigs.crd.k8s.amazonaws.com</span>
<span class="s">spec</span><span class="pi">:</span>
  <span class="s">scope</span><span class="pi">:</span> <span class="s">Cluster</span>
  <span class="s">group</span><span class="pi">:</span> <span class="s">crd.k8s.amazonaws.com</span>
  <span class="s">version</span><span class="pi">:</span> <span class="s">v1alpha1</span>
  <span class="s">names</span><span class="pi">:</span>
    <span class="s">plural</span><span class="pi">:</span> <span class="s">eniconfigs</span>
    <span class="s">singular</span><span class="pi">:</span> <span class="s">eniconfig</span>
    <span class="s">kind</span><span class="pi">:</span> <span class="s">ENIConfig</span></code></pre></figure>

<p>이후에 aws-node daemonset에서 커스텀 네트워크 설정을 활성화하는 환경변수를 업데이트 해줍니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>kubectl <span class="nb">set </span>env daemonset aws-node -n kube-system <span class="nv">AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG</span><span class="o">=</span><span class="nb">true</span></code></pre></figure>

<p>이제 앞서 생성한 서브넷에 대해 ENIConfig CRD를 생성해줍니다.
아래는 ap-northeast-2a, 2c에 대한 2개의 서브넷에 대한 파일입니다.
보안 그룹도 클러스터 설정에 맞게 정의해주시면 됩니다.</p>

<figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="s">apiVersion</span><span class="pi">:</span> <span class="s">crd.k8s.amazonaws.com/v1alpha1</span>
<span class="s">kind</span><span class="pi">:</span> <span class="s">ENIConfig</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">private-user-2a</span>
<span class="s">spec</span><span class="pi">:</span>
  <span class="s">subnet</span><span class="pi">:</span> <span class="s">subnet-0ddddaaaaddddccdd</span>
  <span class="s">securityGroups</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">sg-05555598cc88ffd00</span>

<span class="nn">---</span>
<span class="s">apiVersion</span><span class="pi">:</span> <span class="s">crd.k8s.amazonaws.com/v1alpha1</span>
<span class="s">kind</span><span class="pi">:</span> <span class="s">ENIConfig</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">private-user-2c</span>
<span class="s">spec</span><span class="pi">:</span>
  <span class="s">subnet</span><span class="pi">:</span> <span class="s">subnet-0ccccaaaaddddccee</span>
  <span class="s">securityGroups</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">sg-05555598cc88ffd00</span></code></pre></figure>

<p>마지막으로 클러스터에 해당하는 노드 그룹의 어노테이션을 수정해주어야 합니다. <a href="https://github.com/terraform-aws-modules/terraform-aws-eks">terraform-aws-eks</a> 모듈을 통해 클러스터를 생성하셨다면 <code class="highlighter-rouge">worker_groups</code> 하위에 <code class="highlighter-rouge">kubelet_extra_args</code> 변수 설정을 통해 오토스케일링 그룹 설정을 수정할 수 있습니다.</p>

<p>추가되어야할 어노테이션 값은 아래와 같습니다.
Value 값의 경우, 위에서 추가한 ENIConfig CRD의 name metadata를 넣어주시면 됩니다. 업데이트 이후에는 노드를 재시작해주어야 정상적으로 적용됩니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">k8s.amazonaws.com/eniConfig<span class="o">=</span>private-user-2c</code></pre></figure>

<p><br /></p>

<h3 id="reference">Reference</h3>

<ul>
  <li><a href="https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/cni-custom-network.html">https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/cni-custom-network.html</a></li>
  <li><a href="https://aws.amazon.com/ko/premiumsupport/knowledge-center/eks-multiple-cidr-ranges/">https://aws.amazon.com/ko/premiumsupport/knowledge-center/eks-multiple-cidr-ranges/</a></li>
</ul>

<p>​<br /></p>

	  ]]></description>
	</item>

	<item>
	  <title>EKS의 AutoScaling 이해하기</title>
	  <link>//eks-autoscale</link>
	  <author>Swalloow</author>
	  <pubDate>2019-11-23T19:18:00+09:00</pubDate>
	  <guid>//eks-autoscale</guid>
	  <description><![CDATA[
	     <p>​ ​</p>

<p>오늘은 Kubernetes의 Cluster AutoScaling에 대해 정리해보려 합니다.
그 다음 EKS에서는 어떻게 적용할 수 있는지, 어떤 효과를 볼 수 있는지 알아보겠습니다.</p>

<p>​<br /></p>

<h3 id="kubernetes-cluster-autoscaling">Kubernetes Cluster AutoScaling</h3>

<p>Kubernetes는 Cluster AutoScaler를 통해 동적으로 인프라를 확장할 수 있습니다.
Cluster AutoScaler는 Pod의 리소스 요청에 따라 클러스터의 노드를 추가하거나 제거합니다.
만약 리소스 부족으로 인해 스케줄링 대기 상태의 Pod가 존재하는 경우 Cluster AutoScaler가 노드를 추가합니다.
추가 시 설정한 Min, Max 값을 넘어가지 않도록 구성 할 수 있습니다.</p>

<p><img src="http://drive.google.com/uc?export=view&amp;id=1XDum_t6J_lEt88o0X776XUxCHB-Ry_rW" alt="" /></p>

<p>먼저 AutoScaler를 설정하면 대기 상태의 Pod을 주기적으로 확인합니다.
클러스터 리소스가 부족하면서 사용자가 정의한 최대 노드 수에 도달하지 않은 경우 노드 프로비저닝을 요청합니다.
노드가 추가되면 스케줄러에 의해 대기 상태의 Pod들이 새로운 노드로 할당됩니다.</p>

<p>노드를 축소하는 프로세스는 사용자가 정의한 메트릭에 의해 시작됩니다.
예를 들어 CPU Utilization이 50% 이하로 설정했다고 가정해보겠습니다.
Cluster AutoScaler는 삭제할 노드에서 실행 중인 Pod를 다른 노드로 안전하게 이동시킬 수 있는지 확인합니다.
이때 Pod가 로컬 스토리지를 사용하고 있었다면 데이터 유실이 발생할 수 있으니 <strong>PV 사용</strong>을 권장합니다.
이러한 확인 프로세스를 노드 또는 Pod 단위로 수행하고 Pod이 모두 이동하게 되면 노드를 제거합니다.</p>

<p><br /></p>

<h3 id="eks-autoscaler">EKS AutoScaler</h3>

<p>EKS의 AutoScaler는 AWS의 Auto Scaling Group을 활용하고 있습니다.
ASG는 주기적으로 현재 상태를 확인하고 <strong>Desired State</strong>로 변화하는 방식으로 동작합니다.
사용자는 클러스터 노드 수를 제한하는 Min, Max 값을 지정할 수 있습니다.</p>

<p><img src="http://drive.google.com/uc?export=view&amp;id=1qZRhnghSiYRTzKTFJat2594-mv3hdPKS" alt="" /></p>

<p>위와 같이 목적에 따라 여러 종류의 ASG를 설정하고 서로 다른 <strong>AutoScaling Policy</strong>를 적용할 수 있습니다.
<strong>Spot Instance Group</strong>을 설정하면 저렴하지만 입찰 가격에 의해 언제든지 인스턴스가 내려갈 수 있습니다.
하지만 EKS의 <strong>Spot Interrupt Handler (DeamonSet)</strong> 에 의해 정상적으로 실행 중인 Pod들을 재배치할 수 있습니다.</p>

<p><img src="http://drive.google.com/uc?export=view&amp;id=1jErnWXOdrtMQU0o1Aa504yZGMqSy8dbf" alt="" /></p>

<p>그리고 위 그림과 같이 앞서 설정한 Cluster AutoScaler에 의해 새로운 Spot Instance가 추가됩니다.
분석용 클러스터 같은 경우, 주말과 야간 시간에 사용량이 낮다는 사실을 이미 알고 있기 때문에
<strong>CloudWatch Scheduled Policy</strong>를 통해 노드를 축소하면 비용을 절감할 수 있습니다.</p>

<p><br /></p>

<h3 id="eks-autoscaler-">EKS AutoScaler 설정</h3>

<p>먼저 EKS에 ASG 권한을 가지는 IAM Role을 만들어서 Worker Node 보안 그룹에 추가합니다.
다음으로 ASG 그룹 태그를 설정하고 yaml 파일을 클러스터에 배포합니다.
자세한 내용은 <a href="https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/cluster-autoscaler.html">공식 문서</a>를 통해 진행하실 수 있습니다.
만약 Policy로 메모리 지표를 사용하고 싶다면 각 노드에 <strong>CloudWatch Agent</strong>를 배포해야 합니다.</p>

<p>​<br /></p>

	  ]]></description>
	</item>

	<item>
	  <title>EKS의 VPC 네트워크 구성 이해하기</title>
	  <link>//eks-vpc-cni</link>
	  <author>Swalloow</author>
	  <pubDate>2019-11-04T19:18:00+09:00</pubDate>
	  <guid>//eks-vpc-cni</guid>
	  <description><![CDATA[
	     <p>​ ​</p>

<p>모든 Kubernetes as a Service가 그렇듯 EKS 역시 빠르게 변화하고 있습니다.
오늘의 주제는 EKS의 VPC 네트워크 구성과 CNI 플러그인 입니다.</p>

<p>​</p>

<h3 id="aws-vpc">AWS VPC</h3>

<p><img src="http://drive.google.com/uc?export=view&amp;id=1YFGsyvNbctia5Q3dPrRT8KxtirnkII71" alt="" /></p>

<p>아마 AWS를 production 환경에서 사용하고 있다면 VPC 구성은 이미 잘 이해하고 계시리라 생각합니다.
VPC 내에 생성한 인스턴스는 eth0이라는 기본 네트워크 인터페이스를 가지게 됩니다.
그리고 네트워크 인터페이스에 하나 이상의 IPv4 또는 IPv6 주소를 할당할 수 있습니다.
또한 각 Subnet에 존재하는 인스턴스는 Route Table을 통해 통신을 할 수 있습니다.
여기까지가 우리가 알고 있는 VPC 내의 Host 간 통신입니다.</p>

<p>그렇다면 EKS는 어떤 점이 다를까요?
쿠버네티스의 Pod은 <strong>한 개 이상의 컨테이너를 구성</strong>하고 같은 Host와 Network 스택을 공유합니다.
그리고 여러 Host에 사이에 걸쳐 생성된 Pod은 <strong>Overlay Network</strong>를 통해 서로 통신하게 됩니다.
기존 VPC 환경에서는 Pod 네트워크 통신을 기존 방식처럼 지원하기 어려웠습니다.</p>

<p>하지만 대부분의 사용자들이 VPC 기반의 인프라를 구성하고 있었기 때문에
EKS는 VPC를 지원할 수 있어야 했습니다.
예를 들어 사용자는 Security Group, VPC Flow 로그 등의 기능을 그대로 사용하면서,
PrivateLink를 통해 다른 AWS 서비스와 통신할 수 있어야 합니다.
이 문제를 해결하기 위해 AWS는 CNI 라는 네트워크 플러그인을 지원하기 시작했습니다.</p>

<p><br /></p>

<h3 id="eks-cni">EKS CNI</h3>

<p><img src="http://drive.google.com/uc?export=view&amp;id=1pMXq0s4jrnvwLjoUGwfTfj4HQBsf2NiB" alt="" /></p>

<p>CNI 는 다음과 같은 통신을 지원합니다.</p>

<ul>
  <li>단일 Host 내에 존재하는 Pod 간의 통신</li>
  <li>서로 다른 Host 내에 존재하는 Pod 간의 통신</li>
  <li>Pod 과 다른 AWS 서비스 간의 통신</li>
  <li>Pod 과 온프레미스 데이터 센터 간의 통신</li>
  <li>Pod 과 인터넷 간의 통신</li>
</ul>

<p>앞서 말했듯 VPC 내의 EC2는 여러 개의 ENI 를 가질 수 있으며,
ENI 는 여러 개의 IP 주소를 가질 수 있습니다.
하지만 인스턴스 유형 별 가질 수 있는 ENI 와 주소의 최대 수에는 제한이 있습니다.
만약 EC2 인스턴스가 N개의 ENI와 M개의 주소를 가질 수 있다면 최대 IP는 아래와 같이 계산됩니다.</p>

<p><code class="highlighter-rouge">Max IPs = min((N * M - N), subnet's free IP)</code></p>

<p>처음 Worker Node가 추가되면 하나의 ENI 가 인스턴스에 할당됩니다.
하지만 실행되는 Pod의 수가 단일 ENI 에서 허용하는 주소를 초과하면 CNI는 노드에 새로운 ENI 를 추가합니다.
ENI 에 secondary IP 할당과 Pod에 할당할 노드의 IP 주소 풀 관리는 <strong>L-IPAM</strong> 데몬을 통해 이루어집니다.
L-IPAM 데몬은 모든 노드에 DeamonSet으로 배포되며 gRPC를 통해 CNI 플러그인과 통신합니다.</p>

<p>사용하고 있는 인스턴스 유형이 m5.xlarge라고 가정하고 예시를 들어보겠습니다.
우선 m5.xlarge 유형은 4 ENI 와 ENI 당 15 개의 IP 주소를 가질 수 있습니다.
배포된 Pod의 수가 0에서 14 사이라면 IPAM 데몬은 2개의 Warm Pool을 유지하기 위해 ENI를 하나 더 할당합니다.
이때 사용가능한 IP 수는 <code class="highlighter-rouge">2 * (15 - 1) = 28</code> 개가 됩니다.
이런식으로 Warm Pool을 늘려가면서 최대 <code class="highlighter-rouge">4 * (15 - 1) = 56</code> 개의 IP를 가질 수 있습니다.
물론 이 부분은 <code class="highlighter-rouge">WARM_ENI_TARGET</code> 과 같은 CNI 옵션을 통해 수정할 수 있습니다.</p>

<p><br /></p>

<p><img src="http://drive.google.com/uc?export=view&amp;id=1YctEii4hpUFN5ajMU0j0LSEv6AjGS3PU" alt="" /></p>

<p>구체적으로 CNI를 통해 Pod1 과 Pod2가 어떻게 통신하는지 다이어그램으로 표현하면 위와 같습니다.
각 Pod의 eth0에는 secondary IP address가 할당되며 Pod Side Route Table를 가지고 있습니다.
노드의 네트워크 인터페이스까지 도달한 패킷은 EC2-VPC fabric에 의해 포워딩 됩니다.</p>

<p>따라서 EKS 노드를 결정할 때 ENI 제한 관련 부분도 중요하게 생각하셔야 합니다.
노드 당 ENI, IP 주소 제한은 <a href="https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/using-eni.html">해당 공식 문서</a>에서 확인하실 수 있습니다.
물론 CNI를 사용하지 않고 기존의 Calico와 같은 Overlay Network를 사용할 수도 있습니다.
하지만 이를 사용하게 되면 네트워크까지 관리해야하며 새로운 장애 포인트로 이어질 수 있습니다.</p>

<p>​<br /></p>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="https://github.com/aws/amazon-vpc-cni-k8s/blob/master/docs/cni-proposal.md">https://github.com/aws/amazon-vpc-cni-k8s/blob/master/docs/cni-proposal.md</a></li>
  <li><a href="https://medium.com/google-cloud/understanding-kubernetes-networking-pods-7117dd28727">https://medium.com/google-cloud/understanding-kubernetes-networking-pods-7117dd28727</a></li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>AWS MFA CLI 설정 변경 자동화하기</title>
	  <link>//aws-cli-mfa</link>
	  <author>Swalloow</author>
	  <pubDate>2019-10-03T19:18:00+09:00</pubDate>
	  <guid>//aws-cli-mfa</guid>
	  <description><![CDATA[
	     <p>​ ​</p>

<p>클라우드 인프라를 관리하는 경우 여러 계정에 걸친 CLI를 사용하는 경우가 빈번합니다.
만일 CLI 사용 시 MFA 인증을 요구하는 계정과 아닌 계정이 혼재되어 있다면 설정이 정말 귀찮아집니다.
이 글에서는 간단한 스크립트를 통해 AWS CLI 설정 변경을 자동화해보려 합니다.</p>

<p>​</p>

<h3 id="aws-credential-">AWS Credential 설정</h3>

<p>먼저 사용하는 프로필의 credential 정보를 설정해줍니다.
MFA인 프로필과 MFA가 아닌 프로필을 구분하기 위해 아래와 같은 구조로 저장하겠습니다.
<code class="highlighter-rouge">-default</code>가 붙은 프로필이 MFA를 사용하는 프로필입니다.
스크립트 실행을 통해 autogen 값이 자동생성됩니다.</p>

<script src="https://gist.github.com/Swalloow/6f20f25f2efaf1c572b4b53942fb8148.js"></script>

<ul>
  <li><code class="highlighter-rouge">aws_arn_mfa</code> : MFA 에 대한 ARN 값 입니다. 콘솔의 security credentials 메뉴에서 확인하실 수 있습니다.</li>
  <li><code class="highlighter-rouge">aws_session_token</code>: MFA 인증을 거치게 되면 생성되는 STS 토큰 값 입니다. 스크립트를 통해 자동생성 됩니다.</li>
</ul>

<p><br /></p>

<h3 id="mfa--">MFA 설정 스크립트</h3>

<p>다음으로 아래의 스크립트를 각 <code class="highlighter-rouge">awsp.sh</code>, <code class="highlighter-rouge">mfa.py</code> 이름으로 <code class="highlighter-rouge">~/.aws/</code> 경로에 추가해줍니다.
이후에 <code class="highlighter-rouge">~/.zshrc</code> 또는 <code class="highlighter-rouge">~/.bashrc</code> 경로에 <code class="highlighter-rouge">source ~/.aws/awsp.sh</code>를 추가해줍니다.</p>

<script src="https://gist.github.com/Swalloow/916b21cd599295aa86e5a67783f5eb47.js"></script>

<p><br />
이제 <code class="highlighter-rouge">awsp [프로필명] [mfa code]</code>과 같은 명령어로 사용하실 수 있습니다.
예를 들어 dev 프로필의 경우 <code class="highlighter-rouge">awsp dev 012345</code>와 같이 실행합니다.
MFA 조건이 없는 프로필의 경우 <code class="highlighter-rouge">awsp test 0</code>과 같이 실행하시면 됩니다.</p>

<p>​</p>

	  ]]></description>
	</item>

	<item>
	  <title>Terraform 입문자를 위한 미세 팁</title>
	  <link>//tf-tips</link>
	  <author>Swalloow</author>
	  <pubDate>2019-09-20T19:18:00+09:00</pubDate>
	  <guid>//tf-tips</guid>
	  <description><![CDATA[
	     <p>​ ​</p>

<p>클라우드를 활용하는 경우, 인프라 구성 관리 도구로 테라폼을 많이 사용합니다.
오늘은 처음 테라폼을 도입하려고 할때 알아두면 좋은 점들에 대해 정리해보려 합니다.</p>

<p>​</p>

<h3 id="procedural-vs-declarative">Procedural vs Declarative</h3>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># Ansible</span>

- ec2:
  count: 10
  image: ami-v1
  instance_type: t2.micro

<span class="c"># Terraform</span>

resource <span class="s2">"aws_instance"</span> <span class="s2">"example"</span> <span class="o">{</span>
count <span class="o">=</span> 10
ami <span class="o">=</span> <span class="s2">"ami-v1"</span>
instance_type <span class="o">=</span> <span class="s2">"t2.micro"</span>
<span class="o">}</span></code></pre></figure>

<p>위에 나와있는 코드는 Ansible과 Terraform으로 EC2 인스턴스를 구성하는 코드입니다.
만일 여기서 둘의 count 값을 15로 변경한다면 어떻게 변할까요?</p>

<p>먼저 Ansible의 경우, procedural이며 mutable infrastructure를 지향합니다.
따라서 이미 생성된 10개의 인스턴스에 15개의 인스턴스가 추가로 생성되어 총 25개의 인스턴스가 떠있게 됩니다.
반면에 Terraform의 경우, declarative이며 immutable infrastructure를 지향합니다.
count를 15로 선언했기 때문에 Terraform은 이전 상태와 비교한다음, 5만큼의 변경에 대해 교체를 수행합니다.
결과적으로 총 15개의 인스턴스가 떠있게 됩니다.</p>

<p>서로 지향하는 성격이 다르다보니 적절한 상황에 사용하거나 함께 사용하면 좋습니다.
예를 들어 Provisioning 단계에서 Terraform을 사용하고
Configuration, Dependency 설정 단계에서 Ansible을 사용하실 수 있습니다.</p>

<p><br /></p>

<h3 id="terraform-vs-cloudformation">Terraform vs CloudFormation</h3>

<p>AWS를 사용하는 경우, 클라우드 내에서 CloudFormation이라는 서비스를 제공합니다.
CloudFormation 역시 Terraform과 같은 기능을 제공하다보니 도입하기 전에 비교를 많이 합니다.
우선 모듈화, 개발, 문서 측면에서는 Terraform이 더 편했습니다.
이외의 큰 차이를 정리하자면 아래와 같습니다.</p>

<p><strong>CloudFormation은 AWS 지원이 빠릅니다.</strong>
신규 릴리즈된 서비스나 설정들은 Terraform AWS 모듈에 반영되기까지 시간이 좀 걸립니다.
반면에 CloudFormation은 대부분 바로 지원해주다보니 더 편할 수 있습니다.</p>

<p><strong>Terraform은 다른 클라우드 서비스도 지원합니다(Azure, Google Cloud).</strong>
만일 멀티클라우드 이슈에 대한 대응까지 고려하고 있다면 Terraform을 추천드립니다.</p>

<p><br /></p>

<h3 id="terraform-remote-backend">Terraform Remote Backend</h3>

<p><img src="http://drive.google.com/uc?export=view&amp;id=1NRXR-axT-hjEycpr3SigMI3x6e5m8Utx" alt="" /></p>

<p>Terraform은 상태를 <code class="highlighter-rouge">Consul, S3, Enterprise</code> 등의 원격 스토리지에 저장할 수 있습니다.
여러 명이 팀으로 일하는 경우, 인프라 변경 상태에 대한 동기화가 필요합니다.
이 경우 Remote Backend를 고려하시면 좋습니다.
state 파일은 workspace, env에 따라 서로 다른 파일로 관리할 수 있습니다.</p>

<p><br /></p>

<h3 id="terraform-migration">Terraform Migration</h3>

<p>이미 생성되어 있는 수 많은 인프라를 한번에 Terraform으로 옮기는 일은 정말 어렵습니다.
우선 모듈마다 점진적으로 마이그레이션 하는 방법을 추천드립니다.
Terraform은 아래의 코드처럼 이미 생성되어 있는 리소스를 불러올 수 있습니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">resource <span class="s2">"aws_vpc"</span> <span class="s2">"default"</span> <span class="o">{</span>

<span class="c"># resource configuration...</span>

<span class="o">}</span>

<span class="c"># update remote state</span>

<span class="gp">\$ </span>terraform import aws_vpc.default vpc-abc12345</code></pre></figure>

<p>또는 data 블럭을 이용해서 id, arn 등의 값을 불러올 수 있습니다.
예를 들어 아래는 Packer로 생성된 가장 최근 버전의 AMI를 불러오는 코드입니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">data <span class="s2">"aws_ami"</span> <span class="s2">"app"</span> <span class="o">{</span>
most_recent <span class="o">=</span> <span class="nb">true
</span>name_regex <span class="o">=</span> <span class="s2">"app-</span><span class="se">\\</span><span class="s2">d{10}"</span>
owners <span class="o">=</span> <span class="o">[</span><span class="s2">"account_number"</span><span class="o">]</span>
<span class="o">}</span></code></pre></figure>

<p><br /></p>

<h3 id="terraform-module">Terraform Module</h3>

<p>Terraform은 모듈화를 통해 인프라를 재사용할 수 있습니다.
하지만 먼저 기존의 인프라를 어떻게 모듈화할지 많은 고민이 필요합니다.
자주 변경되어야 하는 일부분은 Terraform 관리 대상에서 제외시키는 방법도 있습니다.
또한 인프라 장애 대응이 필요한 부분은 쉽게 HA를 구성할 수 있도록 작성해야 합니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">module <span class="s2">"network"</span> <span class="o">{</span>
<span class="nb">source</span> <span class="o">=</span> <span class="s2">"./network"</span>
name <span class="o">=</span> <span class="s2">"default"</span>
cidr <span class="o">=</span> <span class="s2">"000.0.0.0/16"</span>

azs <span class="o">=</span> <span class="o">[</span><span class="s2">"ap-northeast-2a"</span>, <span class="s2">"ap-northeast-2c"</span><span class="o">]</span>
public_subnets <span class="o">=</span> <span class="o">[</span><span class="s2">"000.0.0.0/22"</span>, <span class="s2">"111.1.1.1/22"</span><span class="o">]</span>

tags <span class="o">=</span> <span class="o">{</span>
dept <span class="o">=</span> <span class="s2">"mydept"</span>
service <span class="o">=</span> <span class="s2">"app"</span>
<span class="o">}</span>
<span class="o">}</span>

<span class="c"># common tags</span>

tags <span class="o">=</span> <span class="s2">"</span><span class="se">\$</span><span class="s2">{merge(var.tags, map("</span>Name<span class="s2">", format("</span>%s-public-%s<span class="s2">", var.name, var.azs[count.index])))}"</span></code></pre></figure>

<p>각 모듈은 Input과 Output Variable을 가집니다.
위의 예시는 네트워크에 관련된 모듈입니다.
모듈을 통해 생성된 모든 리소스는 공통된 태그를 통해 관리할 수 있으며
만일 네트워크 구성을 변경해야하는 경우, CIDR 값만 수정하면 됩니다.</p>

<p><br /></p>

<h3 id="terraform-loop-conditionls-012">Terraform Loop, Conditionls (0.12)</h3>

<p>Terraform은 0.12 버전을 기점으로 더 효율적인 코드를 작성할 수 있게 되었습니다.
따라서 새로 시작하신다면 0.12+ 버전 사용을 권장드립니다.
예시를 통해 Terraform에서 루프를 어떻게 정의하는지 설명드리겠습니다.
아래의 예시는 IAM Role에 Policy를 연결하는 코드입니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">locals <span class="o">{</span>
lambda_backend_policy_arns <span class="o">=</span> <span class="o">[</span>
<span class="s2">"arn:aws:iam::aws:policy/AmazonRDSFullAccess"</span>,
<span class="s2">"arn:aws:iam::aws:policy/CloudWatchFullAccess"</span>,
<span class="s2">"arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess"</span>,
<span class="o">]</span>
<span class="o">}</span>

resource <span class="s2">"aws_iam_role_policy_attachment"</span> <span class="s2">"attach"</span> <span class="o">{</span>
count <span class="o">=</span> <span class="s2">"</span><span class="se">\$</span><span class="s2">{length(local.lambda_backend_policy_arns)}"</span>

role <span class="o">=</span> <span class="s2">"</span><span class="k">${</span><span class="nv">aws_iam_role</span><span class="p">.lambda_backend.name</span><span class="k">}</span><span class="s2">"</span>
   policy_arn <span class="o">=</span> <span class="s2">"</span><span class="k">${</span><span class="nv">local</span><span class="p">.lambda_backend_arns[count.index]</span><span class="k">}</span><span class="s2">"</span>
<span class="o">}</span></code></pre></figure>

<p>이전에는 위와 같이 Array 타입의 인덱스를 통해 Loop를 정의해야 했습니다.
하지만 0.12 버전부터 for-loop, for-each 구문을 지원하기 시작했습니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">variable <span class="s2">"subnet_numbers"</span> <span class="o">{</span>
default <span class="o">=</span> <span class="o">{</span>
<span class="s2">"ap-northeast-2a"</span> <span class="o">=</span> 1
<span class="s2">"ap-northeast-2b"</span> <span class="o">=</span> 2
<span class="s2">"ap-northeast-2c"</span> <span class="o">=</span> 3
<span class="o">}</span>
<span class="o">}</span>

resource <span class="s2">"aws_subnet"</span> <span class="s2">"example"</span> <span class="o">{</span>
for_each <span class="o">=</span> var.subnet_numbers

vpc_id <span class="o">=</span> aws_vpc.example.id
availability_zone <span class="o">=</span> each.key
cidr_block <span class="o">=</span> cidrsubset<span class="o">(</span>
aws_vpc.example.cidr_block, each.value
<span class="o">)</span>
<span class="o">}</span></code></pre></figure>

<p>이외에도 찾아보시면 다양한 타입과 연산을 지원합니다.
이 글이 처음 입문하시는데 조금 도움이 되셨으면 좋겠습니다!</p>

<p>​</p>

	  ]]></description>
	</item>

	<item>
	  <title>KOPS로 AWS에 Kubernetes 클러스터 구축하기</title>
	  <link>//aws-kops</link>
	  <author>Swalloow</author>
	  <pubDate>2019-02-10T19:18:00+09:00</pubDate>
	  <guid>//aws-kops</guid>
	  <description><![CDATA[
	     <p>​   ​</p>

<p>Kubernetes 클러스터를 구성하는 방법은 여러 가지가 있습니다.
그 중에서 kubeadam은 온프레미스 환경에서 많이 사용하고 kops는 클라우드 환경에서 많이 사용하고 있습니다.
이번 글에서는 kops로 AWS EC2에 Kubernetes 클러스터 구축하는 방법에 대해 정리해보겠습니다.</p>

<p>​   ​</p>

<h3 id="kops-kubectl-awscli--linux">kops, kubectl, awscli 설치 (Linux)</h3>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># kops 설치</span>
wget -O kops https://github.com/kubernetes/kops/releases/download/<span class="k">$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span class="s1">'"'</span> -f 4<span class="k">)</span>/kops-linux-amd64
chmod +x ./kops
sudo mv ./kops /usr/local/bin/

<span class="c"># kubectl 설치</span>
wget -O kubectl https://storage.googleapis.com/kubernetes-release/release/<span class="k">$(</span>curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt<span class="k">)</span>/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin/kubectl

<span class="c"># aws-cli 설치 (amazon linux라면 불필요)</span>
pip install awscli</code></pre></figure>

<p>​   ​</p>

<h3 id="iam-user-">IAM User 설정</h3>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># 아래의 권한이 필요</span>
AmazonEC2FullAccess
AmazonRoute53FullAccess
AmazonS3FullAccess
IAMFullAccess
AmazonVPCFullAccess</code></pre></figure>

<p>​   ​</p>

<h3 id="aws-cli-iam--">aws-cli로 IAM 계정 생성</h3>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">aws iam create-group --group-name kops

aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonRoute53FullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/IAMFullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess --group-name kops

aws iam create-user --user-name kops
aws iam add-user-to-group --user-name kops --group-name kops
aws iam create-access-key --user-name kops

aws configure   <span class="c"># AccessKeyID와 SecretAccessKey 등록</span></code></pre></figure>

<p>​   ​</p>

<h3 id="dns-cluster-state-storage-">DNS, Cluster State storage 설정</h3>

<ul>
  <li>kops 1.6.2 버전 이상이라면 DNS 설정은 옵션 (gossip-based cluster)</li>
  <li>Cluster Configuration Storage로 S3를 사용 (Bucket 미리 생성해야 함)</li>
  <li>S3 default bucket encryption을 사용할 수 있음</li>
  <li>default encryption 설정이 안되어 있다면 kops에서 AES256 encryption</li>
</ul>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># Create Bucket</span>
aws s3api create-bucket <span class="se">\</span>
    --bucket prefix-example-com-state-store <span class="se">\</span>
    --region ap-northeast-2

<span class="c"># S3 versioning</span>
aws s3api put-bucket-versioning <span class="se">\</span>
    --bucket prefix-example-com-state-store <span class="se">\</span>
    --versioning-configuration <span class="nv">Status</span><span class="o">=</span>Enabled</code></pre></figure>

<p>​   ​</p>

<h3 id="kubernetes-cluster-">Kubernetes Cluster 생성</h3>

<ul>
  <li>kops를 통해 생성된 인스턴스는 자동으로 Auto Scaling 그룹에 들어감</li>
  <li><code class="highlighter-rouge">kops create</code>: cluster configuration을 생성, SSH-Key가 필요</li>
  <li><code class="highlighter-rouge">kops edit</code>: cluster configuation을 수정</li>
  <li><code class="highlighter-rouge">kops update</code>: Build 단계, kubernetes component를 모두 설치하고 나면 ready 상태로 전환</li>
  <li><code class="highlighter-rouge">kops delete</code>: cluster 제거, –yes (구성요소까지 전부 삭제)</li>
  <li><code class="highlighter-rouge">kops rolling-update</code>: downtime이 없는 rolling-update 실행</li>
</ul>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># Environment</span>
<span class="nb">export </span><span class="nv">NAME</span><span class="o">=</span>myfirstcluster.example.com  <span class="c"># DNS가 설정되어 있는 경우</span>
<span class="nb">export </span><span class="nv">NAME</span><span class="o">=</span>myfirstcluster.k8s.local    <span class="c"># DNS가 설정되어 있지 않은 경우</span>
<span class="nb">export </span><span class="nv">KOPS_STATE_STORE</span><span class="o">=</span>s3://prefix-example-com-state-store

<span class="c"># Seoul region</span>
aws ec2 describe-availability-zones --region ap-northeast-2
kops create cluster --zones ap-northeast-2 <span class="k">${</span><span class="nv">NAME</span><span class="k">}</span>
kops edit cluster <span class="k">${</span><span class="nv">NAME</span><span class="k">}</span>
kops update cluster <span class="k">${</span><span class="nv">NAME</span><span class="k">}</span> --yes
kops validate cluster

<span class="c"># Kubectl</span>
kubectl get nodes
kubectl cluster-info
kubectl -n kube-system get po   <span class="c"># system pod</span>

<span class="c"># Dashboard</span>
kops get secrets admin -oplaintext
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml

<span class="c"># Access https://&lt;kubernetes-master-hostname&gt;/ui</span>
kops get secrets admin --type secret -oplaintext

<span class="c"># Stop cluster</span>
<span class="c"># Change minSize, MaxSize to 0</span>
kops get ig
kops edit ig nodes
kops edit ig master</code></pre></figure>

<p>​   ​</p>

<h3 id="advanced">Advanced</h3>

<ul>
  <li>Network topology를 설정할 수 있음 (public, private)</li>
  <li>Private: VPC내의 private subnet으로 생성</li>
  <li>Public: VPC내의 public subnet으로 생성 (routed to Internet Gateway)</li>
  <li>Multiple zone, HA Master를 구성할 수 있음 (–master-zones=us-east-1b,us-east-1c,us-east-1d)</li>
  <li>Instance Group을 지정 가능 (https://github.com/kubernetes/kops/blob/master/docs/instance_groups.md)</li>
  <li>AMI를 지정가능, CoreOS AMI</li>
  <li>Container Network Interface (CNI) 지정 가능 (https://kubernetes.io/docs/concepts/cluster-administration/networking/)</li>
  <li>Authorization (https://kubernetes.io/docs/reference/access-authn-authz/authorization/)</li>
</ul>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># SSH Key</span>
ssh-keygen -t rsa -f <span class="nv">$NAME</span>.key -N <span class="s1">''</span>
<span class="nb">export </span><span class="nv">PUBKEY</span><span class="o">=</span><span class="s2">"</span><span class="nv">$NAME</span><span class="s2">.key.pub"</span>

<span class="c"># CoreOS Image</span>
<span class="nb">export </span><span class="nv">IMAGE</span><span class="o">=</span><span class="k">$(</span>curl -s https://coreos.com/dist/aws/aws-stable.json|sed <span class="s1">'s/-/_/g'</span>|jq <span class="s1">'.'</span><span class="nv">$REGION</span><span class="s1">'.hvm'</span>|sed <span class="s1">'s/_/-/g'</span> | sed <span class="s1">'s/\"//g'</span><span class="k">)</span>

<span class="c"># Create Cluster</span>
kops create cluster --kubernetes-version<span class="o">=</span>1.12.1 <span class="se">\</span>
    --ssh-public-key <span class="nv">$PUBKEY</span> <span class="se">\</span>
    --networking flannel <span class="se">\</span>
    --api-loadbalancer-type public <span class="se">\</span>
    --admin-access 0.0.0.0/0 <span class="se">\</span>
    --authorization RBAC <span class="se">\</span>
    --zones ap-northeast-2 <span class="se">\</span>
    --master-zones ap-northeast-2 <span class="se">\</span>
    --master-size t2.medium <span class="se">\</span>
    --node-size t2.medium <span class="se">\</span>
    --image <span class="nv">$IMAGE</span> <span class="se">\</span>
    --node-count 3 <span class="se">\</span>
    --cloud aws <span class="se">\</span>
    --bastion <span class="se">\</span>
    --name <span class="nv">$NAME</span> <span class="se">\</span>
    --yes</code></pre></figure>

<p>​   ​</p>

<h3 id="reference">Reference</h3>

<ul>
  <li>https://github.com/kubernetes/kops</li>
  <li>
    <p>https://kubernetes.io/ko/docs/setup/custom-cloud/kops/</p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>influxDB와 Grafana로 실시간 서버 모니터링 구축하기(2)</title>
	  <link>//influx-grafana2</link>
	  <author>Swalloow</author>
	  <pubDate>2017-04-05T21:18:00+09:00</pubDate>
	  <guid>//influx-grafana2</guid>
	  <description><![CDATA[
	     <p>​   ​</p>

<p><a href="https://swalloow.github.io/influx-grafana1">지난 포스팅</a>에 이어서 Grafana를 연동해보도록 하겠습니다.</p>

<p>​</p>

<h3 id="grafana">Grafana</h3>

<p><img src="/assets/images/grafana.png" alt="Grafana" /></p>

<p>지난 번에 설치했던 Grafana 도커 이미지를 컨테이너로 실행하면 위와 같이 로그인 화면이 나타납니다.
아이디는 admin, 비밀번호는 admin으로 접속하시면 됩니다.
이제 아까 만들었던 database를 Grafana의 data source에 등록할 차례입니다.</p>

<p><img src="/assets/images/grafana-ds.png" alt="Grafana-ds" /></p>

<p>Type을 InfluxDB로 맞추고, Url과 Database만 잘 설정해주시면 됩니다.
이제 새로운 대시보드를 생성하고 <code class="highlighter-rouge">Add Graph</code>로 그래프를 추가합니다.</p>

<p>​<img src="/assets/images/grafana-graph.png" alt="Grafana-graph" /></p>

<p>위와 같이 cpu, memory 지표 외에도 다양한 지표를 쉽게 추가할 수 있습니다.</p>

<p><img src="/assets/images/grafana-result.png" alt="Mac" /></p>

<p>​   ​</p>

	  ]]></description>
	</item>

	<item>
	  <title>influxDB와 Grafana로 실시간 서버 모니터링 구축하기(1)</title>
	  <link>//influx-grafana1</link>
	  <author>Swalloow</author>
	  <pubDate>2017-04-05T19:18:00+09:00</pubDate>
	  <guid>//influx-grafana1</guid>
	  <description><![CDATA[
	     <p>​   ​</p>

<p>요즘 실시간 로그 수집 및 분석 도구로 <strong>ELK (Elastic Search)</strong> 를 많이 사용하지만,
간단한 서버 모니터링이나 시계열 데이터 분석도구를 찾으신다면, <strong>influxDB-Grafana</strong> 도 좋습니다.
이 포스팅에서는 간단한 예제를 통해 influxDB와 Telegraf, Grafana에 대해 알아보겠습니다.</p>

<p>​</p>

<h3 id="influxdb">influxDB</h3>

<p><img src="/assets/images/influxdb.png" alt="influxDB" /></p>

<p>influxDB는 시계열 데이터베이스입니다.
다른 시계열 데이터베이스도 많지만, 설치가 쉽고 간편합니다.
그리고 SQL 구문과 graphite 등 여러 프로토콜을 지원하여 확장성이 높습니다.</p>

<p>작년 9월에 1.0 버전을 릴리즈했으며, 현재 기준 1.2가 최신 버전입니다.
시계열 데이터베이스는 로그, 시스템 모니터링 도구로 활용될 수도 있지만,
주식, 환율과 같은 시계열 데이터의 분석 도구로도 많이 사용됩니다.</p>

<p>그렇다면 바로 로컬에 설치해보도록 하겠습니다.
여기서는 OS X 기준이며, 다른 OS는 <a href="https://docs.influxdata.com/influxdb/v1.2/introduction/installation/">설치 페이지를</a> 참조하시면 됩니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>brew update influxdb
<span class="gp">$ </span>brew install influxdb</code></pre></figure>

<p>​</p>

<p>실습은 Docker를 통해 진행하도록 하겠습니다.
DockerHub로부터 influxdb와 Grafana 이미지를 받아옵니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>docker pull tutum/influxdb
<span class="gp">$ </span>docker pull grafana/grafana</code></pre></figure>

<p><img src="/assets/images/influx-port.png" alt="influx-port" /></p>

<p>받아온 이미지를 컨테이너로 실행시키고, 위와 같이 포트를 변경해줍니다.
8083 포트는 웹 클라이언트로 사용되며, 8086 포트는 데이터를 주입하고 가져가는 용도로 사용됩니다.
이제 호스트의 8086 주소로 들어가보면 influxDB의 관리자 페이지를 볼 수 있습니다.</p>

<p><img src="/assets/images/influx-admin.png" alt="influx-admin" /></p>

<p>쿼리를 잘 모르더라도 좌측의 Query Templates를 통해 쉽게 입력할 수 있습니다.
스키마는 기존의 데이터베이스와 비슷하면서도 조금 다릅니다.
HTTP로 데이터를 핸들링 할 수 있으며, 이외에도 여러 가지 특징이 있지만
여기서 설명하기보다는 공식 래퍼런스를 참조하는 편이 나을 것 같습니다.</p>

<p>​</p>

<h3 id="telegraf">Telegraf</h3>

<p>이제 Telegraf로 실시간 시스템 지표를 influxDB에 넣어보겠습니다.
Telegraf는 다양한 데이터 소스에서 plugin을 통해 데이터를 수집하고 저장합니다.
제공하는 input plugin은 <a href="https://github.com/influxdata/telegraf#input-plugins">다음 페이지</a>에서 확인하실 수 있습니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>brew update
<span class="gp">$ </span>brew install telegraf</code></pre></figure>

<p>위와 같이 설치한 다음, system plugin을 사용해보도록 하겠습니다.
<code class="highlighter-rouge">--sample-config</code>를 통해 .conf 파일을 생성할 수 있습니다.
input filter는 cpu와 memory 지표로, output은 influxDB에 저장됩니다.
.conf 파일을 열어 <code class="highlighter-rouge">output host url</code>을 본인의 호스트에 맞게 변경해주어야 합니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>telegraf --sample-config  --input-filter cpu:mem --output-filter influxdb &gt; telegraf.conf
<span class="gp">$ </span>telegraf -config telegraf.conf</code></pre></figure>

<p>이제 influxDB로 돌아와서 보시면, telegraf 라는 데이터베이스가 생성되어 있습니다.
measurement를 확인해보시거나 쿼리를 날려 데이터를 확인할 수 있습니다.</p>

<p><img src="/assets/images/influx-admin2.png" alt="influx-admin2" /></p>

<p>influxDB는 언젠가 디스크가 찰 수 있어서, 데이터 보관 또는 삭제에 대한 정책이 필요합니다.
이에 대해서는 <code class="highlighter-rouge">Retention Policy</code>를 찾아보시면 됩니다.</p>

<p>​</p>

	  ]]></description>
	</item>

	<item>
	  <title>Docker와 Gitlab CI를 활용한 빌드, 테스트 자동화</title>
	  <link>//gitlabci-docker</link>
	  <author>Swalloow</author>
	  <pubDate>2017-03-31T19:18:00+09:00</pubDate>
	  <guid>//gitlabci-docker</guid>
	  <description><![CDATA[
	     <p>​   ​</p>

<p>Gitlab은 설치형 GitHub이라고 이해하시면 편합니다.
무료로 private repository와 CI 서버를 제공해줍니다.
심지어 Docker Registry도 무료로 제공하고 있습니다.
아직 많은 분들이 Gitlab CI의 여러 장점들을 잘 모르시는 것 같아 정리해보았습니다.</p>

<p>​</p>

<h3 id="gitlab-ci">Gitlab CI</h3>

<p>Gitlab CI는 Gitlab에서 무료로 제공하는 CI 툴 입니다.
Gitlab과 완벽하게 연동되며 CI를 위해 <strong>CI linter, pipeline, cycle analytics</strong> 등 다양한 서비스를 제공합니다.</p>

<p><img src="/assets/images/gitlab-ci.png" alt="Gitlab-CI" /></p>

<p>travis, circle CI와 마찬가지로 Gitlab CI는 <code class="highlighter-rouge">gitlab-ci.yml</code> 파일로 설정할 수 있습니다.
Gitlab은 DigitalOcean과 제휴하여 CI 서버(Runner)를 따로 제공합니다.
따라서 <code class="highlighter-rouge">Runner</code>에 job을 할당하여 돌아가도록 설정할 수 있습니다.</p>

<p><img src="/assets/images/gitlab-pipe.png" alt="Gitlab-Pipe" /></p>

<p>그리고 Runner는 <strong>Docker 컨테이너</strong> 를 기반으로 돌아갑니다.
Gitlab CI를 실행해보면 처음에 Ruby 이미지를 받아와서 컨테이너를 실행시키는 것을 볼 수 있습니다.
따라서, <strong>Base Image를 내 어플리케이션 이미지로 바꾸면 빌드 및 테스트 속도가 빠르게 향상됩니다</strong>.</p>

<p>​</p>

<h3 id="gitlab-registry">Gitlab Registry</h3>

<p><img src="/assets/images/gitlab-registry.png" alt="Gitlab-Registry" /></p>

<p>Docker 친화적인 Gitlab은 Docker Registry도 무료로 제공해줍니다.
<code class="highlighter-rouge">Gitlab Registry</code> 탭에 들어가면 Docker Registry의 주소가 적혀있고 친절하게 명령어까지 써있습니다.</p>

<p>아마 많은 분들이 DockerHub를 결제하거나, AWS S3를 이용하여 Docker Registry를 구축하셨을 겁니다.
하지만 Gitlab에서는 그럴 필요가 없습니다.</p>

<p>​</p>

<h3 id="docker-with-gitlab-ci">Docker with Gitlab CI</h3>

<p>gitlab-ci 설정파일은 대략 다음과 같습니다.</p>

<figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="s">image</span><span class="pi">:</span> <span class="s">gitlab-registry</span>
<span class="s">stages</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">build</span>
  <span class="pi">-</span> <span class="s">test</span>
  <span class="pi">-</span> <span class="s">deploy</span>

<span class="s">job-build</span><span class="pi">:</span>
  <span class="s">stage</span><span class="pi">:</span> <span class="s">build</span>
  <span class="s">script</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">pip install -r requirements.txt</span>
  <span class="pi">-</span> <span class="s">python -m py_compile run.py</span>

<span class="s">job-test</span><span class="pi">:</span>
  <span class="s">stage</span><span class="pi">:</span> <span class="s">test</span>
  <span class="s">script</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">pytest --pep8 -m pep8 backend/</span>

<span class="s">job-deploy</span><span class="pi">:</span>
  <span class="s">stage</span><span class="pi">:</span> <span class="s">deploy</span>
  <span class="s">script</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">deployment</span></code></pre></figure>

<p>Gitlab CI와 Docker를 활용한 빌드 테스트 자동화는 위의 그림과 같이 이루어집니다.</p>

<p><img src="/assets/images/ci-process.png" alt="CI" /></p>

<ol>
  <li>사용자가 Gitlab 저장소에 push를 하면, Gitlab CI Runner로 전달됩니다.</li>
  <li>Gitlab CI는 Gitlab Registry로부터 Docker 이미지를 받아옵니다. Docker 이미지에는 어플리케이션 환경이 설정되어 있습니다.</li>
  <li>Docker 컨테이너가 실행되면 첫번째 job에 정의된 대로 필요한 패키지를 설치하고 빌드를 수행합니다.</li>
  <li>빌드가 통과되면 두번째 job에 정의된 대로 테스트를 수행합니다.</li>
  <li>테스트가 통과되면 세번째 job에 정의된 대로 배포 과정을 수행합니다.</li>
  <li>
    <p>각 과정은 모두 Slack 알림으로 확인할 수 있습니다.</p>

    <p>​</p>
  </li>
</ol>

<p><img src="/assets/images/gitlab-pipeline.png" alt="Gitlab" /></p>

<p>위와 같이 모든 과정을 <code class="highlighter-rouge">Gitlab Pipeline</code>을 통해 확인하실 수 있습니다.</p>

<p>Gitlab의 단점이라면 Community 버전의 서버가 조금 불안정하다는 점입니다.
물론 설치형 Gitlab을 사용하신다면 이런 단점마저 존재하지 않습니다.
소규모의 팀이라면 충분히 도입을 검토해볼만 하다고 생각합니다.</p>

<p>​   ​</p>

	  ]]></description>
	</item>

	<item>
	  <title>올바른 Dockerfile 작성을 위한 가이드라인</title>
	  <link>//dockerfile-ignore</link>
	  <author>Swalloow</author>
	  <pubDate>2017-03-28T19:18:00+09:00</pubDate>
	  <guid>//dockerfile-ignore</guid>
	  <description><![CDATA[
	     <p>​   ​</p>

<p>Docker가 처음이라면, 이전 포스팅을 참고하시기 바랍니다.</p>

<ul>
  <li><a href="https://swalloow.github.io/docker-install">Docker 간편한 설치부터 실행까지</a></li>
  <li><a href="https://swalloow.github.io/docker-command">Docker, DockerHub 명령어 정리</a></li>
  <li><a href="https://swalloow.github.io/dockerfile">파이썬을 위한 Dockerfile 작성하기</a></li>
  <li>
    <p><a href="https://swalloow.github.io/dockerfile-ignore">올바른 Dockerfile 작성은 위한 가이드라인</a></p>

    <p>​</p>
  </li>
</ul>

<h2 id="dockerfile">Dockerfile</h2>

<p>Dockerfile은 일종의 이미지 설정파일입니다.
생긴 모양새는 쉘 스크립트와 유사하지만 자체의 문법을 가지고 있습니다.
이렇게 작성된 Dockerfile은 <code class="highlighter-rouge">build</code> 명령어를 통해 이미지를 생성할 수 있습니다.</p>

<p>이 포스팅에서는 Dockerfile 레퍼런스에 나와 있는 가이드라인을 정리해보도록 하겠습니다.
<a href="https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/">https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/</a>에 자세한 내용이 설명되어 있습니다.</p>

<p>​</p>

<h4 id="section">컨테이너는 일시적이어야 한다</h4>

<p>일시적이라는 말은 가능한 최소한의 설정 및 구성으로 이루어져있어야 한다는 것을 의미합니다.
이에 대한 내용은 <a href="https://12factor.net/">Twelve Factors Application</a>을 참고하시면 좋습니다.</p>

<p>​   ​</p>

<h4 id="dockerignore-">.dockerignore을 활용하자</h4>

<p>대부분의 경우 각 Docker 파일을 빈 디렉토리에 저장하는 것이 가장 좋습니다.
그런 다음 Dockerfile을 빌드하는 데 필요한 파일만 해당 디렉토리에 추가하시면 됩니다.
빌드의 성능을 높이려면 해당 디렉토리에 <code class="highlighter-rouge">.dockerignore</code> 파일을 추가하여 파일 및 디렉토리를 제외 할 수 있습니다.
<code class="highlighter-rouge">.dockerignore</code> 파일은 <code class="highlighter-rouge">.gitignore</code> 파일과 유사하게 동작한다고 보시면 됩니다.</p>

<figure class="highlight"><pre><code class="language-markdown" data-lang="markdown"><span class="err">*</span>.md
!README.md</code></pre></figure>

<p>위와 같은 <code class="highlighter-rouge">.dockerignore</code> 파일은 <em>README.md</em> 파일을 제외한 모든 마크다운 파일을 제외시킵니다.
이런식으로 원하지 않는 파일 및 디렉토리를 제외시켜 이미지의 용량을 줄일 수 있습니다.</p>

<p>​</p>

<h4 id="section-1">불필요한 패키지를 설치하지 말자</h4>

<p>복잡성, 의존성, 파일 크기 및 빌드 시간을 줄이기 위해서는 불필요한 패키지를 설치하지 말아야 합니다.
예를 들어, 데이터베이스 이미지에 텍스트 편집기를 포함시킨다거나 하는 일은 없어야 합니다.</p>

<p>​</p>

<h4 id="section-2">컨테이너는 오직 하나의 관심사만 갖는다</h4>

<p>애플리케이션을 여러 컨테이너로 분리하면 컨테이너를 확장하고 재사용하는 것이 훨씬 쉬워집니다.
예를 들어, 일반적인 어플리케이션은 웹 어플리케이션, 데이터베이스, 인메모리-캐시와 같이 세 개의 컨테이너로 구성 될 수 있습니다.</p>

<p><strong>컨테이너 당 하나의 프로세스</strong> 가 있어야한다는 말을 들어 보셨을 겁니다.
하지만, 언제나 컨테이너 당 하나의 운영 체제 프로세스만 있어야 하는 것은 아닙니다.
컨테이너가 init 프로세스로 생성 될 수 있다는 사실 외에도 일부 프로그램은 자체적으로 추가 프로세스를 생성 할 수 있습니다.
예를 들어 Celery는 여러 작업자 프로세스를 생성하거나 Apache 스스로 요청에 따른 프로세스를 생성 할 수 있습니다.
컨테이너를 깔끔한 모듈 형식으로 유지하기 위해 신중히 선택해야 합니다.
컨테이너에 서로 의존성이 생기는 경우 Docker 컨테이너 네트워크를 사용하여 서로 통신 할 수 있습니다.</p>

<p>​</p>

<h4 id="section-3">레이어의 수를 최소화하자</h4>

<p>사용하는 레이어의 수에 대해 전략적이고 신중해야합니다.
장기적인 관점에서 보았을 때 유지보수를 위해서는 레이어의 수를 최소화하는 것이 현명한 선택이 될 수 있습니다.</p>

<p>​</p>

<h4 id="section-4">줄바꿈을 사용하여 정렬하자</h4>

<figure class="highlight"><pre><code class="language-dockerfile" data-lang="dockerfile">RUN apt-get update &amp;&amp; apt-get install -y \
  bzr \
  cvs \
  git</code></pre></figure>

<p>위와 같이 줄바꿈을 사용하면, 패키지의 중복을 피하고 목록을 훨씬 쉽게 업데이트 할 수 있습니다.
백 슬래시 (<code class="highlighter-rouge">\</code>) 앞에 공백을 추가하면 가독성을 높이는 데에 도움이됩니다.</p>

<p>​</p>

<h4 id="section-5">캐시를 활용하여 빌드하자</h4>

<p>이미지를 작성하는 과정에서 Docker는 지정한 순서대로 Dockerfile을 단계 별로 실행합니다.
각 명령을 실행할 때 Docker는 매번 새로운 이미지를 만드는 대신 캐시에서 기존 이미지를 찾아 재사용 할 수 있습니다.
캐시를 전혀 사용하지 않으려는 경우 docker 빌드 명령에서 <code class="highlighter-rouge">--no-cache = true</code> 옵션을 사용하시면 됩니다.</p>

<p>Docker가 캐시를 사용하게하려면 일치하는 이미지를 찾을 때와 그렇지 않을 때를 이해하는 것이 매우 중요합니다.
Docker cache의 기본 규칙은 다음과 같습니다.</p>

<ul>
  <li>
    <p>이미 캐시에 있는 기본 이미지로 시작하여 다음 명령어가 해당 기본 이미지에서 파생된 모든 하위 이미지와 비교되어 그 중 하나가 정확히 동일한 명령어를 사용하여 빌드되었는지 확인합니다. 그렇지 않으면 캐시가 무효화됩니다.</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">ADD, COPY</code> 명령을 제외하고 캐시 검사는 컨테이너의 파일을보고 캐시 일치를 판별하지 않습니다. 예를 들어 <code class="highlighter-rouge">RUN apt-get -y update</code> 명령을 처리 할 때 컨테이너에서 업데이트 된 파일은 캐시 히트가 있는지 여부를 확인하기 위해 검사되지 않습니다. 이 경우 명령 문자열 자체만 일치하는지 확인합니다.</p>
  </li>
  <li>
    <p>캐시가 무효화되면 이후의 모든 Dockerfile 명령은 새로운 이미지를 생성하고 캐시는 사용되지 않습니다.</p>
  </li>
</ul>

<p>​   ​</p>

	  ]]></description>
	</item>


</channel>
</rss>
