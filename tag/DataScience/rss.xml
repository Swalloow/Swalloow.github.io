<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>swalloow.github.io/</title>
   
   <link>http://swalloow.github.io/</link>
   <description>About Data Science, Data Engineering</description>
   <language>ko-KO</language>
   <managingEditor> Swalloow</managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>Deep Learning Programming Style: Symbolic, Imperative</title>
	  <link>//deep-learning-style</link>
	  <author>Swalloow</author>
	  <pubDate>2018-01-05T19:18:00+09:00</pubDate>
	  <guid>//deep-learning-style</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>TensorFlow 1.5 버전부터 <a href="https://github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/contrib/eager">Eager Execution</a> 이라는 기능이 추가되었습니다.
다시 말해서 <code class="highlighter-rouge">imperative programming style</code>을 지원한다고 적혀있는데, 기존의 방식과 어떤 차이가 있는지 알아보겠습니다.
MXNet의 <a href="https://mxnet.incubator.apache.org/architecture/program_model.html">Deep Learning Programming Style</a> 문서를 번역한 내용입니다.</p>

<p>​</p>

<h2 id="deep-learning-programming-style">Deep Learning Programming Style</h2>
<p>우리는 항상 <strong>성능과 최적화</strong>에 대한 고민을 합니다. 하지만 그 이전에 잘 동작하는 코드인지 여부가 중요합니다. 이제는 다양한 딥러닝 라이브러리들이 존재하지만 각자 프로그래밍 방식에 대해 다른 접근 방식을 가지고 있기 때문에 학습하는 것도 힘들며, 이를 이용하여 명확하고 직관적인 deep learning 코드를 작성하는 것도 어렵습니다.</p>

<p>이 문서에서는 가장 중요한 두 가지 디자인 패턴에 집중하려고 합니다.</p>

<ol>
  <li>
    <p>Whether to embrace the symbolic or imperative paradigm for mathematical computation.</p>
  </li>
  <li>
    <p>Whether to build networks with bigger (more abstract) or more atomic operations.</p>
  </li>
</ol>

<hr />

<h2 id="symbolic-vs-imperative-programs">Symbolic vs. Imperative Programs</h2>

<p>만일 당신이 파이썬 또는 C++ 개발자라면, 이미 <code class="highlighter-rouge">Imperative program</code>과 친숙할 것 입니다.
Imperative style program들은 바로 연산을 수행합니다. 대부분의 파이썬 코드들이 imperative 한 형태를 보여주는데, 예를 들면 아래와 같은 Numpy 코드를 말합니다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">b</span> <span class="o">*</span> <span class="n">a</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">1</span></code></pre></figure>

<p>프로그램이 <code class="highlighter-rouge">c = b * a</code>를 수행하도록 명령을 내리면, 실제로 연산이 실행됩니다.</p>

<p>반면에 <code class="highlighter-rouge">Symbolic program</code>은 조금 다릅니다. Symbolic-style program에서는 먼저 function (potentially complex) 을 정의합니다. <strong>function을 정의했다고 해서 실제 연산이 수행되는 것은 아닙니다.</strong> 우리는 그저 placeholder 값에 function을 정의한 것 뿐 입니다. 이 과정 이후에 function을 컴파일 할 수 있으며, 실제 입력 값을 통해 이를 평가하게 됩니다. 아래는 위에서 언급했던 imperative 코드를 symbolic style로 변환한 예제입니다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">A</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="s">'A'</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="s">'B'</span><span class="p">)</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">B</span> <span class="o">*</span> <span class="n">A</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">C</span> <span class="o">+</span> <span class="n">Constant</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="c"># compiles the function</span>
<span class="n">f</span> <span class="o">=</span> <span class="nb">compile</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">A</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">B</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span></code></pre></figure>

<p>보시다시피 symbolic 버전에서는 <code class="highlighter-rouge">C = B * A</code>가 수행되는 시점에 실제로 연산이 일어나지 않습니다. 대신에 이 operation은 연산 과정을 표현하는 <strong>computation graph (aka. symbolic graph)</strong> 를 생성합니다. 예를 들면, D의 연산을 위해 아래와 같은 computation graph가 생성됩니다.</p>

<p style="text-align: center;"><img src="http://drive.google.com/uc?export=view&amp;id=1hjs2EOtub_KjPqj4nflGGR1Rjcaklp7M" alt="comp_graph" /></p>

<p>대부분의 symbolic-style 프로그램들은 명시적으로든 암시적으로든 컴파일 단계를 포함합니다. 이를 통해 computation graph를 언제든 호출할 수 있는 함수로 변환시켜줍니다. 위의 예제에서도 실제 연산은 코드의 마지막 줄에서만 수행됩니다. 이를 통해 얻을 수 있는 점은 computation graph를 작성하는 단계와 실행하는 단계를 명확히 분리할 수 있다는 것 입니다. Neural Network에서도 우리는 전체 모델을 단일 computation graph로 정의합니다.</p>

<p><code class="highlighter-rouge">Torch</code>, <code class="highlighter-rouge">Chiner</code> 그리고 <code class="highlighter-rouge">Minerva</code>와 같은 딥러닝 라이브러리들은 imperative style을 사용하고 있습니다. symbolic-style을 사용하는 딥러닝 라이브러리로는 <code class="highlighter-rouge">Theano</code>, <code class="highlighter-rouge">CGT</code> 그리고 <code class="highlighter-rouge">TensorFlow</code>가 있습니다. 그리고 <code class="highlighter-rouge">CXXNet</code> 이나 <code class="highlighter-rouge">Caffe</code>와 같은 라이브러리들은 설정파일에 의존하는 방식으로 symbolic style을 지원합니다. (ex. Caffe의 prototxt)
이제 두 가지 딥러닝 프로그래밍 방식에 대해 이해했으니, 각 방식의 장점에 대해 알아보겠습니다.</p>

<hr />

<h2 id="imperative-programs-tend-to-be-more-flexible">Imperative Programs Tend to be More Flexible</h2>

<p>imperative program은 프로그래밍 언어의 flow와 상당히 잘 맞아들어가며 유연하게 동작하는 것 처럼 보입니다. 그렇다면 왜 수 많은 딥러닝 라이브러리들이 symbolic 패러다임을 선택할까요? 가장 큰 이유는 <strong>메모리 사용량과 속도 측면에서의 효율성</strong> 때문입니다. 위에서 언급했던 예제로 돌아가 천천히 설명드리겠습니다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">b</span> <span class="o">*</span> <span class="n">a</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">1</span>
<span class="o">...</span></code></pre></figure>

<p style="text-align: center;"><img src="http://drive.google.com/uc?export=view&amp;id=1hjs2EOtub_KjPqj4nflGGR1Rjcaklp7M" alt="comp_graph" /></p>

<p>주어진 array의 각 셀이 8 바이트의 메모리를 소모한다고 가정해보겠습니다. 콘솔에서 위의 프로그램을 실행하면 메모리가 얼마나 소모될까요?</p>

<p>imperative program에서는 각 라인마다 메모리 할당이 요구됩니다. 사이즈가 10인 array가 4개 할당되므로 <code class="highlighter-rouge">4 * 10 * 8 = 320 bytes</code>의 메모리가 요구됩니다.
반면 computation graph에서는 궁극적으로 d가 필요하다는 것을 알고 있기 때문에, 즉시 값을 메모리에 할당하는 대신에 <strong>메모리를 재사용</strong>할 수 있습니다. 예를 들어 b를 위해 할당된 공간에 c를 저장하도록 재사용하고, c를 위해 할당된 공간에 다시 d를 저장하도록 한다면 결국 요구되는 메모리는 <code class="highlighter-rouge">2 * 10 * 8 = 160 bytes</code> 절반으로 줄어들게 됩니다.</p>

<p style="text-align: center;"><img src="http://drive.google.com/uc?export=view&amp;id=1L42AX9v9qyDYbx2qkw_rGlubWOm-NzJD" alt="comp_graph_fold" /></p>

<p>Symbolic program은 사실 이보다 더 엄격합니다. 우리가 D에 대한 컴파일을 호출하면, 시스템은 오직 d 값이 필요하다는 사실만 인지합니다. 따라서 위와 같은 경우, 즉시 연산에 의해 c는 존재하지 않는 값으로 취급합니다.</p>

<p>symbolic program이 안전하게 메모리를 재사용함으로 인해 우리가 얻는 장점은 분명 있습니다. 하지만, 나중에 우리가 c에 대해 접근해야하는 경우가 생긴다면 난감해집니다. 따라서 imperative program은 모든 가능한 경우의 수에 접근해야 할 때 더 좋은 대안이 될 수 있습니다. 대표적으로 파이썬 콘솔에서 imperative 버전의 코드를 실행시킨다면, 미래에 발생할 수 있는 변수를 중간 과정을 통해 미리 검사할 수 있습니다.</p>

<p>Symbolic program은 <code class="highlighter-rouge">operation folding</code> 최적화도 수행해줍니다. 다시 위의 예시를 살펴보면 곱셈과 합 연산이 하나의 operation으로 합쳐지는 것을 그래프를 통해 확인할 수 있습니다. 만일 연산이 GPU 프로세서에 의해 실행된다면, 두 개가 아닌 하나의 GPU 커널만 실행될 것 입니다. 실제로 이는 CXXNet, Caffe와 같은 라이브러리에서 연산을 수행하는 방식입니다. Operation folding 방식은 계산 효율을 향상시켜줍니다.</p>

<p>아시다시피 imperative program에서는 중간 값이 나중에 참조될 수 있기 때문에 operation folding 방식을 수행할 수 없습니다. 반면, computation graph에서는 전체 계산 그래프를 얻을 수 있고 어떤 값을 필요로하는지 알 수 있기 때문에 operation folding이 가능합니다.</p>

<hr />

<h2 id="case-study-backprop-and-autodiff">Case Study: Backprop and AutoDiff</h2>

<p>이제 <code class="highlighter-rouge">auto differentiation</code>이나 <code class="highlighter-rouge">backpropagation</code>과 같은 문제를 통해 두 가지 프로그래밍 모델을 비교해보겠습니다. (chaining rule이 어떻게 동작하는지 보여주겠다)
미분은 모델을 훈련시키는 메커니즘이기 때문에 딥러닝에 있어 정말 중요합니다. 우선 대부분의 딥러닝 모델에서 loss function을 정의하는데 이는 모델이 예측한 값이 실제 값과 얼마나 멀리 떨어져 있는지를 말합니다. 그리고 나서 훈련 데이터를 모델에게 전달하고, 각 step에서 모델의 parameter를 업데이트하여 loss를 최소화합니다. 즉, parameter가 업데이트 하는 방향은 loss function 결과에 의해 결정됩니다.</p>

<p>imperative와 symbolic 방식 모두 <code class="highlighter-rouge">gradient</code> 계산을 수행할 수 있습니다. 먼저 아래의 파이썬 코드를 통해 imperative program이 어떻게 automatic differentiation을 수행하는지 알아보겠습니다.</p>

<script src="https://gist.github.com/Swalloow/ecb8f637d6966115d9d9870415fc5623.js"></script>

<hr />

<h2 id="model-checkpoints">Model Checkpoints</h2>

<p>모델을 저장하고 다시 불러오는 일 또한 중요합니다. 보통 Neural Network 모델을 저장한다는 것은 네트워크의 구조, 설정 값 그리고 weight 값의 저장을 의미합니다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">A</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="s">'A'</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="s">'B'</span><span class="p">)</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">B</span> <span class="o">*</span> <span class="n">A</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">C</span> <span class="o">+</span> <span class="n">Constant</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">D</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s">'mygraph'</span><span class="p">)</span>
<span class="n">D2</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s">'mygraph'</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="nb">compile</span><span class="p">([</span><span class="n">D2</span><span class="p">])</span>
<span class="c"># more operations</span>
<span class="o">...</span></code></pre></figure>

<p>설정 값을 체크하는 일은 symbolic program이 더 유리 합니다. symbolic 구조에서는 실제 연산을 수행할 필요가 없기 때문에 computation graph를 그대로 serialize 하면 됩니다.
반면에 Imperative program은 연산 할 때 실행되기 때문에 코드 자체를 설정 파일로 저장하거나 그 위에 또 다른 레이어를 구성해야합니다.</p>

<hr />

<h2 id="parameter-updates">Parameter Updates</h2>

<p>computation graph의 경우 연산과정은 쉽게 설명할 수 있지만 parameter 업데이트에 대해서는 명확하지 못합니다. parameter update는 기본적으로 값의 변경(mutation)을 요구하기 때문에 computation graph의 개념과 맞지 않습니다. 따라서 대부분의 symbolic program들은 persistent state를 갱신하기 위해 special update 구문을 사용하고 있습니다.</p>

<p>반면에 imperative style에서는 parameter 업데이트를 작성하는 것이 쉽습니다. 특히 서로 연관된 여러 업데이트가 필요할 때 더욱 그렇습니다. Symbolic program의 경우 업데이트 문은 사용자가 호출 할 때 실행됩니다. 이런 점에서 대부분의 symbolic deep learning 라이브러리는 parameter 업데이트에 대해 gradient 연산을 수행하면서 업데이트를 수행하는 imperative style로 다시 돌아갑니다.</p>

<hr />

<h2 id="there-is-no-strict-boundary">There Is No Strict Boundary</h2>

<p>두 가지 프로그래밍 스타일을 비교해서, 하나만 사용하라는 말이 아닙니다. 명령형 프로그램을 상징형 프로그램처럼 만들거나 그 반대도 가능합니다. 예를 들면, Python으로 JIT (just-in-time) 컴파일러를 작성하여 명령형 Python 프로그램을 컴파일 할 수 있습니다. 하지만 두 가지 아키텍쳐를 이해하는 것은 수 많은 딥러닝 라이브러리의 추상화와 그 차이를 이해하는데 도움이 됩니다. <strong>결국, 우리는 프로그래밍 스타일 간에 명확한 경계선이 없다고 결론을 내릴 수 있습니다.</strong></p>

<p>​</p>

	  ]]></description>
	</item>

	<item>
	  <title>Bagging과 Boosting 그리고 Stacking</title>
	  <link>//bagging-boosting</link>
	  <author>Swalloow</author>
	  <pubDate>2017-07-19T19:18:00+09:00</pubDate>
	  <guid>//bagging-boosting</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>오늘은 머신러닝 성능을 최대로 끌어올릴 수 있는 앙상블 기법에 대해 정리해보았습니다.</p>

<p>​</p>

<h3 id="ensemble-hybrid-method">Ensemble, Hybrid Method</h3>

<p>앙상블 기법은 동일한 학습 알고리즘을 사용해서 여러 모델을 학습하는 개념입니다.
Weak learner를 결합한다면, Single learner보다 더 나은 성능을 얻을 수 있다는 아이디어입니다.
<strong>Bagging</strong> 과 <strong>Boosting</strong> 이 이에 해당합니다.</p>

<p>동일한 학습 알고리즘을 사용하는 방법을 앙상블이라고 한다면,
서로 다른 모델을 결합하여 새로운 모델을 만들어내는 방법도 있습니다.
대표적으로 <strong>Stacking</strong> 이 있으며, 최근 Kaggle 에서 많이 소개된 바 있습니다.</p>

<p>​</p>

<h3 id="bagging">Bagging</h3>

<p>Bagging은 샘플을 여러 번 뽑아 각 모델을 학습시켜 결과를 <strong>집계(Aggregating)</strong> 하는 방법입니다.
아래의 그림을 통해 자세히 알아보겠습니다.</p>

<p><img src="/assets/images/boosting.png" alt="" /></p>

<p>먼저 대상 데이터로부터 비복원 랜덤 샘플링을 합니다.
이렇게 추출한 데이터가 일종의 표본 집단이 됩니다.
이제 여기에 동일한 모델을 학습시킵니다.
그리고 학습된 모델의 예측변수들을 집계하여 그 결과로 모델을 생성해냅니다.</p>

<p>이러한 방식을 <strong>Bootstrap Aggregating</strong> 이라고 부릅니다.</p>

<p>이렇게 하는 이유는 “알고리즘의 안정성과 정확성을 향상시키기 위해서” 입니다.
대부분 학습에서 나타나는 오류는 다음과 같습니다.</p>

<ol>
  <li>높은 bias로 인한 Underfitting</li>
  <li>높은 Variance로 인한 Overfitting</li>
</ol>

<p>앙상블 기법은 이러한 오류를 최소화하는데 도움이 됩니다.
특히 Bagging은 각 샘플에서 나타난 결과를 일종의 중간값으로 맞추어 주기 때문에,
Overfitting을 피할 수 있습니다.</p>

<p>일반적으로 Categorical Data인 경우, 투표 방식 (Voting)으로 집계하며
Continuous Data인 경우, 평균 (Average)으로 집계합니다.</p>

<p>대표적인 Bagging 알고리즘으로 <code class="highlighter-rouge">RandomForest</code> 모델이 있습니다.
원래 단일 DecisionTree 모델은 boundary가 discrete 한 모양일 수 밖에 없지만,
RandomForest는 여러 트리 모델을 결합하여 이를 넘어설 수 있게 되었습니다.</p>

<p>결과는 아래와 같습니다.</p>

<p><img src="/assets/images/agg_result.png" alt="" /></p>

<p>​</p>

<h3 id="boosting">Boosting</h3>

<p>Bagging이 일반적인 모델을 만드는데 집중되어있다면,
Boosting은 맞추기 어려운 문제를 맞추는데 초점이 맞춰져 있습니다.</p>

<p>수학 문제를 푸는데 9번 문제가 엄청 어려워서 계속 틀렸다고 가정해보겠습니다.
Boosting 방식은 9번 문제에 가중치를 부여해서 9번 문제를 잘 맞춘 모델을 최종 모델로 선정합니다.
아래 그림을 통해 자세히 알아보겠습니다.</p>

<p><img src="https://quantdare.com/wp-content/uploads/2016/04/bb3.png" alt="" /></p>

<p>Boosting도 Bagging과 동일하게 비복원 랜덤 샘플링을 하지만, 가중치를 부여한다는 차이점이 있습니다.
Bagging이 병렬로 학습하는 반면, Boosting은 순차적으로 학습시킵니다.
학습이 끝나면 나온 결과에 따라 가중치가 재분배됩니다.</p>

<p>오답에 대해 높은 가중치를 부여하고, 정답에 대해 낮은 가중치를 부여하기 때문에
오답에 더욱 집중할 수 있게 되는 것 입니다.
Boosting 기법의 경우, 정확도가 높게 나타납니다.
하지만, 그만큼 Outlier에 취약하기도 합니다.</p>

<p>가중치를 결정하는 방법에 따라 AdaBoost, XGBoost, GradientBoost 등 다양한 모델이 있습니다.
그 중에서도 XGBoost 모델은 강력한 성능을 보여줍니다. 최근 대부분의 Kaggle 대회 우승 알고리즘이기도 합니다.</p>

<p>​</p>

<h3 id="stacking">Stacking</h3>

<p><strong>Meta Modeling</strong> 이라고 불리기도 하는 이 방법은 위의 2가지 방식과는 조금 다릅니다.
“Two heads are better than one” 이라는 아이디어에서 출발합니다.</p>

<p>Stacking은 서로 다른 모델들을 조합해서 최고의 성능을 내는 모델을 생성합니다.
여기에서 사용되는 모델은 SVM, RandomForest, KNN 등 다양한 알고리즘을 사용할 수 있습니다.
이러한 조합을 통해 서로의 장점은 취하고 약점을 보완할 수 있게 되는 것 입니다.</p>

<p>Stacking은 이미 느끼셨겠지만 필요한 연산량이 어마어마합니다.
적용해보고 싶다면 아래의 StackNet을 사용하시는 걸 강력하게 추천합니다.</p>

<p><a href="https://github.com/kaz-Anova/StackNet">https://github.com/kaz-Anova/StackNet</a></p>

<p>문제에 따라 정확도를 요구하기도 하지만, 안정성을 요구하기도 합니다.
따라서, 주어진 문제에 적절한 모델을 선택하는 것이 중요합니다.</p>

<p>​</p>

	  ]]></description>
	</item>

	<item>
	  <title>DecisionTree와 RandomForest에 대하여</title>
	  <link>//decison-randomforest</link>
	  <author>Swalloow</author>
	  <pubDate>2017-02-10T19:18:00+09:00</pubDate>
	  <guid>//decison-randomforest</guid>
	  <description><![CDATA[
	     <p>​</p>

<h3 id="decisiontree">의사결정트리 (DecisionTree)</h3>

<p>의사결정나무는 다양한 의사결정 경로와 결과를 나타내는데 트리 구조를 사용합니다. 보통 어렸을 때의 스무고개 놀이를 예로 드는 경우가 많습니다.</p>

<p><img src="http://cfile3.uf.tistory.com/image/2720193757A374DF33F327" alt="img" /></p>

<p>​</p>

<p>위의 그림은 타이타닉 생존자를 찾는 의사결정트리 모델입니다. 첫번째 뿌리 노드를 보면 성별 &lt;= 0.5 라고 되어있는데 이는 남자냐? 여자냐? 라고 질문하는 것과 같습니다.</p>

<p>최종적으로, 모든 승객에 대한 분류(Classification)를 통해 생존확률을 예측할 수 있게 됩니다.</p>

<p>이처럼, 숫자형 결과를 반환하는 것을 <strong>회귀나무(Regression Tree)</strong> 라고 하며, 범주형 결과를 반환하는 것을 <strong>분류나무(Classification Tree)</strong> 라고 합니다. 의사결정트리를 만들기 위해서는 먼저 어떤 질문을 할 것인지, 어떤 순서로 질문을 할 것인지 정해야 합니다.</p>

<p>가장 좋은 방법은 예측하려는 대상에 대해 가장 많은 정보를 담고 있는 질문을 고르는 것이 좋습니다. 이러한 ‘얼마만큼의 정보를 담고 있는가’를 엔트로피(entropy) 라고 합니다. 엔트로피는 보통 데이터의 불확실성(?)을 나타내며, 결국 엔트로피가 클 수록 데이터 정보가 잘 분포되어 있기 때문에 좋은 지표라고 예상할 수 있습니다.</p>

<p>그림과 같이 의사결정트리는 이해하고 해석하기 쉽다는 장점이 있습니다. 또한 예측할 때 사용하는 프로세스가 명백하며, 숫자형 / 범주형 데이터를 동시에 다룰 수 있습니다. 그리고 특정 변수의 값이 누락되어도 사용할 수 있습니다.</p>

<p>하지만 최적의 의사결정트리를 찾는 것이 정말 어려운 문제입니다. (어떤 것들을 조건(Feature)으로 넣어야 할지, 깊이(Depth)는 얼마로 정해야 할지…) 그리고 의사결정트리의 단점은 새로운 데이터에 대한 일반화 성능이 좋지 않게 오버피팅(Overfitting)되기 쉽다는 것입니다.</p>

<p>잠깐 오버피팅에 대해 설명하자면, 오버피팅이란 Supervised Learning에서 과거의 학습한 데이터에 대해서는 잘 예측하지만 새로 들어온 데이터에 대해서 성능이 떨어지는 경우를 말합니다. 즉, 학습 데이터에 지나치게 최적화되어 일반화가 어렵다는 말입니다. 이러한 오버피팅을 방지할 수 있는 대표적인 방법 중 하나가 바로 앙상블 기법을 적용한 <strong>랜덤포레스트(Random Forest)</strong> 입니다.</p>

<p>​</p>

<h3 id="randomforest">랜덤포레스트 (RandomForest)</h3>

<p>랜덤포레스트는 위에서 말한 것과 같이 의사결정트리를 이용해 만들어진 알고리즘입니다.</p>

<blockquote>
  <p>랜덤포레스트는 분류, 회귀 분석 등에 사용되는 앙상블 학습 방법의 일종으로, </p>

  <p>훈련 과정에서 구성한 다수의 결정 트리로부터 분류 또는 평균 예측치를 출력함으로써 동작한다.</p>
</blockquote>

<p>​</p>

<p>즉, 랜덤포레스트란 여러 개의 의사결정트리를 만들고, 투표를 시켜 다수결로 결과를 결정하는 방법입니다.</p>

<p>​</p>

<p><img src="http://cfile24.uf.tistory.com/image/2325343B57A37515289AD9" alt="img" /></p>

<p>위의 그림은 고작 몇 십개의 트리노드가 있지만 실제로는 수 백개에서 수 만개까지 노드가 생성될 수 있습니다. 이렇게 여러 개의 트리를 통해 투표를 해서 오버피팅이 생길 경우에 대비할 수 있습니다.</p>

<p>그런데 보통 구축한 트리에는 랜덤성이 없는데 어떻게하면 랜덤하게 트리를 얻을 수 있나? 라는 의문이 듭니다. 랜덤포레스트에서는 데이터를 bootstrap 해서 포레스트를 구성합니다.</p>

<p>bootstrap aggregating 또는 begging 이라고 하는데, 전체 데이터를 전부 이용해서 학습시키는 것이 아니라 샘플의 결과물을 각 트리의 입력 값으로 넣어 학습하는 방식입니다. 이렇게 하면 각 트리가 서로 다른 데이터로 구축되기 때문에 랜덤성이 생기게 됩니다. 그리고 파티션을 나눌 때 변수에 랜덤성을 부여합니다. 즉, 남아있는 모든 변수 중에서 최적의 변수를 선택하는 것이 아니라 변수 중 일부만 선택하고 그 일부 중에서 최적의 변수를 선택하는 것입니다.</p>

<p>이러한 방식을 <strong>앙상블 기법(ensemble learning)</strong> 이라고 합니다. 랜덤포레스트는 아주 인기가 많고 자주 사용되는 알고리즘 중 하나입니다. 샘플링되지 않은 데이터를 테스트 데이터로 이용할 수 있기 때문에 데이터 전체를 학습에 사용할 수 있으며, 의사결정트리에 비해 일반화도 적용될 수 있습니다.</p>

<p>하지만 실제로 테스트 해보면 꼭 모든 경우에 뛰어나다고 할 수는 없습니다. (예를 들면 데이터 셋이 비교적 적은 경우)</p>

<p>​</p>

<h2 id="section">실제로 사용해보자</h2>

<p>이렇게 이론을 공부하고 나면 실제로 적용해보는 예측모델을 만들고 싶어집니다.
정말 고맙게도 파이썬의 scikit-learn에 다양한 트리 모델이 구현되어 있습니다.
다른 앙상블 모델 뿐만 아니라 RandomForest까지 제공합니다.</p>

<p>공식 레퍼런스는 아래의 링크를 참조</p>

<p><a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</a></p>

<p><img src="/assets/images/decisiontree_load.png" alt="decisiontree_load" /></p>

<p>방법은 간단합니다. sklearn.tree에 있는 model 을 import 합니다.
skikit-learn의 모델들은 대부분 파라메터로 X, y 값을 넣는다는 공통점이 있습니다.
여기서 X는 input 또는 feature가 되고, y는 output이 됩니다.</p>

<p><img src="/assets/images/decisiontree_fit.png" alt="decisiontree_fit" /></p>

<p>model.fit(X, y)를 하면 모델에 대한 정보가 출력됩니다. 최초에 모델을 로드할 때 random_state 값만 조정했기 때문에 전부 다 default 값이 적용된 걸 볼 수 있습니다.</p>

<p>RandomForest나 DecisionTree 같은 경우에는 max_depth, n_estimator에 따라 결과 값이 달라집니다. 데이터에 따라 다르지만, 보통 100-150 사이의 값 중에 성능이 가장 잘 나오는 값으로 결정합니다.</p>

<p>Kaggle Titanic에 적용한 스크립트는 아래의 링크를 참조하시면 됩니다.</p>

<p><a href="https://github.com/Swalloow/Kaggle/blob/master/Titanic%20Survivors/Titanic%20Tree%20Modeling.ipynb">https://github.com/Swalloow/Kaggle/blob/master/Titanic%20Survivors/Titanic%20Tree%20Modeling.ipynb</a></p>

<p>​</p>

	  ]]></description>
	</item>

	<item>
	  <title>머신러닝을 시작하기 위한 기초 지식 (2)</title>
	  <link>//pyml-intro2</link>
	  <author>Swalloow</author>
	  <pubDate>2017-02-08T19:18:00+09:00</pubDate>
	  <guid>//pyml-intro2</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>이번 포스팅에서는 데이터를 분석하기 위한 과정에 대해 하나하나 간단히 알아보겠습니다.
일반적으로 어떠한 데이터로부터 어떠한 가치를 창출하고자 할 때, 다음과 같이 <strong>[전처리 - 모델링 - 평가 - 예측]</strong> 프로세스를 따르게 됩니다.</p>

<p><img src="/assets/images/process.png" alt="process" /></p>

<p>​</p>

<h2 id="preprocessing">Preprocessing</h2>

<p>전처리 과정은 <strong>Preprocessing</strong> 또는 <strong>Data Cleaning</strong> / <strong>Wrangling</strong> /  <strong>Munging</strong> 과 같이 다양한 용어로 불립니다. 이 과정에서 어떤 일을 하는지 한마디로 요약하자면 비정형 데이터를 정형화시키는 작업을 합니다. 대부분의 우리가 수집한 데이터는 쓰레기의 집합에 가깝습니다. 따라서, 원하는 형태에 맞게 수정 / 변환해주는 작업이 필요한 것입니다.</p>

<p>비정형 데이터를 가치있는 데이터로 변환하기 이전에, 먼저 어떻게 생겼는지 확인해보는 것이 좋습니다. 실제 Kaggle에 가보면 많은 사이언티스트들이 데이터를 시각화해놓은 스크립트를 많이 볼 수 있습니다. 이러한 과정을 통해 모델링 이전에 인사이트를 얻게 된다면, 이후에 많은 시간을 절약할 수 있게 됩니다.</p>

<p>대충 확인했다면 이제 데이터를 정형화시키는 작업을 진행합니다. 이 단계는 데이터에 따라 다르겠지만 약간 노가다 작업이 많이 들어갑니다. 텍스트 날짜 데이터를 datetime 객체로 변환하고 날짜별로 정리한다던지, Null 데이터를 적절한 값으로 채우거나 버리는 작업이 이에 해당합니다.</p>

<p>만일 텍스트 데이터를 분석해야 한다면, 상황에 따라 다음과 같은 과정이 필요할 것입니다. 이 부분에 대해서는 나중에 자세히 다루겠습니다.</p>

<ul>
  <li>Stopword 제거</li>
  <li>한글 형태소 분석</li>
  <li>개체명 인식</li>
  <li>One-Hot Encoding</li>
  <li>Word Embedding</li>
  <li>그 밖에 여러가지</li>
</ul>

<p>데이터가 어느정도 깨끗해지고 나면 Training Dataset과 Test Dataset을 나누어 줘야 합니다. Training Dataset으로 학습하고 나면, 모델이 그 데이터에 최적화 되어 있기 때문에 학습에 사용하지 않은 데이터로 나누어주는 것이 좋습니다. Coursera 강의를 보면 일반적으로 Training Dataset을 70%, Test Dataset을 30%로 나누는게 적당하다고 말합니다.</p>

<p>​</p>

<h2 id="modeling">Modeling</h2>

<p>모델링 과정은 적절한 Learning Algorithm을 선택하여 Training Dataset을 넣고 학습하는 과정입니다. 먼저 Model Selection 같은 경우, 분석하고자 하는 과정이 Classification인지, Regression인지 등 여러 경우와 데이터에 따라 적절한 모델을 선택하면 됩니다. 이 과정에서 통계적 지식이 조금 요구됩니다.</p>

<p>이렇게 모델을 만들고 나서 바로 Test Dataset 과 비교하는 것이 아니라, <strong>Cross-Validation</strong> 과정을 거치게 됩니다. Cross-Validation은 학습에 사용된 Training Dataset 중 일부분을 나누어 검증하는 데 사용하는 것을 말합니다. 왜 중간에 검증 과정이 필요한가 하면, 우리가 가지고 있는 데이터는 전체 데이터 중 일부를 샘플링 한 데이터이기 때문에 신뢰도가 100%일 수 없습니다. 따라서, Cross-Validation은 통계적으로 신뢰도를 높이기 위한 과정입니다. 대표적인 방법으로 <strong>K-Fold</strong>와 <strong>Bootstrap</strong>이 있습니다.</p>

<p>그리고 모델의 성능을 높이기 위한 방법으로 <strong>Hyperparameter Optimization</strong> 이 있습니다. Parameter Tuning 이라고 부르기도 합니다. 간단히 말해서 모델에 들어가는 파라메터들을 최적의 값으로 튜닝하는 것입니다. 딥러닝의 경우 뉴럴넷을 통해 알아서 최적화 되지만, 머신러닝 알고리즘의 경우 결과 값을 확인하여 최적화하는 경우가 많습니다. 대표적으로 <strong>Grid Search</strong>, <strong>Random Search</strong> 등의 방법이 있습니다.</p>

<p>​</p>

<h2 id="evaluation--prediction">Evaluation &amp; Prediction</h2>

<p>마지막은 모델을 평가하고 결과를 예측하는 과정입니다. 모델을 평가한다는 것은 말 그대로 이 모델이 좋은지 별로인지, 너무 Training Dataset 에만 최적화 된 것은 아닌지 등을 평가하는 작업을 말합니다. 일반적으로 모델 평가에는 <strong>performance matrix</strong> 나 <strong>loss function</strong> 이 사용됩니다. Log Loss, F1 Score, RMSE 등의 값들이 이에 해당합니다.</p>

<p>​</p>

<h2 id="python-library">Python Library</h2>

<p>다행히 파이썬에는 일련의 과정에 필요한 패키지들이 다양하게 제공됩니다. 그 중에서 제가 자주 쓰는 라이브러리를 간단히 정리해보았습니다.</p>

<h4 id="data-analysis">Data Analysis</h4>

<ul>
  <li>Numpy, Pandas</li>
  <li>Scikit-learn, Scipy</li>
  <li>Gensim, NLTK</li>
  <li>TensorFlow</li>
</ul>

<h4 id="data-visualization">Data Visualization</h4>

<ul>
  <li>Matplotlib, Seaborn</li>
  <li>
    <p>Chart.js, HighChart.js, D3.js</p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>머신러닝을 시작하기 위한 기초 지식 (1)</title>
	  <link>//pyml-intro1</link>
	  <author>Swalloow</author>
	  <pubDate>2017-02-05T19:18:00+09:00</pubDate>
	  <guid>//pyml-intro1</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>이 글의 목차나 그림은 <strong>Sebastian Rashka - Python Machine Learning</strong> 을 참고하였습니다.</p>

<p>사실 기계학습, 인공지능에 대한 연구는 예전부터 존재했지만 발전이 없었으며 소수에 연구원들에 의한 주제였기에 대중화 될 수 없었습니다. 하지만 풍부한 데이터의 확보, 컴퓨팅 성능향상, 오픈소스 라이브러리로 인해 많은 개발자들이 인공지능 연구에 참여하게 되었습니다.</p>

<p>이 글에서는 기계학습에 대한 간략한 소개와 데이터 분석 시스템을 어떻게 디자인해야 되는지, 마지막으로 파이썬을 이용한 데이터 분석에 대해 소개해드리겠습니다.</p>

<p>​</p>

<p><img src="/assets/images/ml-diagram.png" alt="ml-diagram" /></p>

<p>요즘에는 머신러닝을 크게 3가지 분야로 나누어 볼 수 있습니다. 흔히 알고있는 <strong>지도학습(Supervised Learning)</strong> 과 <strong>비지도학습(Unsupervised Learning)</strong> 이 있으며, 마지막으로 알파고에 적용되었던 <strong>강화학습(Reinforcement Learning)</strong> 이 있습니다. 이제 세 가지 다른 알고리즘 간의 근본적인 차이점에 대해 알아보고, 실제로 어떤 문제에 적용되는지 알아보겠습니다.</p>

<p>​</p>

<h2 id="supervised-learning">Supervised Learning</h2>

<p>지도학습과 비지도학습의 궁극적인 목표는 모두 과거, 현재의 데이터로부터 미래를 예측하는 것이라 할 수 있습니다. 다만, 두 가지 방법의 차이점은 라벨링 된 데이터인지 아닌지에 따라 결정됩니다.</p>

<p>만일 내가 가지고 있는 데이터가 라벨링 되어 있다면 지도학습이라고 볼 수 있습니다. 여기에서 라벨링 된 데이터(Labeled data)란 데이터에 대한 답이 주어져 있는 것 (평가가 되어 있는 것) 을 말합니다.</p>

<p><img src="/assets/images/learning-diagram.png" alt="learning-diagram" /></p>

<p>​</p>

<h4 id="classification">Classification</h4>

<p>지도학습은 Classification과 Regression으로 나누어집니다. 먼저, Classification은 주어진 데이터를 정해진 카테고리에 따라 분류하는 문제를 말합니다. 최근에 많이 사용되는 이미지 분류도 Classification 문제 중에 하나입니다.</p>

<p>예를 들어, 이메일이 스팸메일인지 아닌지를 예측한다고 하면 이메일은 스팸메일 / 정상적인 메일로 라벨링 될 수 있을 것입니다. 비슷한 예시로 암을 예측한다고 가정했을 때 이 종양이 악성종양인지 / 아닌지로 구분할 수 있습니다. 이처럼 맞다 / 아니다로 구분되는 문제를 <strong>Binary Classification</strong> 이라고 부릅니다.</p>

<p>분류 문제가 모두 맞다 / 아니다로 구분되지는 않습니다. 예를 들어, 공부시간에 따른 전공 Pass / Fail 을 예측한다고 하면 이는 Binary Classifiaction 으로 볼 수 있습니다. 반면에, 수능 공부시간에 따른 전공 학점을 A / B / C / D / F 으로 예측하는 경우도 있습니다. 이러한 분류를 <strong>Multi-label Classification</strong> 이라고 합니다.</p>

<p>​</p>

<h4 id="regression">Regression</h4>

<p>다음으로 Regression은 연속된 값을 예측하는 문제를 말합니다. 주로 어떤 패턴이나 트렌드, 경향을 예측할 때 사용됩니다. Coursera에서는 Regression을 설명할 때 항상 집의 크기에 따른 매매가격을 예로 듭니다. 아까와 유사한 예를 들자면, 공부시간에 따른 전공 시험 점수를 예측하는 문제를 예로 들 수 있습니다.</p>

<p>​</p>

<h2 id="unsupervised-learning">Unsupervised Learning</h2>

<p>비지도학습은 앞에서 언급한 것 처럼 라벨링이 되어 있지 않은 데이터로부터 미래를 예측하는 학습방법입니다. 평가되어 있지 않은 데이터로부터 숨어있는 패턴이나 형태를 찾아야 하기 때문에 당연히 더 어렵습니다. 비지도학습도 데이터가 분리되어 있는지 (Categorial data) 연속적인지 (Continuous data)로 나누어 생각할 수 있습니다.</p>

<p>대표적으로 클러스터링 (Clustering) 이 있습니다. 실제로는 그 데이터의 label이나 category가 무엇인지 알 수 없는 경우가 많기 때문에 이러한 방법이 중요하다고 볼 수 있습니다. 이외에도 차원축소(Dimentionality Reduction), Hidden Markov Model 등이 있습니다.</p>

<p>​</p>

<h2 id="reinforcement-learning">Reinforcement Learning</h2>

<p><img src="/assets/images/agent-environment.png" alt="ml-diagram" /></p>

<p>마지막으로 강화학습은 앞서 말했던 학습방법과는 조금 다른 개념입니다. 데이터가 정답이 있는 것도 아니며, 심지어 주어진 데이터가 없을 수도 있습니다. 강화학습이란, 자신이 한 행동에 대한 “보상”을 알 수 있어서 그로부터 학습하는 것을 말합니다.</p>

<p>예를 들면, 아이가 걷는 것을 배우는 것처럼 어떻게 행동할 줄 모르지만 환경과 상호작용하면서 걷는 법을 알아가는 것과 같은 학습 방법을 강화학습이라고 합니다.</p>

<p><img src="https://dnddnjs.gitbooks.io/rl/content/90-6.png" alt="atari" /></p>

<p>보통 아타리 게임 인공지능을 많이 예시로 드는데, 여기에서 학습 대상 (agent) 은 움직이면서 적을 물리치는 존재입니다. 이 학습 대상은 움직이면서 적을 물리치면 보상 (reward) 을 받게 됩니다. 이러한 과정을 스스로 반복 학습 (Trial and Error) 하면서 점수를 최대화하는 것이 목표입니다.</p>

<p>​</p>

<h2 id="how">How?</h2>

<p>처음에는 공부를 시작하기에 막막한데 다행히 아주 좋은 강의와 자료들이 많이 있습니다.</p>

<h4 id="machine-learning--deep-learning">Machine Learning / Deep Learning</h4>

<ul>
  <li>Coursera - Andrew Ng : <a href="https://www.coursera.org/learn/machine-learning">https://www.coursera.org/learn/machine-learning</a></li>
  <li>모두의 딥러닝 : <a href="https://hunkim.github.io/ml/">https://hunkim.github.io/ml/</a></li>
  <li>
    <p>CS231n : <a href="http://cs231n.stanford.edu/">http://cs231n.stanford.edu/</a></p>

    <p>​</p>
  </li>
</ul>

<h4 id="reinforcement-learning-1">Reinforcement Learning</h4>

<ul>
  <li>모두의 연구소 깃북 : <a href="https://dnddnjs.gitbooks.io/rl/content/">https://dnddnjs.gitbooks.io/rl/content/</a></li>
  <li>
    <p>Deep Mind - David Silver : <a href="https://www.youtube.com/watch?v=2pWv7GOvuf0">https://www.youtube.com/watch?v=2pWv7GOvuf0</a></p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>[CS224d] Lecture2. Word Vectors</title>
	  <link>//cs224d-lecture2</link>
	  <author>Swalloow</author>
	  <pubDate>2017-01-21T19:18:00+09:00</pubDate>
	  <guid>//cs224d-lecture2</guid>
	  <description><![CDATA[
	     <p>​</p>

<h3 id="how-do-we-represent-the-meaning-of-a-word">How do we represent the meaning of a word?</h3>

<p>“단어의 의미를 어떻게 표현해야 할까?”에 대한 고민은 예전부터 지속되어 왔다. 결국 사람이 단어나 몸짓으로 표현하는 것은 쉽지만 컴퓨터가 이를 이해하기는 어렵다.
마찬가지로 글쓰기, 미술 등으로 표현하는 것 또한 마찬가지이다.</p>

<p>과거에는 WordNet과 같은 방법을 사용했다. WordNet이란, 각 단어끼리의 관계(상위단어, 동의어) 가 나타나 있는 트리구조의 그래프 모형이다.
물론 이를 구축하기 위한 작업은 전부 사람이 했다. 그러다보니 주관적이고 유지하는데 있어 많은 노동이 필요하다는 한계가 존재했다.</p>

<p>​</p>

<h3 id="problems-with-this-discrete-representation">Problems with this discrete representation</h3>

<p>기존의 discrete representation에는 다음과 같은 문제점들이 존재한다.
우선, 아까와 같은 경우 리소스로는 충분하지만 뉘앙스가 부족하다. 비슷한 예로 동의어 문제가 있다.
<code class="highlighter-rouge">he's good / he's very proficient</code> 와 같은 문장을 비교해보면,
proficient는 보통 말을 능숙하게 하는 사람을 표현하듯이 문맥에 따라 의미가 달라질 것이다.</p>

<p>​</p>

<p>두번째 이유는 매번 신조어가 나타나는데 이를 최신화하기 어렵다.
wicked, badass, ninjia… (미국에서 쓰는 신조어인듯 싶다.)</p>

<p>​</p>

<p>세번째 이유는 주관적이고 유지하는데 사람의 노동이 필요하다.
사람들이 직접 구축한 것이기 때문에 주관적이다.
특히 WordNet은 영어 이외의 언어에서 잘 구축된 경우가 별로 없다.
(국내의 몇몇 대학원에서 구축된 것이 있지만 절대 공개하지 않는다…)</p>

<p>​</p>

<p>네번째, 단어 간의 유사도를 계산하기 어렵다.
어떤 단어가 어느정도 동의어인지 아닌지 계산하기도 어렵다는 말이다.</p>

<p>​</p>

<p>그래서 보통 Rule-based 와 통계적 NLP를 사용하는 모델들은 단어를 <code class="highlighter-rouge">atomic symbol</code>로써 사용한다.
예를 들어, 2만 개의 단어 중에 <code class="highlighter-rouge">hotel</code> 이라는 단어를 vector space로 나타낸다면,
<code class="highlighter-rouge">[0, 0, 0, 0, 0, 0, ..., 0, 0, 1, 0, 0, 0]</code> 이런 식이다. (2만 개의 차원)
이러한 경우, <code class="highlighter-rouge">motel</code> 이라는 단어와 AND operation을 통해 유사도를 계산한다면
무조건 0이 나오게 될 것이다. (별로 좋지 않다)</p>

<p>​</p>

<p>이러한 벡터를 <code class="highlighter-rouge">One-Hot Vector</code> 라고 부르며,
이러한 NLP 방법론을 <code class="highlighter-rouge">Bag of Words Representation</code> 이라고 부른다.</p>

<p>​</p>

<h3 id="distributional-similarity-based-representations">Distributional similarity based representations</h3>

<p>이 방법은 현대 통계적 NLP 방법론 중에서 가장 좋게 평가받는 모델 중 하나이다.</p>

<p><img src="assets/images/distributional.png" alt="distributional" /></p>

<p>위 그림을 예로 들면, banking 이라는 단어를 표현할 때,
단어의 왼쪽과 오른쪽으로부터 banking이라는 단어의 정보를 얻는다. (dept, crises…)
이러한 방법을 통해 문맥으로부터 뭔가 더 얻을 수 있지 않을까? 라는 생각에서 출발한 모델이다.</p>

<p>​</p>

<h3 id="how-to-make-neighbors-represent-words">How to make neighbors represent words?</h3>

<p>그렇다면 어떻게 이웃된 정보를 표현할 수 있을까? <code class="highlighter-rouge">cooccurrence matrix X</code>를 통해 표현한다.
여기에는 2가지 옵션이 있는데 full document 와 window 이다.</p>

<p>​</p>

<ol>
  <li>
    <p>Full document matrix</p>

    <p>전체 문서의 matrix X를 통해 일반적인 토픽을 얻는다.
예를 들면, <code class="highlighter-rouge">swimming / wet / boat / ship</code> 은 문맥에 의해 어떤 하나의 공통된 토픽을 갖게 될 것이다.
대표적으로 <code class="highlighter-rouge">Latent Semantic Analysis</code> 와 같은 모델이 있다. (이 강의에서는 크게 다루지 않음)</p>

    <p>​</p>
  </li>
  <li>
    <p>Window based matrix</p>

    <p>이 방법은 Window의 길이 (일반적으로 5 - 10) 에 따라 대칭적으로 이동하면서 확인하는 방법이다.</p>

    <ul>
      <li>I like deep learning.</li>
      <li>Iike NLP.</li>
      <li>I enjoy flying</li>
    </ul>

    <p>위와 같은 corpus가 있을 때, 이를 matrix로 표현하면 다음과 같다.
간단히 보면 각 단어의 빈도 수를 체크한 것이다.</p>

    <p><img src="assets/images/matrix.png" alt="matrix" /></p>

    <p>​</p>
  </li>
</ol>

<h3 id="problems-with-simple-cooccurrence-vectors">Problems with simple cooccurrence vectors</h3>

<p>이 방법의 문제점은 단어의 크기가 커지면 matrix의 차원이 엄청나게 커진다는 점이다.
여기에서 sparsity matrix란, 행렬의 요소가 대부분이 0인 행렬을 말한다.
이 문제를 해결하는 방법은 Low dimensional vector를 사용하는 것이다.</p>

<p>​</p>

<h3 id="method-1-dimensionality-reduction-on-x">Method 1: Dimensionality Reduction on X</h3>

<p>그렇다면 차원은 어떻게 낮출 수 있을까? 이에 대한 방법을 소개한다.</p>

<p>​</p>

<h4 id="singular-value-decomposition-svd">Singular Value Decomposition (SVD)</h4>

<p><img src="assets/images/svd.png" alt="svd" /></p>

<p>​	SVD는 원래의 matrix X를 3개의 matrix로 분해한다.
​	여기에서 n보다 훨씬 작은 값 k를 설정해서 top - k만 가져가고,
​	그 이상의 차원은 날려버리는 방법으로 차원 축소를 해버린다.
​	보통 이러한 decomposition 방식을 행렬 내의 dependancy를 풀어주기 위한 방법이라고 이해하면 쉽다.
​	dense matrix란, 앞에서 언급했던 sparse matrix와 반대되는 개념이다. (가득찬, 꽉찬)</p>

<p>​</p>

<p>몇 가지 질문을 통해 나온 내용을 정리하자면, SVD의 dimension은 실험을 통해 최적의 값을 찾아야 한다. 어떤 데이터셋이냐에 따라 다르다.
일반적으로 25에서 1000 사이의 차원으로 축소한다.</p>

<p>​</p>

<h3 id="hacks-to-x">Hacks to X</h3>

<p>아직 몇 가지 문제가 남아있다.
function words (the, he, has)가 너무 빈번하게 나타나는데,
실제로는 이러한 단어가 큰 의미를 갖지 않는다.
하지만, count를 세게 되면 위와 같은 단어가 큰 영향을 미친다.</p>

<p>​</p>

<p>이를 해결하기 위한 방법은 다음과 같다.</p>

<ul>
  <li>corpus 자체에서 저런 단어를 모두 지운다.</li>
  <li>min(X, t), with t~100 : min을 씌워 빈번하게 발생하는 단어의 영향력을 낮춘다. (100번이 넘으면 무조건 100으로 고정)</li>
  <li>Ramped windows : window가 맞물려 있을 때 가까운 단어에 더 가중치를 준다.</li>
  <li>
    <p>Pearson correlations</p>

    <p>​</p>
  </li>
</ul>

<h3 id="problems-with-svd">Problems with SVD</h3>

<p>SVD는 다음과 같은 문제가 있다.
우선 O(<script type="math/tex">mn^2</script>) 의 복잡도를 갖기 때문에 Computational cost가 심하다.
따라서, 수 백만의 단어나 문서를 사용하는 경우 좋지 않다.
그리고 새로운 단어를 새로 적용하기가 어렵다.
매번 새로 돌려야 하며, 다른 딥러닝 모델과 학습체계가 다르다.</p>

<p>​</p>

<h2 id="word2vec">Word2Vec</h2>

<p>Word2Vec은 처음부터 바로 낮은 차원의 word vectors를 학습시키자! 라는 아이디어에서 출발한다.
이전에는 cooccurrence를 직접 count 했다면,
Word2Vec은 모든 단어에 대해 주변의 단어를 예측한다.
더 빠르고 쉽게 새로운 문장과 어울리며, 새로운 단어를 추가할 수도 있다.
Word2Vec과 GloVe는 상당히 유사한데, Word2Vec은 구글에서 GloVe는 스탠포드에서 만들었다.
특히 강의자가 논문에 참여했기 때문에 강의 중간에 자랑이 많이 나온다.</p>

<p>초기논문 참고 : “Pennington 2014, Glove : Global Vectors for Word Representation”</p>

<p>​</p>

<p>window 크기만큼 양옆으로 확장해가며 sliding 방식으로 이동해서 전체 단어로 확장한다.
Objective Function : 중심단어를 기준으로 log probability가 최대가 되도록 한다.
wt가 나왔을 때, wt 주변의 단어가 나올 확률이 얼마나 되는지가 J(0)이다.</p>

<p>​</p>

<p><img src="assets/images/word2vec cost.png" alt="word2vec cost" /></p>

<ul>
  <li>window length : n</li>
  <li>T token의 아주 큰 corpus</li>
  <li>m만큼 좌우를 확인</li>
  <li>
    <p>하나의 중심 단어로 주변의 단어를 찾는 probability를 최대화하는 것이 목표</p>

    <p>​</p>
  </li>
</ul>

<p><img src="assets/images/word2vec cost2.png" alt="word2vec cost" /></p>

<ul>
  <li>o : outside word id</li>
  <li>c : center word id</li>
  <li>u : outside vectors of o</li>
  <li>
    <p>v : center vectors of c</p>

    <p>​</p>
  </li>
</ul>

<p>내적한 값은 단어 간의 유사도를 말하며,
분자가 최대한 높고 분모가 최대한 작은 것이 좋다.
항상 두개의 벡터가 나오는데 하나의 벡터는 outside word를 표현하고,
다른 하나의 벡터는 outside words를 예측하는데 사용한다.</p>

<p>​</p>

<p>50분부터 Chain Rule에 대한 설명 참조,
59분부터 log probability를 적용한 설명 참조</p>

<p>​</p>

<h3 id="negative-sampling">Negative Sampling</h3>

<p>추가로 중간에 빼먹은 내용 (Slide 29) 중에 잠깐 Negative Sampling에 대한 내용이 나온다.
간단히 짚고 넘어가자면, 전체 데이터에서 샘플링 했을 떄 어차피 대부분은 연관이 없고(negative), 확실히 연관이 있는 것(positive)은 적다.
그래서 negative는 버리고 positive만 골라서 해도 비슷한 결과가 나온다.
실제로 속도는 빠르고 성능도 괜찮다고 한다.</p>

<p>​</p>

<h3 id="linear-relationships-in-word2vec">Linear Relationships in word2vec</h3>

<p>이러한 표현 방식은 유사도를 계산하는데 아주 좋은 결과를 보인다.
단순히 vector subtraction으로도 연산 가능하다.</p>

<p>한국어 word2vec 데모 : <a href="http://w.elnn.kr/search/">http://w.elnn.kr/search/</a></p>

<p>​</p>

<h3 id="count-based-vs-direct-prediction">Count based vs Direct prediction</h3>

<ol>
  <li>Count based</li>
</ol>

<ul>
  <li>LSA, HAL, COALS, PCA</li>
  <li>Training 속도가 빠름</li>
  <li>통계기법을 효율적으로 사용</li>
  <li>(단점) 단어의 유사도를 계산하는데만 사용 (Relationship 계산 불가)</li>
  <li>
    <p>(단점) count가 큰 경우에 불균등한 중요성이 주어짐 (?)</p>

    <p>​</p>
  </li>
</ul>

<ol>
  <li>Direct prediction</li>
</ol>

<ul>
  <li>NNLM, HLBL, RNN, Skip-gram/CBOW</li>
  <li>(단점) 말뭉치의 크기에 따라 Scale 가능</li>
  <li>(단점) 통계기법을 활용하지 않음</li>
  <li>대부분의 영역에서 좋은 성능을 내는 모델 생성</li>
  <li>
    <p>단어 유사도에 대해 복잡한 패턴도 잡아냄</p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>[CS224d] Lecture1. Intro to NLP and Deep Learning</title>
	  <link>//cs224d-lecture1</link>
	  <author>Swalloow</author>
	  <pubDate>2017-01-14T19:18:00+09:00</pubDate>
	  <guid>//cs224d-lecture1</guid>
	  <description><![CDATA[
	     <p>​</p>

<h3 id="pre-requisites">Pre-requisites</h3>

<ul>
  <li>파이썬을 능숙하게 사용할 줄 알아야 한다.</li>
  <li>미적분학, 선형대수학 지식</li>
  <li>기본적인 확률과 통계 이론</li>
  <li>머신러닝에 대한 지식</li>
  <li>Cost functions</li>
  <li>Simple derivatives</li>
  <li>
    <p>Optimization with Gradient descent</p>

    <p>​</p>
  </li>
</ul>

<h3 id="what-is-natural-language-processing-nlp">What is Natural Language Processing (NLP)?</h3>

<ul>
  <li>자연어처리는 다음과 같은 영역에서 서로 상호작용 한다.
    <ul>
      <li>Computer Science</li>
      <li>Artifical Intelligence</li>
      <li>and linguistics</li>
    </ul>

    <p>​</p>
  </li>
  <li>자연어처리의 목표는 컴퓨터에게 언어를 이해시켜 유용한 일을 수행하도록 하는 것이다.
    <ul>
      <li>Question Answering</li>
    </ul>

    <p>​</p>
  </li>
</ul>

<h3 id="nlp-levels">NLP Levels</h3>

<p><img src="assets/images/nlp level.png" alt="nlp level" /></p>

<p>​
### NLP in Industry</p>

<ul>
  <li>검색 (Written and Spoken)</li>
  <li>온라인 광고</li>
  <li>기계 번역</li>
  <li>마케팅이나 금융영역에서의 감성 분석</li>
  <li>음성 인식</li>
  <li>
    <p>고객 지원 자동화 (챗봇)</p>

    <p>​</p>
  </li>
</ul>

<p>최근에는 NLP의 문제들을 해결하기 위해 딥러닝 방법을 사용한다. 크게 개선된 분야는 다음과 같다.</p>

<ul>
  <li>levels : speech, morphology, syntax, semantics</li>
  <li>
    <p>applications : machine translation, sentiment analysis and question answering</p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>[CS224d] 자연어처리 강의 소개</title>
	  <link>//cs224d-intro</link>
	  <author>Swalloow</author>
	  <pubDate>2017-01-07T19:18:00+09:00</pubDate>
	  <guid>//cs224d-intro</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>CS224d 강의는 스탠포드 대학의 대표적인 자연어처리 강의입니다.</p>

<p>이미지 프로세싱은 CS231n, 자연어처리는 CS224d 로 배우라는 말이 있을 정도로 인기 강좌이며,</p>

<p>본 강의는 2016년 3월 강의로 현재 시점 기준 가장 최신 강의입니다.</p>

<p>앞으로 진행될 강의에서 새로운 내용이 추가되면 최대한 반영하도록 하겠습니다.</p>

<p>​</p>

<p>강의를 진행하는 Richard Socher는 세일즈포스에 인수된 MetaMind의 창업자로써,</p>

<p>Andrew Ng 교수님 아래에서 박사학위를 받았습니다.</p>

<p>​</p>

<h2 id="nlp----">NLP에서 자주 사용하는 영단어 정리</h2>

<p>대부분 영강의에 대한 부담이 많기 때문에,</p>

<p>강의에서 자주 나오는 영단어를 먼저 정리하고 가려합니다.</p>

<p>​</p>

<ul>
  <li>representation : 표현</li>
  <li>discrete : 추상적인, 분리된</li>
  <li>phrase : 구</li>
  <li>synonyms : 동의어</li>
  <li>subjective : 주어</li>
  <li>symmetric matrix : 대칭행렬</li>
  <li>corpus : 말뭉치</li>
  <li>robust : 힘든</li>
  <li>sparse : 드문드문 난</li>
  <li>
    <p>orthonomal matrix : 직교행렬</p>

    <p>​</p>
  </li>
</ul>

<h2 id="section">강의 참고자료</h2>

<p>강의자료와 유튜브 동영상 강의, 기타 참고할 만한 자료들의 링크를 정리합니다.</p>

<p>​</p>

<h4 id="section-1">링크 참조</h4>

<ul>
  <li>Syllabus : <a href="http://cs224d.stanford.edu/syllabus.html">http://cs224d.stanford.edu/syllabus.html</a></li>
  <li>
    <p>Youtube : <a href="https://www.youtube.com/playlist?list=PLlJy-eBtNFt4CSVWYqscHDdP58M3zFHIG">https://www.youtube.com/playlist?list=PLlJy-eBtNFt4CSVWYqscHDdP58M3zFHIG</a></p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>


</channel>
</rss>
