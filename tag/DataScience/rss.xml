<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>swalloow.github.io/</title>
   
   <link>http://swalloow.github.io/</link>
   <description>About Data Science, Data Engineering</description>
   <language>ko-KO</language>
   <managingEditor> Swalloow</managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>파이썬 머신러닝을 시작하기 위한 기초 지식 (1)</title>
	  <link>//pyml-intro1</link>
	  <author>Swalloow</author>
	  <pubDate>2017-02-05T19:18:00+09:00</pubDate>
	  <guid>//pyml-intro1</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>이 글의 목차나 그림은 <strong>Sebastian Rashka - Python Machine Learning</strong> 을 참고하였습니다.</p>

<p>사실 기계학습, 인공지능에 대한 연구는 예전부터 존재했지만 발전이 없었으며 소수에 연구원들에 의한 주제였기에 대중화 될 수 없었습니다. 하지만 풍부한 데이터의 확보, 컴퓨팅 성능향상, 오픈소스 라이브러리로 인해 많은 개발자들이 인공지능 연구에 참여하게 되었습니다.</p>

<p>이 글에서는 기계학습에 대한 간략한 소개와 데이터 분석 시스템을 어떻게 디자인해야 되는지, 마지막으로 파이썬을 이용한 데이터 분석에 대해 소개해드리겠습니다.</p>

<p>​</p>

<p><img src="/assets/images/ml-diagram.png" alt="ml-diagram" /></p>

<p>요즘에는 머신러닝을 크게 3가지 분야로 나누어 볼 수 있습니다. 흔히 알고있는 <strong>지도학습(Supervised Learning)</strong> 과 <strong>비지도학습(Unsupervised Learning)</strong> 이 있으며, 마지막으로 알파고에 적용되었던 <strong>강화학습(Reinforcement Learning)</strong> 이 있습니다. 이제 세 가지 다른 알고리즘 간의 근본적인 차이점에 대해 알아보고, 실제로 어떤 문제에 적용되는지 알아보겠습니다.</p>

<p>​</p>

<h2 id="supervised-learning">Supervised Learning</h2>

<p>지도학습과 비지도학습의 궁극적인 목표는 모두 과거, 현재의 데이터로부터 미래를 예측하는 것이라 할 수 있습니다. 다만, 두 가지 방법의 차이점은 라벨링 된 데이터인지 아닌지에 따라 결정됩니다.</p>

<p>만일 내가 가지고 있는 데이터가 라벨링 되어 있다면 지도학습이라고 볼 수 있습니다. 여기에서 라벨링 된 데이터(Labeled data)란 데이터에 대한 답이 주어져 있는 것 (평가가 되어 있는 것) 을 말합니다.</p>

<p><img src="/assets/images/learning-diagram.png" alt="learning-diagram" /></p>

<p>​</p>

<h4 id="classification">Classification</h4>

<p>지도학습은 Classification과 Regression으로 나누어집니다. 먼저, Classification은 주어진 데이터를 정해진 카테고리에 따라 분류하는 문제를 말합니다. 최근에 많이 사용되는 이미지 분류도 Classification 문제 중에 하나입니다.</p>

<p>예를 들어, 이메일이 스팸메일인지 아닌지를 예측한다고 하면 이메일은 스팸메일 / 정상적인 메일로 라벨링 될 수 있을 것입니다. 비슷한 예시로 암을 예측한다고 가정했을 때 이 종양이 악성종양인지 / 아닌지로 구분할 수 있습니다. 이처럼 맞다 / 아니다로 구분되는 문제를 <strong>Binary Classification</strong> 이라고 부릅니다.</p>

<p>분류 문제가 모두 맞다 / 아니다로 구분되지는 않습니다. 예를 들어, 공부시간에 따른 전공 Pass / Fail 을 예측한다고 하면 이는 Binary Classifiaction 으로 볼 수 있습니다. 반면에, 수능 공부시간에 따른 전공 학점을 A / B / C / D / F 으로 예측하는 경우도 있습니다. 이러한 분류를 <strong>Multi-label Classification</strong> 이라고 합니다.</p>

<p>​</p>

<h4 id="regression">Regression</h4>

<p>다음으로 Regression은 연속된 값을 예측하는 문제를 말합니다. 주로 어떤 패턴이나 트렌드, 경향을 예측할 때 사용됩니다. Coursera에서는 Regression을 설명할 때 항상 집의 크기에 따른 매매가격을 예로 듭니다. 아까와 유사한 예를 들자면, 공부시간에 따른 전공 시험 점수를 예측하는 문제를 예로 들 수 있습니다.</p>

<p>​</p>

<h2 id="unsupervised-learning">Unsupervised Learning</h2>

<p>비지도학습은 앞에서 언급한 것 처럼 라벨링이 되어 있지 않은 데이터로부터 미래를 예측하는 학습방법입니다. 평가되어 있지 않은 데이터로부터 숨어있는 패턴이나 형태를 찾아야 하기 때문에 당연히 더 어렵습니다. 비지도학습도 데이터가 분리되어 있는지 (Categorial data) 연속적인지 (Continuous data)로 나누어 생각할 수 있습니다.</p>

<p>대표적으로 클러스터링 (Clustering) 이 있습니다. 실제로는 그 데이터의 label이나 category가 무엇인지 알 수 없는 경우가 많기 때문에 이러한 방법이 중요하다고 볼 수 있습니다. 이외에도 차원축소(Dimentionality Reduction), Hidden Markov Model 등이 있습니다.</p>

<p>​</p>

<h2 id="reinforcement-learning">Reinforcement Learning</h2>

<p><img src="/assets/images/agent-environment.png" alt="ml-diagram" /></p>

<p>마지막으로 강화학습은 앞서 말했던 학습방법과는 조금 다른 개념입니다. 데이터가 정답이 있는 것도 아니며, 심지어 주어진 데이터가 없을 수도 있습니다. 강화학습이란, 자신이 한 행동에 대한 “보상”을 알 수 있어서 그로부터 학습하는 것을 말합니다.</p>

<p>예를 들면, 아이가 걷는 것을 배우는 것처럼 어떻게 행동할 줄 모르지만 환경과 상호작용하면서 걷는 법을 알아가는 것과 같은 학습 방법을 강화학습이라고 합니다.</p>

<p><img src="https://dnddnjs.gitbooks.io/rl/content/90-6.png" alt="atari" /></p>

<p>보통 아타리 게임 인공지능을 많이 예시로 드는데, 여기에서 학습 대상 (agent) 은 움직이면서 적을 물리치는 존재입니다. 이 학습 대상은 움직이면서 적을 물리치면 보상 (reward) 을 받게 됩니다. 이러한 과정을 스스로 반복 학습 (Trial and Error) 하면서 점수를 최대화하는 것이 목표입니다.</p>

<p>​</p>

<h2 id="how">How?</h2>

<p>처음에는 공부를 시작하기에 막막한데 다행히 아주 좋은 강의와 자료들이 많이 있습니다.</p>

<h4 id="machine-learning--deep-learning">Machine Learning / Deep Learning</h4>

<ul>
  <li>Coursera - Andrew Ng : <a href="https://www.coursera.org/learn/machine-learning">https://www.coursera.org/learn/machine-learning</a></li>
  <li>모두의 딥러닝 : <a href="https://hunkim.github.io/ml/">https://hunkim.github.io/ml/</a></li>
  <li>
    <p>CS231n : <a href="http://cs231n.stanford.edu/">http://cs231n.stanford.edu/</a></p>

    <p>​</p>
  </li>
</ul>

<h4 id="reinforcement-learning-1">Reinforcement Learning</h4>

<ul>
  <li>모두의 연구소 깃북 : <a href="https://dnddnjs.gitbooks.io/rl/content/">https://dnddnjs.gitbooks.io/rl/content/</a></li>
  <li>
    <p>Deep Mind - David Silver : <a href="https://www.youtube.com/watch?v=2pWv7GOvuf0">https://www.youtube.com/watch?v=2pWv7GOvuf0</a></p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>DecisionTree와 RandomForest에 대하여</title>
	  <link>//decison-randomforest</link>
	  <author>Swalloow</author>
	  <pubDate>2017-02-05T19:18:00+09:00</pubDate>
	  <guid>//decison-randomforest</guid>
	  <description><![CDATA[
	     <p>​</p>

<h3 id="decisiontree">의사결정트리 (DecisionTree)</h3>

<p>의사결정나무는 다양한 의사결정 경로와 결과를 나타내는데 트리 구조를 사용합니다. 보통 어렸을 때의 스무고개 놀이를 예로 드는 경우가 많습니다.</p>

<p><img src="http://cfile3.uf.tistory.com/image/2720193757A374DF33F327" alt="img" /></p>

<p>​</p>

<p>위의 그림은 타이타닉 생존자를 찾는 의사결정트리 모델입니다. 첫번째 뿌리 노드를 보면 성별 &lt;= 0.5 라고 되어있는데 이는 남자냐? 여자냐? 라고 질문하는 것과 같습니다.</p>

<p>최종적으로, 모든 승객에 대한 분류(Classification)를 통해 생존확률을 예측할 수 있게 됩니다.</p>

<p>이처럼, 숫자형 결과를 반환하는 것을 <strong>회귀나무(Regression Tree)</strong> 라고 하며, 범주형 결과를 반환하는 것을 <strong>분류나무(Classification Tree)</strong> 라고 합니다. 의사결정트리를 만들기 위해서는 먼저 어떤 질문을 할 것인지, 어떤 순서로 질문을 할 것인지 정해야 합니다.</p>

<p>가장 좋은 방법은 예측하려는 대상에 대해 가장 많은 정보를 담고 있는 질문을 고르는 것이 좋습니다. 이러한 ‘얼마만큼의 정보를 담고 있는가’를 엔트로피(entropy) 라고 합니다. 엔트로피는 보통 데이터의 불확실성(?)을 나타내며, 결국 엔트로피가 클 수록 데이터 정보가 잘 분포되어 있기 때문에 좋은 지표라고 예상할 수 있습니다.</p>

<p>그림과 같이 의사결정트리는 이해하고 해석하기 쉽다는 장점이 있습니다. 또한 예측할 때 사용하는 프로세스가 명백하며, 숫자형 / 범주형 데이터를 동시에 다룰 수 있습니다. 그리고 특정 변수의 값이 누락되어도 사용할 수 있습니다.</p>

<p>하지만 최적의 의사결정트리를 찾는 것이 정말 어려운 문제입니다. (어떤 것들을 조건(Feature)으로 넣어야 할지, 깊이(Depth)는 얼마로 정해야 할지…) 그리고 의사결정트리의 단점은 새로운 데이터에 대한 일반화 성능이 좋지 않게 오버피팅(Overfitting)되기 쉽다는 것입니다.</p>

<p>잠깐 오버피팅에 대해 설명하자면, 오버피팅이란 Supervised Learning에서 과거의 학습한 데이터에 대해서는 잘 예측하지만 새로 들어온 데이터에 대해서 성능이 떨어지는 경우를 말합니다. 즉, 학습 데이터에 지나치게 최적화되어 일반화가 어렵다는 말입니다. 이러한 오버피팅을 방지할 수 있는 대표적인 방법 중 하나가 바로 앙상블 기법을 적용한 <strong>랜덤포레스트(Random Forest)</strong> 입니다.</p>

<p>​</p>

<h3 id="randomforest">랜덤포레스트 (RandomForest)</h3>

<p>랜덤포레스트는 위에서 말한 것과 같이 의사결정트리를 이용해 만들어진 알고리즘입니다.</p>

<blockquote>
  <p>랜덤포레스트는 분류, 회귀 분석 등에 사용되는 앙상블 학습 방법의 일종으로, </p>

  <p>훈련 과정에서 구성한 다수의 결정 트리로부터 분류 또는 평균 예측치를 출력함으로써 동작한다.</p>
</blockquote>

<p>​</p>

<p>즉, 랜덤포레스트란 여러 개의 의사결정트리를 만들고, 투표를 시켜 다수결로 결과를 결정하는 방법입니다.</p>

<p>​</p>

<p><img src="http://cfile24.uf.tistory.com/image/2325343B57A37515289AD9" alt="img" /></p>

<p>위의 그림은 고작 몇 십개의 트리노드가 있지만 실제로는 수 백개에서 수 만개까지 노드가 생성될 수 있습니다. 이렇게 여러 개의 트리를 통해 투표를 해서 오버피팅이 생길 경우에 대비할 수 있습니다.</p>

<p>그런데 보통 구축한 트리에는 랜덤성이 없는데 어떻게하면 랜덤하게 트리를 얻을 수 있나? 라는 의문이 듭니다. 랜덤포레스트에서는 데이터를 bootstrap 해서 포레스트를 구성합니다.</p>

<p>bootstrap aggregating 또는 begging 이라고 하는데, 전체 데이터를 전부 이용해서 학습시키는 것이 아니라 샘플의 결과물을 각 트리의 입력 값으로 넣어 학습하는 방식입니다. 이렇게 하면 각 트리가 서로 다른 데이터로 구축되기 때문에 랜덤성이 생기게 됩니다. 그리고 파티션을 나눌 때 변수에 랜덤성을 부여합니다. 즉, 남아있는 모든 변수 중에서 최적의 변수를 선택하는 것이 아니라 변수 중 일부만 선택하고 그 일부 중에서 최적의 변수를 선택하는 것입니다.</p>

<p>이러한 방식을 <strong>앙상블 기법(ensemble learning)</strong> 이라고 합니다. 랜덤포레스트는 아주 인기가 많고 자주 사용되는 알고리즘 중 하나입니다. 샘플링되지 않은 데이터를 테스트 데이터로 이용할 수 있기 때문에 데이터 전체를 학습에 사용할 수 있으며, 의사결정트리에 비해 일반화도 적용될 수 있습니다.</p>

<p>하지만 실제로 테스트 해보면 꼭 모든 경우에 뛰어나다고 할 수는 없습니다. (예를 들면 데이터 셋이 비교적 적은 경우)</p>

<p>​</p>

<h2 id="section">실제로 사용해보자</h2>

<p>이렇게 이론을 공부하고 나면 실제로 적용해보는 예측모델을 만들고 싶어집니다.
정말 고맙게도 파이썬의 scikit-learn에 다양한 트리 모델이 구현되어 있습니다.
다른 앙상블 모델 뿐만 아니라 RandomForest까지 제공합니다.</p>

<p>공식 레퍼런스는 아래의 링크를 참조</p>

<p><a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</a></p>

<p><img src="/assets/images/decisiontree_load.png" alt="decisiontree_load" /></p>

<p>방법은 간단합니다. sklearn.tree에 있는 model 을 import 합니다.
skikit-learn의 모델들은 대부분 파라메터로 X, y 값을 넣는다는 공통점이 있습니다.
여기서 X는 input 또는 feature가 되고, y는 output이 됩니다.</p>

<p><img src="/assets/images/decisiontree_fit.png" alt="decisiontree_fit" /></p>

<p>model.fit(X, y)를 하면 모델에 대한 정보가 출력됩니다. 최초에 모델을 로드할 때 random_state 값만 조정했기 때문에 전부 다 default 값이 적용된 걸 볼 수 있습니다.</p>

<p>RandomForest나 DecisionTree 같은 경우에는 max_depth, n_estimator에 따라 결과 값이 달라집니다. 데이터에 따라 다르지만, 보통 100-150 사이의 값 중에 성능이 가장 잘 나오는 값으로 결정합니다.</p>

<p>​</p>

	  ]]></description>
	</item>

	<item>
	  <title>[CS224d] Lecture2. Word Vectors</title>
	  <link>//cs224d-lecture2</link>
	  <author>Swalloow</author>
	  <pubDate>2017-01-21T19:18:00+09:00</pubDate>
	  <guid>//cs224d-lecture2</guid>
	  <description><![CDATA[
	     <p>​</p>

<h3 id="how-do-we-represent-the-meaning-of-a-word">How do we represent the meaning of a word?</h3>

<p>“단어의 의미를 어떻게 표현해야 할까?”에 대한 고민은 예전부터 지속되어 왔다. 결국 사람이 단어나 몸짓으로 표현하는 것은 쉽지만 컴퓨터가 이를 이해하기는 어렵다.
마찬가지로 글쓰기, 미술 등으로 표현하는 것 또한 마찬가지이다.</p>

<p>과거에는 WordNet과 같은 방법을 사용했다. WordNet이란, 각 단어끼리의 관계(상위단어, 동의어) 가 나타나 있는 트리구조의 그래프 모형이다.
물론 이를 구축하기 위한 작업은 전부 사람이 했다. 그러다보니 주관적이고 유지하는데 있어 많은 노동이 필요하다는 한계가 존재했다.</p>

<p>​</p>

<h3 id="problems-with-this-discrete-representation">Problems with this discrete representation</h3>

<p>기존의 discrete representation에는 다음과 같은 문제점들이 존재한다.
우선, 아까와 같은 경우 리소스로는 충분하지만 뉘앙스가 부족하다. 비슷한 예로 동의어 문제가 있다.
<code class="highlighter-rouge">he's good / he's very proficient</code> 와 같은 문장을 비교해보면,
proficient는 보통 말을 능숙하게 하는 사람을 표현하듯이 문맥에 따라 의미가 달라질 것이다.</p>

<p>​</p>

<p>두번째 이유는 매번 신조어가 나타나는데 이를 최신화하기 어렵다.
wicked, badass, ninjia… (미국에서 쓰는 신조어인듯 싶다.)</p>

<p>​</p>

<p>세번째 이유는 주관적이고 유지하는데 사람의 노동이 필요하다.
사람들이 직접 구축한 것이기 때문에 주관적이다.
특히 WordNet은 영어 이외의 언어에서 잘 구축된 경우가 별로 없다.
(국내의 몇몇 대학원에서 구축된 것이 있지만 절대 공개하지 않는다…)</p>

<p>​</p>

<p>네번째, 단어 간의 유사도를 계산하기 어렵다.
어떤 단어가 어느정도 동의어인지 아닌지 계산하기도 어렵다는 말이다.</p>

<p>​</p>

<p>그래서 보통 Rule-based 와 통계적 NLP를 사용하는 모델들은 단어를 <code class="highlighter-rouge">atomic symbol</code>로써 사용한다.
예를 들어, 2만 개의 단어 중에 <code class="highlighter-rouge">hotel</code> 이라는 단어를 vector space로 나타낸다면,
<code class="highlighter-rouge">[0, 0, 0, 0, 0, 0, ..., 0, 0, 1, 0, 0, 0]</code> 이런 식이다. (2만 개의 차원)
이러한 경우, <code class="highlighter-rouge">motel</code> 이라는 단어와 AND operation을 통해 유사도를 계산한다면
무조건 0이 나오게 될 것이다. (별로 좋지 않다)</p>

<p>​</p>

<p>이러한 벡터를 <code class="highlighter-rouge">One-Hot Vector</code> 라고 부르며,
이러한 NLP 방법론을 <code class="highlighter-rouge">Bag of Words Representation</code> 이라고 부른다.</p>

<p>​</p>

<h3 id="distributional-similarity-based-representations">Distributional similarity based representations</h3>

<p>이 방법은 현대 통계적 NLP 방법론 중에서 가장 좋게 평가받는 모델 중 하나이다.</p>

<p><img src="assets/images/distributional.png" alt="distributional" /></p>

<p>위 그림을 예로 들면, banking 이라는 단어를 표현할 때,
단어의 왼쪽과 오른쪽으로부터 banking이라는 단어의 정보를 얻는다. (dept, crises…)
이러한 방법을 통해 문맥으로부터 뭔가 더 얻을 수 있지 않을까? 라는 생각에서 출발한 모델이다.</p>

<p>​</p>

<h3 id="how-to-make-neighbors-represent-words">How to make neighbors represent words?</h3>

<p>그렇다면 어떻게 이웃된 정보를 표현할 수 있을까? <code class="highlighter-rouge">cooccurrence matrix X</code>를 통해 표현한다.
여기에는 2가지 옵션이 있는데 full document 와 window 이다.</p>

<p>​</p>

<ol>
  <li>
    <p>Full document matrix</p>

    <p>전체 문서의 matrix X를 통해 일반적인 토픽을 얻는다.
예를 들면, <code class="highlighter-rouge">swimming / wet / boat / ship</code> 은 문맥에 의해 어떤 하나의 공통된 토픽을 갖게 될 것이다.
대표적으로 <code class="highlighter-rouge">Latent Semantic Analysis</code> 와 같은 모델이 있다. (이 강의에서는 크게 다루지 않음)</p>

    <p>​</p>
  </li>
  <li>
    <p>Window based matrix</p>

    <p>이 방법은 Window의 길이 (일반적으로 5 - 10) 에 따라 대칭적으로 이동하면서 확인하는 방법이다.</p>

    <ul>
      <li>I like deep learning.</li>
      <li>Iike NLP.</li>
      <li>I enjoy flying</li>
    </ul>

    <p>위와 같은 corpus가 있을 때, 이를 matrix로 표현하면 다음과 같다.
간단히 보면 각 단어의 빈도 수를 체크한 것이다.</p>

    <p><img src="assets/images/matrix.png" alt="matrix" /></p>

    <p>​</p>
  </li>
</ol>

<h3 id="problems-with-simple-cooccurrence-vectors">Problems with simple cooccurrence vectors</h3>

<p>이 방법의 문제점은 단어의 크기가 커지면 matrix의 차원이 엄청나게 커진다는 점이다.
여기에서 sparsity matrix란, 행렬의 요소가 대부분이 0인 행렬을 말한다.
이 문제를 해결하는 방법은 Low dimensional vector를 사용하는 것이다.</p>

<p>​</p>

<h3 id="method-1-dimensionality-reduction-on-x">Method 1: Dimensionality Reduction on X</h3>

<p>그렇다면 차원은 어떻게 낮출 수 있을까? 이에 대한 방법을 소개한다.</p>

<p>​</p>

<h4 id="singular-value-decomposition-svd">Singular Value Decomposition (SVD)</h4>

<p><img src="assets/images/svd.png" alt="svd" /></p>

<p>​	SVD는 원래의 matrix X를 3개의 matrix로 분해한다.
​	여기에서 n보다 훨씬 작은 값 k를 설정해서 top - k만 가져가고,
​	그 이상의 차원은 날려버리는 방법으로 차원 축소를 해버린다.
​	보통 이러한 decomposition 방식을 행렬 내의 dependancy를 풀어주기 위한 방법이라고 이해하면 쉽다.
​	dense matrix란, 앞에서 언급했던 sparse matrix와 반대되는 개념이다. (가득찬, 꽉찬)</p>

<p>​</p>

<p>몇 가지 질문을 통해 나온 내용을 정리하자면, SVD의 dimension은 실험을 통해 최적의 값을 찾아야 한다. 어떤 데이터셋이냐에 따라 다르다.
일반적으로 25에서 1000 사이의 차원으로 축소한다.</p>

<p>​</p>

<h3 id="hacks-to-x">Hacks to X</h3>

<p>아직 몇 가지 문제가 남아있다.
function words (the, he, has)가 너무 빈번하게 나타나는데,
실제로는 이러한 단어가 큰 의미를 갖지 않는다.
하지만, count를 세게 되면 위와 같은 단어가 큰 영향을 미친다.</p>

<p>​</p>

<p>이를 해결하기 위한 방법은 다음과 같다.</p>

<ul>
  <li>corpus 자체에서 저런 단어를 모두 지운다.</li>
  <li>min(X, t), with t~100 : min을 씌워 빈번하게 발생하는 단어의 영향력을 낮춘다. (100번이 넘으면 무조건 100으로 고정)</li>
  <li>Ramped windows : window가 맞물려 있을 때 가까운 단어에 더 가중치를 준다.</li>
  <li>
    <p>Pearson correlations</p>

    <p>​</p>
  </li>
</ul>

<h3 id="problems-with-svd">Problems with SVD</h3>

<p>SVD는 다음과 같은 문제가 있다.
우선 O(<script type="math/tex">mn^2</script>) 의 복잡도를 갖기 때문에 Computational cost가 심하다.
따라서, 수 백만의 단어나 문서를 사용하는 경우 좋지 않다.
그리고 새로운 단어를 새로 적용하기가 어렵다.
매번 새로 돌려야 하며, 다른 딥러닝 모델과 학습체계가 다르다.</p>

<p>​</p>

<h2 id="word2vec">Word2Vec</h2>

<p>Word2Vec은 처음부터 바로 낮은 차원의 word vectors를 학습시키자! 라는 아이디어에서 출발한다.
이전에는 cooccurrence를 직접 count 했다면,
Word2Vec은 모든 단어에 대해 주변의 단어를 예측한다.
더 빠르고 쉽게 새로운 문장과 어울리며, 새로운 단어를 추가할 수도 있다.
Word2Vec과 GloVe는 상당히 유사한데, Word2Vec은 구글에서 GloVe는 스탠포드에서 만들었다.
특히 강의자가 논문에 참여했기 때문에 강의 중간에 자랑이 많이 나온다.</p>

<p>초기논문 참고 : “Pennington 2014, Glove : Global Vectors for Word Representation”</p>

<p>​</p>

<p>window 크기만큼 양옆으로 확장해가며 sliding 방식으로 이동해서 전체 단어로 확장한다.
Objective Function : 중심단어를 기준으로 log probability가 최대가 되도록 한다.
wt가 나왔을 때, wt 주변의 단어가 나올 확률이 얼마나 되는지가 J(0)이다.</p>

<p>​</p>

<p><img src="assets/images/word2vec cost.png" alt="word2vec cost" /></p>

<ul>
  <li>window length : n</li>
  <li>T token의 아주 큰 corpus</li>
  <li>m만큼 좌우를 확인</li>
  <li>
    <p>하나의 중심 단어로 주변의 단어를 찾는 probability를 최대화하는 것이 목표</p>

    <p>​</p>
  </li>
</ul>

<p><img src="assets/images/word2vec cost2.png" alt="word2vec cost" /></p>

<ul>
  <li>o : outside word id</li>
  <li>c : center word id</li>
  <li>u : outside vectors of o</li>
  <li>
    <p>v : center vectors of c</p>

    <p>​</p>
  </li>
</ul>

<p>내적한 값은 단어 간의 유사도를 말하며,
분자가 최대한 높고 분모가 최대한 작은 것이 좋다.
항상 두개의 벡터가 나오는데 하나의 벡터는 outside word를 표현하고,
다른 하나의 벡터는 outside words를 예측하는데 사용한다.</p>

<p>​</p>

<p>50분부터 Chain Rule에 대한 설명 참조,
59분부터 log probability를 적용한 설명 참조</p>

<p>​</p>

<h3 id="negative-sampling">Negative Sampling</h3>

<p>추가로 중간에 빼먹은 내용 (Slide 29) 중에 잠깐 Negative Sampling에 대한 내용이 나온다.
간단히 짚고 넘어가자면, 전체 데이터에서 샘플링 했을 떄 어차피 대부분은 연관이 없고(negative), 확실히 연관이 있는 것(positive)은 적다.
그래서 negative는 버리고 positive만 골라서 해도 비슷한 결과가 나온다.
실제로 속도는 빠르고 성능도 괜찮다고 한다.</p>

<p>​</p>

<h3 id="linear-relationships-in-word2vec">Linear Relationships in word2vec</h3>

<p>이러한 표현 방식은 유사도를 계산하는데 아주 좋은 결과를 보인다.
단순히 vector subtraction으로도 연산 가능하다.</p>

<p>한국어 word2vec 데모 : <a href="http://w.elnn.kr/search/">http://w.elnn.kr/search/</a></p>

<p>​</p>

<h3 id="count-based-vs-direct-prediction">Count based vs Direct prediction</h3>

<ol>
  <li>Count based</li>
</ol>

<ul>
  <li>LSA, HAL, COALS, PCA</li>
  <li>Training 속도가 빠름</li>
  <li>통계기법을 효율적으로 사용</li>
  <li>(단점) 단어의 유사도를 계산하는데만 사용 (Relationship 계산 불가)</li>
  <li>
    <p>(단점) count가 큰 경우에 불균등한 중요성이 주어짐 (?)</p>

    <p>​</p>
  </li>
</ul>

<ol>
  <li>Direct prediction</li>
</ol>

<ul>
  <li>NNLM, HLBL, RNN, Skip-gram/CBOW</li>
  <li>(단점) 말뭉치의 크기에 따라 Scale 가능</li>
  <li>(단점) 통계기법을 활용하지 않음</li>
  <li>대부분의 영역에서 좋은 성능을 내는 모델 생성</li>
  <li>
    <p>단어 유사도에 대해 복잡한 패턴도 잡아냄</p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>[CS224d] Lecture1. Intro to NLP and Deep Learning</title>
	  <link>//cs224d-lecture1</link>
	  <author>Swalloow</author>
	  <pubDate>2017-01-14T19:18:00+09:00</pubDate>
	  <guid>//cs224d-lecture1</guid>
	  <description><![CDATA[
	     <p>​</p>

<h3 id="pre-requisites">Pre-requisites</h3>

<ul>
  <li>파이썬을 능숙하게 사용할 줄 알아야 한다.</li>
  <li>미적분학, 선형대수학 지식</li>
  <li>기본적인 확률과 통계 이론</li>
  <li>머신러닝에 대한 지식</li>
  <li>Cost functions</li>
  <li>Simple derivatives</li>
  <li>
    <p>Optimization with Gradient descent</p>

    <p>​</p>
  </li>
</ul>

<h3 id="what-is-natural-language-processing-nlp">What is Natural Language Processing (NLP)?</h3>

<ul>
  <li>자연어처리는 다음과 같은 영역에서 서로 상호작용 한다.
    <ul>
      <li>Computer Science</li>
      <li>Artifical Intelligence</li>
      <li>and linguistics</li>
    </ul>

    <p>​</p>
  </li>
  <li>자연어처리의 목표는 컴퓨터에게 언어를 이해시켜 유용한 일을 수행하도록 하는 것이다.
    <ul>
      <li>Question Answering</li>
    </ul>

    <p>​</p>
  </li>
</ul>

<h3 id="nlp-levels">NLP Levels</h3>

<p><img src="assets/images/nlp level.png" alt="nlp level" /></p>

<p>​
### NLP in Industry</p>

<ul>
  <li>검색 (Written and Spoken)</li>
  <li>온라인 광고</li>
  <li>기계 번역</li>
  <li>마케팅이나 금융영역에서의 감성 분석</li>
  <li>음성 인식</li>
  <li>
    <p>고객 지원 자동화 (챗봇)</p>

    <p>​</p>
  </li>
</ul>

<p>최근에는 NLP의 문제들을 해결하기 위해 딥러닝 방법을 사용한다. 크게 개선된 분야는 다음과 같다.</p>

<ul>
  <li>levels : speech, morphology, syntax, semantics</li>
  <li>
    <p>applications : machine translation, sentiment analysis and question answering</p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>[CS224d] 자연어처리 강의 소개</title>
	  <link>//cs224d-intro</link>
	  <author>Swalloow</author>
	  <pubDate>2017-01-07T19:18:00+09:00</pubDate>
	  <guid>//cs224d-intro</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>CS224d 강의는 스탠포드 대학의 대표적인 자연어처리 강의입니다.</p>

<p>이미지 프로세싱은 CS231n, 자연어처리는 CS224d 로 배우라는 말이 있을 정도로 인기 강좌이며,</p>

<p>본 강의는 2016년 3월 강의로 현재 시점 기준 가장 최신 강의입니다.</p>

<p>앞으로 진행될 강의에서 새로운 내용이 추가되면 최대한 반영하도록 하겠습니다.</p>

<p>​</p>

<p>강의를 진행하는 Richard Socher는 세일즈포스에 인수된 MetaMind의 창업자로써,</p>

<p>Andrew Ng 교수님 아래에서 박사학위를 받았습니다.</p>

<p>​</p>

<h2 id="nlp----">NLP에서 자주 사용하는 영단어 정리</h2>

<p>대부분 영강의에 대한 부담이 많기 때문에,</p>

<p>강의에서 자주 나오는 영단어를 먼저 정리하고 가려합니다.</p>

<p>​</p>

<ul>
  <li>representation : 표현</li>
  <li>discrete : 추상적인, 분리된</li>
  <li>phrase : 구</li>
  <li>synonyms : 동의어</li>
  <li>subjective : 주어</li>
  <li>symmetric matrix : 대칭행렬</li>
  <li>corpus : 말뭉치</li>
  <li>robust : 힘든</li>
  <li>sparse : 드문드문 난</li>
  <li>
    <p>orthonomal matrix : 직교행렬</p>

    <p>​</p>
  </li>
</ul>

<h2 id="section">강의 참고자료</h2>

<p>강의자료와 유튜브 동영상 강의, 기타 참고할 만한 자료들의 링크를 정리합니다.</p>

<p>​</p>

<h4 id="section-1">링크 참조</h4>

<ul>
  <li>Syllabus : <a href="http://cs224d.stanford.edu/syllabus.html">http://cs224d.stanford.edu/syllabus.html</a></li>
  <li>
    <p>Youtube : <a href="https://www.youtube.com/playlist?list=PLlJy-eBtNFt4CSVWYqscHDdP58M3zFHIG">https://www.youtube.com/playlist?list=PLlJy-eBtNFt4CSVWYqscHDdP58M3zFHIG</a></p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>


</channel>
</rss>
