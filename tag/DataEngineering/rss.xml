<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>swalloow.github.io/</title>
   
   <link>http://swalloow.github.io/</link>
   <description>About Data Science, Data Engineering</description>
   <language>ko-KO</language>
   <managingEditor> Swalloow</managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>Spark의 Shuffling 이해하기</title>
	  <link>//spark-shuffling</link>
	  <author>Swalloow</author>
	  <pubDate>2017-08-25T19:18:00+09:00</pubDate>
	  <guid>//spark-shuffling</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>효율적인 Spark Application을 개발하기 위해 <strong>Shuffling</strong> 은 상당히 중요한 개념입니다.
이에 대해 간단히 정리해보았습니다.</p>

<p>​</p>

<h2 id="spark-architecture-shuffle">Spark Architecture: Shuffle</h2>

<p><img src="/assets/images/shuffle.png" alt="" /></p>

<p>Shuffle을 설명하기 전에 한 가지 예시를 들어보겠습니다.
테이블에 전화 통화 기록 목록이 있고 매일 발생한 통화량을 계산한다고 가정 해보겠습니다.
“날짜”를 키로 설정하고 각 레코드에 대해 값으로 “1”을 지정한 다음, 각 키의 값을 합산하여 결과 값을 계산할 수 있을 것 입니다.</p>

<p>만일 데이터가 여러 클러스터에 저장되어 있다면 어떻게 해야 동일한 키의 값을 합산할 수 있을까요?
이를 위한 유일한 방법은 같은 키의 모든 값을 동일한 시스템에 두는 것입니다. 그런 다음 이 값들을 합치면 됩니다.</p>

<p>​</p>

<h3 id="narrow-and-wide-transformation">Narrow and Wide Transformation</h3>

<p><img src="/assets/images/narrow_and_wide.png" alt="" /></p>

<p>몇 가지 사례를 통해 더 자세히 알아보겠습니다.
만일 데이터가 이미 키 값으로 파티셔닝 되어 있고 키 값에 대해 변화를 주고 싶다면, 좌측의 그림처럼 수행하게 됩니다.
<code class="highlighter-rouge">filter(), sample(), map(), flatMap()</code> 등의 transformation이 이에 해당하며, 이 경우 Shuffle이 필요 없습니다.
이를 <strong>Narrow Transformation</strong> 이라고 합니다.</p>

<p>반면, 서로 다른 파티션으로부터 특정한 값을 기준으로 추출하고 싶은 경우, 그 값을 기준으로 Shuffle이 발생하게 됩니다.
<code class="highlighter-rouge">groupByKey(), reduceByKey()</code> 등이 이에 해당하며, 이를 <strong>Wide Transformation</strong> 이라고 합니다.</p>

<p>​</p>

<h3 id="shuffled-hashjoin">Shuffled HashJoin</h3>

<p><img src="/assets/images/shuffle_join.png" alt="" /></p>

<p>두 개의 테이블을 <code class="highlighter-rouge">Join</code> 할 때에도 Shuffle 이 발생할 수 있습니다.
위의 예시 처럼 두 테이블에서 키 값을 기준으로 Join 하게 되면, 동일한 키를 가진 데이터가 동일한 파티션으로 이동합니다.</p>

<p>하지만 이 때, 셔플 되는 데이터의 양이 성능에 영향을 미칠 수 있습니다.
만일 C의 데이터의 크기가 A보다 훨씬 크다면, C에 대한 작업으로 인해 전체의 수행시간이 오래 걸리게 될 것 입니다.</p>

<p>​</p>

<h3 id="broadcast-hashjoin">Broadcast HashJoin</h3>

<p><img src="/assets/images/broadcast_join.png" alt="" /></p>

<p>이를 개선하기 위해 Spark에서는 <strong>Broadcast Join</strong> 을 제공합니다.
이 경우 RDD 중 하나가 모든 파티션으로 브로드 캐스팅되며 복사됩니다.
만일 RDD 중 하나가 다른 것에 비해 상당히 작다면 큰 RDD가 전혀 셔플 할 필요가 없습니다.
작은 RDD 만 모든 작업자 서버에 복사해야 하므로 Broadcast Join은 전체적으로 네트워크 트래픽을 줄여주는 효과가 있습니다.</p>

<p>Spark 1.2에서는 <code class="highlighter-rouge">spark.sql.autoBroadcastJoinThreshold</code> 값을 설정해주어야 했지만,
2.0 이후 버전의 경우 Spark SQL이 알아서 최적화 잘 해줍니다.</p>

<p>​</p>

<h3 id="spark-shuffle-properties">Spark Shuffle Properties</h3>

<ul>
  <li><code class="highlighter-rouge">spark.shuffle.compress</code>: 엔진이 shuffle 출력을 압축할지 여부를 지정</li>
  <li><code class="highlighter-rouge">spark.shuffle.spill.compress</code>: 중간 shuffle spill 파일을 압축할지 여부를 지정</li>
</ul>

<p>Shuffle에는 위의 두 가지 중요한 Spark Property 가 있습니다.</p>

<p>둘 다 기본적으로 값이 “true”이며, <code class="highlighter-rouge">spark.io.compression.codec</code> 압축 코덱을 기본으로합니다.
그리고 위에서 설명한 것처럼 Spark에는 여러 가지 셔플 구현이 있습니다.
특정 구현에서 사용되는 Shuffle은 <code class="highlighter-rouge">spark.shuffle.manager</code> 값에 의해 결정됩니다.
가능한 옵션은 <strong>hash, sort, tungsten-sort</strong> 이며, “sort” 옵션은 기본적으로 Spark 1.2.0부터 시작합니다.</p>

<p>이외에도 Spark Shuffle 관련된 Property는 아래의 공식문서에서 확인하실 수 있습니다.
<a href="https://spark.apache.org/docs/latest/configuration.html#shuffle-behavior">https://spark.apache.org/docs/latest/configuration.html#shuffle-behavior</a></p>

<p>​</p>

<h3 id="reference">Reference</h3>

<ul>
  <li><a href="https://0x0fff.com/spark-architecture-shuffle">https://0x0fff.com/spark-architecture-shuffle</a></li>
  <li>
    <p><a href="https://www.slideshare.net/databricks/strata-sj-everyday-im-shuffling-tips-for-writing-better-spark-programs">https://www.slideshare.net/databricks/strata-sj-everyday-im-shuffling-tips-for-writing-better-spark-programs</a></p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>Spark groupByKey vs reduceByKey 성능 비교</title>
	  <link>//spark-reduceByKey-groupByKey</link>
	  <author>Swalloow</author>
	  <pubDate>2017-08-22T19:18:00+09:00</pubDate>
	  <guid>//spark-reduceByKey-groupByKey</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>Spark Application 성능 개선을 위해 <code class="highlighter-rouge">groupByKey, reduceBykey</code>에 대해 알아보겠습니다.</p>

<p>​</p>

<h2 id="groupbykey-vs-reducebykey">groupByKey vs reduceBykey</h2>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># reduceByKey</span>
<span class="n">spark</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s">"hdfs://..."</span><span class="p">)</span>
 <span class="o">.</span><span class="n">flatMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
 <span class="o">.</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">word</span><span class="p">:</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
 <span class="o">.</span><span class="n">reduceByKey</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>

<span class="c"># groupByKey</span>
<span class="n">spark</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s">"hdfs://..."</span><span class="p">)</span>
 <span class="o">.</span><span class="n">flatMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
 <span class="o">.</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">word</span><span class="p">:</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
 <span class="o">.</span><span class="n">groupByKey</span><span class="p">()</span>
 <span class="o">.</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">counts</span><span class="p">):</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">counts</span><span class="p">)))</span></code></pre></figure>

<p>가장 흔히 알고 있는 word count 예제를 예로 들어보겠습니다.
위의 예시는 reduceByKey를 사용했으며, 아래의 예시는 groupByKey를 사용했습니다.
둘의 결과는 같지만 성능은 확인히 차이가 납니다.</p>

<p>먼저 위의 코드에서 <code class="highlighter-rouge">flatMap, map</code> 까지는 동일한 노드에서 실행이 됩니다.
하지만 reducer 부분에서는 모든 동일한 단어 쌍을 같은 노드로 이동시켜야 하기 때문에 <strong>Shuffle</strong> 이 발생합니다.</p>

<p><img src="/assets/images/reduceByKey.png" alt="" /></p>

<p>우선 reduceByKey의 경우, 먼저 각 노드에서 중간 집계를 진행하고 이에 대한 결과를 동일한 키 값으로 전송합니다.</p>

<p><img src="/assets/images/groupByKey.png" alt="" /></p>

<p>반면, groupByKey는 각 노드에 있는 데이터에 대해 바로 Shuffle 과정을 거치게 되고 결과를 내보냅니다.
따라서 groupByKey는 네트워크를 통해 전송되는 데이터의 양이 많아질 뿐만 아니라, <strong>Out of disk</strong> 문제가 발생할 수도 있습니다.</p>

<p>Shuffle은 기본적으로 비용이 큰 연산입니다.
groupByKey는 reduceByKey로 대체될 수 있기 때문에 많은 문서에서 이를 권장하고 있습니다.</p>

<p>​</p>

<h3 id="reference">Reference</h3>

<ul>
  <li>
    <p><a href="https://www.slideshare.net/databricks/strata-sj-everyday-im-shuffling-tips-for-writing-better-spark-programs">https://www.slideshare.net/databricks/strata-sj-everyday-im-shuffling-tips-for-writing-better-spark-programs</a></p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>Hive Metastore 구축 관련 문제와 해결과정</title>
	  <link>//hive-metastore-issue</link>
	  <author>Swalloow</author>
	  <pubDate>2017-08-11T19:18:00+09:00</pubDate>
	  <guid>//hive-metastore-issue</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>최근 Hive Metastore를 구축하면서 겪은 이슈와 해결과정을 기록해두려고 합니다.
사용 환경은 Spark 2.1.1 입니다.</p>

<p>​</p>

<h3 id="hive-metastore-parquet">Hive Metastore, Parquet</h3>

<p>먼저 겪었던 문제에 대해 설명드리자면 Hive Metastore에 분명히 테이블이 들어가있고,
Hue에서는 잘 보이는데 Zeppelin에서는 모든 데이터에 null 값이 찍혀있었습니다.</p>

<p>우선 Spark으로 Hive를 사용하는 방식이 2.0 버전 이후 부터 조금 변경되었습니다.
이전에는 HiveContext를 사용했다면, 이제 SparkSession에서 <code class="highlighter-rouge">.enableHiveSupport()</code> 추가만 하면 됩니다.
제플린에서는 SparkSession이 spark이라는 변수로 제공되는데,
이 경우 interpreter에 <code class="highlighter-rouge">zeppelin.spark.useHiveContext=true</code>를 추가해서 사용할 수 있습니다.</p>

<p>다시 문제로 돌아와서 좀 더 확인해보니 컬럼명에 대문자가 들어가면 모든 값이 null로 출력되고 있었습니다.
Spark 공식문서에 이와 관련된 내용이 잘 나와있습니다.</p>

<p>Spark SQL에서 Hive metastore로 데이터를 불러오는 경우, 성능 상의 이슈로 SerDe 대신 Spark SQL의 <strong>MetastoreParquet</strong> 를 사용합니다.
이때 주의사항으로 Hive는 대소문자를 구분하지 않지만, Parquet는 구분합니다. (Hive is case insensitive, while Parquet is not)</p>

<p>이를 위해 Spark 2.1.1 버전부터 새로운 Spark Properties가 추가되었습니다.</p>

<p>따라서, Zeppelin interpreter에 아래의 설정 값을 추가해주시면 해결됩니다.
<code class="highlighter-rouge">spark.sql.hive.caseSensitiveInferenceMode = INFER_AND_SAVE</code></p>

<p>​</p>

<h3 id="reference">Reference</h3>

<ul>
  <li><a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#hive-metastore-parquet-table-conversion">https://spark.apache.org/docs/latest/sql-programming-guide.html#hive-metastore-parquet-table-conversion</a></li>
  <li>
    <p><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#upgrading-from-spark-sql-21-to-22">http://spark.apache.org/docs/latest/sql-programming-guide.html#upgrading-from-spark-sql-21-to-22</a></p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>Spark DataFrame을 MySQL에 저장하는 방법</title>
	  <link>//spark-df-mysql</link>
	  <author>Swalloow</author>
	  <pubDate>2017-07-17T19:18:00+09:00</pubDate>
	  <guid>//spark-df-mysql</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>Spark에서 MySQL에 접근하고 DataFrame을 read, write 하는 방법에 대해 정리해보았습니다.
참고로 저는 Spark 2.1.0 버전을 사용 중 입니다.</p>

<p>​</p>

<h3 id="mysql-jdbc-driver">MySQL JDBC Driver</h3>

<p>JDBC를 통해 접근하기 때문에 드라이버가 필요합니다.
만일 SBT를 사용하신다면, build.sbt에 maven의 <code class="highlighter-rouge">mysql-connector-java</code> 를 추가하시면 됩니다.</p>

<p>직접 jar 파일을 사용해야하는 상황이라면, 다음 링크를 통해 다운받으시면 됩니다.
<a href="https://dev.mysql.com/downloads/connector/j/">https://dev.mysql.com/downloads/connector/j/</a></p>

<p>그리고 받으신 jar 파일을 -jars 옵션으로 추가해주셔야 합니다.</p>

<p><code class="highlighter-rouge">–jars /home/example/jars/mysql-connector-java-5.1.26.jar</code></p>

<p>마지막으로 spark-submit 을 사용하신다면, –packages 옵션을 추가해주시면 됩니다.</p>

<p><code class="highlighter-rouge">--packages mysql:mysql-connector-java:5.1.39</code></p>

<p>​</p>

<h3 id="spark-dataframe-mysql">Spark DataFrame MySQL</h3>

<p>Spark의 DataFrame은 read, write 함수를 통해 쉽게 데이터를 가져오거나 저장할 수 있습니다.
아래 예시는 Scala 언어로 작성했습니다.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.spark.sql.SaveMode</span>
<span class="k">import</span> <span class="nn">java.util.Properties</span>

<span class="k">val</span> <span class="n">tempDF</span> <span class="k">=</span> <span class="nc">List</span><span class="o">((</span><span class="s">"1"</span><span class="o">,</span> <span class="s">"2017-06-01"</span><span class="o">,</span> <span class="s">"2017-06-03"</span><span class="o">)).</span><span class="n">toDF</span><span class="o">(</span><span class="s">"id"</span><span class="o">,</span> <span class="s">"start"</span><span class="o">,</span> <span class="s">"end"</span><span class="o">)</span>
<span class="k">val</span> <span class="n">properties</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Properties</span><span class="o">()</span>
<span class="n">properties</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">"user"</span><span class="o">,</span> <span class="s">"userId"</span><span class="o">)</span>
<span class="n">properties</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">"password"</span><span class="o">,</span> <span class="s">"password"</span><span class="o">)</span>
<span class="n">tempDF</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">mode</span><span class="o">(</span><span class="nc">SaveMode</span><span class="o">.</span><span class="nc">Append</span><span class="o">).</span><span class="n">jdbc</span><span class="o">(</span><span class="s">"jdbc:mysql://url/database"</span><span class="o">,</span> <span class="s">"table"</span><span class="o">,</span> <span class="n">properties</span><span class="o">)</span></code></pre></figure>

<p>위 예제에서는 Properties를 통해 설정값을 넣어주었습니다.
유저 정보나 주소는 맞게 변경해주시면 됩니다.</p>

<p>mode 라는 것이 있는데 <code class="highlighter-rouge">SaveMode.Append</code>는 기존의 테이블에 추가하는 방식이고
<code class="highlighter-rouge">SaveMode.Overwrite</code>의 경우 기존의 테이블을 새로운 데이터로 대체하는 방식입니다.</p>

<p>​</p>

	  ]]></description>
	</item>

	<item>
	  <title>Spark 2.2.0 릴리즈 업데이트 정리</title>
	  <link>//spark22</link>
	  <author>Swalloow</author>
	  <pubDate>2017-07-14T19:18:00+09:00</pubDate>
	  <guid>//spark22</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>7월 11일 약 2개월 만에 Spark 2.2.0이 릴리즈 되었습니다.
어떤 변경 사항들이 있었는지 릴리즈 노트를 통해 간략하게 정리해보았습니다.</p>

<p>​</p>

<h3 id="pypi---pyspark-">pypi 를 통한 PySpark 설치</h3>

<p>드디어 PySpark이 <code class="highlighter-rouge">pip</code>을 지원하게 되었습니다.
<code class="highlighter-rouge">pip install pyspark</code> 명령어를 통해 쉽게 설치 가능합니다.
설치된 버전은 Spark 2.2.0 버전 입니다.</p>

<p><img src="/assets/images/pyspark-install.png" alt="" /></p>

<p><code class="highlighter-rouge">numpy, pandas</code> 파이썬 패키지에 dependency가 있으며,
자세한 사항은 <a href="https://pypi.python.org/pypi/pyspark">pypi 패키지 링크</a>를 통해 확인하실 수 있습니다.
이번 업데이트를 통해 standalone cluster에서 누구나 쉽게 사용해 볼 수 있을 듯 합니다.</p>

<p>​</p>

<h3 id="structured-streaming">Structured Streaming</h3>

<p>이번 버전부터 Structured Streaming이 새로 추가 되었습니다.
Structured Streaming은 스트리밍 어플리케이션을 더 빠르고 쉽게 개발하기 위해 만들어진 패키지입니다.</p>

<p>Spark Streaming이 내부적으로 RDD API를 지원하는 반면, Structured Streaming은 DataFrame, Dataset API를 지원합니다.
언어는 Scala, Java, Python 모두 지원하며, <code class="highlighter-rouge">readStream</code> 이라는 메서드를 통해 다양한 저장소로부터 데이터를 읽을 수 있습니다.
특히 이번 업데이트를 통해 Apache Kafka 스트리밍 지원이 추가되었습니다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># Subscribe to 1 topic</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span> \
  <span class="o">.</span><span class="n">readStream</span> \
  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s">"kafka"</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s">"kafka.bootstrap.servers"</span><span class="p">,</span> <span class="s">"host1:port1,host2:port2"</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s">"subscribe"</span><span class="p">,</span> <span class="s">"topic1"</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">load</span><span class="p">()</span>
<span class="n">df</span><span class="o">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s">"CAST(key AS STRING)"</span><span class="p">,</span> <span class="s">"CAST(value AS STRING)"</span><span class="p">)</span></code></pre></figure>

<p>Structured Streaming에 대한 자세한 내용은 <a href="http://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html">http://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html</a> 에서 확인하실 수 있습니다.</p>

<p>​</p>

<h3 id="mllib">MLlib</h3>

<p>예상했던 대로 MLlib에도 많은 변화가 생겼습니다.
RDD-based MLlib이 아니라 DataFrame-based MLlib을 확인하시면 됩니다.</p>

<ul>
  <li>기존에 scala API만 지원하던 모델들에 <code class="highlighter-rouge">python, R API</code>가 추가되었습니다.</li>
  <li>지원이 추가된 모델은 <strong>Gradient Boosted Trees, Bisecting K-Means, LSH, Distributed PCA, SVD</strong> 입니다.</li>
  <li>DataFreame-based MLlib에 새로운 모델이 추가되었습니다.</li>
  <li>
    <p>추가된 모델은 <strong>LinearSVC (Linear SVM Classifier), ChiSquare test, Correlation,
Imputer feature transformer, Tweedie distribution, FPGrowth frequent pattern mining, AssociationRules</strong> 입니다.</p>

    <p>​</p>
  </li>
</ul>

<h3 id="sparkr">SparkR</h3>

<p>이번 업데이트를 통해 SparkR에서 Spark SQL API가 확대되었습니다.</p>

<ul>
  <li>R API에 Structured Streaming, Catalog가 추가되었습니다.</li>
  <li>to_json, from_json 메서드가 추가되었습니다.</li>
  <li>
    <p>Coalesce, DataFrame checkpointing, Multi-column approxQuantile 기능이 추가되었습니다.</p>

    <p>​</p>
  </li>
</ul>

<h3 id="graphx">GraphX</h3>

<p>GraphX는 버그 수정, 최적화 업데이트가 추가되었습니다.
이번 Structured Steaming이 메인에 추가된 것으로 보아,
추후에 DataFrame, DataSet API 기반의 GraphFrame이 추가될 수도 있다고 예상합니다.</p>

<ul>
  <li>PageRank, vertexRDD/EdgeRDD checkpoint 버그를 수정했습니다.</li>
  <li>
    <p>PageRank, Pregel API가 개선되었습니다.</p>

    <p>​</p>
  </li>
</ul>

<h3 id="core-and-sparksql-deprecations">Core and SparkSQL, Deprecations</h3>

<p>마지막으로 Core, SparkSQL 그리고 Deprecation 업데이트 입니다.
전체 업데이트 및 기타 자세한 내용은 맨 아래의 링크를 참고하시면 됩니다.</p>

<ul>
  <li>Python 2.6, Java 7, Hadoop 2.5 지원이 종료되었습니다.</li>
  <li><code class="highlighter-rouge">ALTER TABLE table_name ADD COLUMNS</code> 구문이 추가되었습니다.</li>
  <li>Cost-Based Optimizer 성능이 개선되었습니다.</li>
  <li>
    <p>CSV, JSON 포멧의 File listing/IO 성능이 개선되었습니다.</p>

    <p>​</p>
  </li>
</ul>

<h2 id="reference">Reference</h2>

<ul>
  <li>
    <p><a href="http://spark.apache.org/releases/spark-release-2-2-0.html">http://spark.apache.org/releases/spark-release-2-2-0.html</a></p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>AWS EMR step을 이용한 Spark Batch 작업 등록하기</title>
	  <link>//emr-step</link>
	  <author>Swalloow</author>
	  <pubDate>2017-07-02T19:18:00+09:00</pubDate>
	  <guid>//emr-step</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>AWS EMR은 특정 작업을 등록할 수 있는 <strong>step</strong> 이라는 기능을 제공합니다.
예를 들어 매일 새벽에 클러스터에서 돌려야하는 Batch 작업이 있다면 step과 스케줄러를 통해 쉽게 해결할 수 있습니다.</p>

<p>​</p>

<h3 id="emr-step">EMR Step</h3>

<p>Step은 AWS console 내에서 추가해도 되지만, AWS-Cli를 이용해서 등록해보도록 하겠습니다.
AWS-Cli로 등록하면 이후에 스크립트로 활용할 수도 있다는 편리함이 있습니다.</p>

<p>AWS EMR step을 등록하는 방법은 아래와 같습니다.
가독성을 위해 줄바꿈, 띄어쓰기를 했지만 실제로 등록할 때는 전부 붙이셔야 합니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>aws emr add-steps
    --cluster-id <span class="nv">$CLUSTERID</span>,
    --steps <span class="nv">Name</span><span class="o">=</span><span class="nv">$JOBNAME</span>,
    <span class="nv">Jar</span><span class="o">=</span><span class="nv">$JARFILE</span>,
    <span class="nv">Args</span><span class="o">=[</span>
        /usr/lib/spark/bin/spark-submit,
        --deploy-mode,client,
        --properties-file,/etc/spark/conf/spark-defaults.conf,
        --conf,spark.yarn.executor.memoryOverhead<span class="o">=</span>2048,
        --conf,spark.executor.memory<span class="o">=</span>4g,
        --packages,<span class="nv">$SPARK_PACKAGES</span>
    <span class="o">]</span>,
    <span class="nv">ActionOnFailure</span><span class="o">=</span><span class="k">${</span><span class="nv">ACTION_ON_FAIL</span><span class="k">}</span><span class="s1">'</span></code></pre></figure>

<p>Spark 작업 실행은 <code class="highlighter-rouge">Spark-submit</code>을 이용하여 클라이언트에 배포하는 형식입니다.
이를 위해 jar 파일이 클라이언트의 로컬 경로에 포함되어 있어야 합니다.
ActionOnFailure를 통해 실패 시 Terminate, Stop 등의 옵션을 지정할 수 있습니다.</p>

<p>만약 등록한 작업을 취소하고 싶다면, <code class="highlighter-rouge">cancel-steps</code>를 이용하시면 됩니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>aws emr cancel-steps ...</code></pre></figure>

<p>Spark 작업이 주기적으로 실행되어야 한다면,
가장 간단한 방법은 위의 EMR step 등록 스크립트를 crontab으로 등록하는 것 입니다.
만약 작업이 다양하고 복잡하다면, <strong>AWS Data Pipeline</strong> 이라는 제품을 고려해보는 것도 방법입니다.
<a href="https://aws.amazon.com/ko/datapipeline/details/">https://aws.amazon.com/ko/datapipeline/details/</a></p>

<p>​</p>

<h2 id="reference">Reference</h2>

<ul>
  <li>
    <p><a href="http://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-submit-step.html">http://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-submit-step.html</a></p>
  </li>
  <li>
    <p><a href="http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-work-with-steps.html">http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-work-with-steps.html</a></p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>AWS 환경에서 Cloudera Manager 설치하기</title>
	  <link>//cloudera-install</link>
	  <author>Swalloow</author>
	  <pubDate>2017-06-23T19:18:00+09:00</pubDate>
	  <guid>//cloudera-install</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>클라우데라 매니저는 하둡 에코시스템을 쉽게 설치하고 관리할 수 있도록 도와주는 도구입니다.
이를 이용하여 AWS EC2 인스턴스에 하둡 클러스터를 설치하고 실행시키는 방법에 대해 정리해보았습니다.
내 노트북에 가상머신 여러 개를 띄우기 힘든 경우에 좋은 대안이 될 수 있습니다.</p>

<p>​</p>

<h3 id="cloudera-manager">Cloudera Manager</h3>

<p>클라우데라는 데이터 인프라 관련 솔루션과 컨설팅을 하는 하둡 전문 기업입니다.
하둡의 창시자 “더그 커팅”도 클라우데라의 수석 아키텍트로 일하고 있습니다.
Impala, Hue 등 여러 오픈소스도 개발하고 있으며,
오픈소스 하둡 프로젝트로부터 사람들이 쉽게 설치할 수 있도록 배포판(CDH)을 만들어서 제공해줍니다.
Cloudera Manager에는 유료, 무료버전이 있는데 여기에서는 무료버전(Express)을 설치하겠습니다.</p>

<p>​</p>

<h3 id="install">Install</h3>

<p><img src="https://hadoopabcd.files.wordpress.com/2015/01/2.png?w=1378&amp;h=510" alt="" /></p>

<p>설치는 AWS EC2 m4.large / CentOS / 8GiB 에서 진행하였습니다.
프리티어인 t2.micro 의 경우 내려가거나 설치가 안될 수 있기 때문에 제외하고 아무거나 쓰셔도 상관없습니다.</p>

<p><img src="https://hadoopabcd.files.wordpress.com/2015/01/3.png" alt="" /></p>

<p>클러스터의 수에 따라 인스턴스를 생성해줍니다. 그리고 Elastic IP로 public IP를 할당시켜줍니다.
Cloudera Manager는 기본으로 7180 포트를 사용하기 때문에 Inbound에서 열어주어야 합니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>yum install wget
<span class="gp">$ </span>wget http://archive.cloudera.com/cm4/installer/latest/cloudera-manager-installer.bin
<span class="gp">$ </span>chmod +x cloudera-manager-installer.bin
<span class="gp">$ </span>./cloudera-manager-installer.bin</code></pre></figure>

<p>먼저 JDK를 설치한 다음, 위의 명령어를 통해 cloudera-manager-installer를 설치하고 실행시켜줍니다.</p>

<p><img src="/assets/images/cloudera-login.png" alt="" /></p>

<p>설치가 완료되고 나서, <code class="highlighter-rouge">localhost:7180</code>으로 접속하면 다음과 같은 로그인 화면이 나타납니다.
초기 유저 아이디와 패스워드는 admin 입니다. 이후에는 본인이 원하는 환경에 맞추어 설치를 진행하면 됩니다.</p>

<p>​</p>

<h3 id="trouble-shooting">Trouble Shooting</h3>

<p><img src="/assets/images/error.png" alt="Error1" /></p>

<p><code class="highlighter-rouge">Fatal Error: SELinux is enabled. It must be disabled to install and use this product.</code>
보안 관련된 문제인데 <code class="highlighter-rouge">vi /etc/selinux/config</code>로 들어가서 SELinux를 해제시켜주면 해결 됩니다.</p>

<p>​</p>

<p><img src="/assets/images/fail.png" alt="Error2" /></p>

<p>CDH를 설치하기 위해 <code class="highlighter-rouge">sudo vi /etc/ssh/sshd_config</code>에 들어가
임시로 루트 계정의 SSH 접속을 허용해주어야 합니다.
설치후에는 다시 잠금 설정해주셔도 됩니다.</p>

<p>​</p>

<h2 id="reference">Reference</h2>

<ul>
  <li>
    <p><a href="http://www.cloudera.com/documentation/manager/5-1-x/Cloudera-Manager-Installation-Guide/cm5ig_install_on_ec2.html">http://www.cloudera.com/documentation/manager/5-1-x/Cloudera-Manager-Installation-Guide/cm5ig_install_on_ec2.html</a></p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>Spark의 Random Sampling에 대하여</title>
	  <link>//spark-sampling</link>
	  <author>Swalloow</author>
	  <pubDate>2017-06-20T19:18:00+09:00</pubDate>
	  <guid>//spark-sampling</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>데이터를 분석하다보면 임의의 샘플을 추출해야 하는 상황이 생깁니다.
그래서 이번에는 Spark에서 랜덤 샘플링을 하는 방법에 대해 정리해보았습니다.</p>

<p>​</p>

<h3 id="sample">Sample()</h3>

<p>Spark RDD API 에는 다양한 sampling 메서드가 존재합니다.
그 중에서 가장 기본이 되는 <code class="highlighter-rouge">sample()</code>에 대해 먼저 알아보겠습니다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># sample(boolean withReplacement, double fraction, long seed)</span>
<span class="n">val</span> <span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="mi">1</span> <span class="n">to</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">rdd</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">false</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">count</span></code></pre></figure>

<p>첫 번째 인자는 추출 방식을 결정합니다. <strong>True면 복원추출, False면 비복원추출</strong> 을 실행합니다.
여기에서 말하는 복원추출이란, 한 번 뽑은 것을 다시 뽑을 수 있게 하는 방법을 말합니다.
세 번째 인자로 시드 변수를 지정할 수 있습니다.
시드란, 컴퓨터가 난수를 일정하게 생성하지 않도록 변화를 주는 값을 말합니다.</p>

<p>​</p>

<h3 id="takesample">takeSample()</h3>

<p>takeSample()도 랜덤 샘플링을 지원하는 메서드지만, 위와 조금 다른 점이 있습니다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># takeSample(boolean withReplacement, int num, long seed)</span>
<span class="n">val</span> <span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="mi">1</span> <span class="n">to</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">rdd</span><span class="o">.</span><span class="n">takeSample</span><span class="p">(</span><span class="n">false</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span></code></pre></figure>

<p><code class="highlighter-rouge">takeSample()</code>은 두 번째 인자를 지정하여 몇 개를 추출할 것인지 정할 수 있습니다.
하지만, 결과 값이 RDD가 아닌 리스트나 배열이기 때문에 <strong>메모리에 주의</strong> 해야 합니다.
정리하자면, 크기를 정해놓고 샘플을 추출하고자 한다면 takeSample() 메서드가 적합하고
메모리를 생각해서 작은 값을 추출할 때 사용하는 것이 좋습니다.</p>

<p>이외에도 <code class="highlighter-rouge">sampleByKey, sampleByKeyExtract</code> 메서드가 존재합니다.</p>

<p>​</p>

<h2 id="reference">Reference</h2>

<ul>
  <li>
    <p><a href="https://spark.apache.org/docs/1.6.2/api/java/org/apache/spark/rdd/RDD.html">https://spark.apache.org/docs/1.6.2/api/java/org/apache/spark/rdd/RDD.html</a></p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>Spark의 Temporary View에 대하여</title>
	  <link>//spark-temp-view</link>
	  <author>Swalloow</author>
	  <pubDate>2017-06-16T19:18:00+09:00</pubDate>
	  <guid>//spark-temp-view</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>SQL의 View 처럼 Spark에서도 View를 지원합니다.
이 포스팅에서는 Spark 2.1.0 부터 생긴 <code class="highlighter-rouge">Spark Global Temporary View</code>와
기존의 <code class="highlighter-rouge">TempView</code>가 어떤 차이가 있는지 그리고 어떻게 사용해야하는지 알아보곘습니다.</p>

<p>​</p>

<h3 id="spark-temporary-view">Spark Temporary View</h3>

<p>공식문서를 보면 Spark의 Temporary View는 Session-Scope 입니다.
무슨 말이냐 하면, View의 생명주기가 세션에 달려있다는 뜻 입니다.
(여기에서 말하는 세션은 SparkSession 입니다)
그리고, 세션이 종료되면 자동으로 View 테이블이 Drop 됩니다.</p>

<p>​</p>

<h3 id="createorreplacetempview">CreateOrReplaceTempView</h3>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="n">query</span><span class="p">)</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span>
<span class="k">print</span> <span class="n">df</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="n">df</span><span class="o">.</span><span class="n">CreateOrReplaceTempView</span><span class="p">(</span><span class="s">"TempView"</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">dropTempView</span><span class="p">(</span><span class="s">"TempView"</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">unpersist</span><span class="p">()</span></code></pre></figure>

<p>먼저 기존에 사용하던 TempView를 보겠습니다.
위의 예시는 PySpark 코드입니다.
세 번째 줄의 <code class="highlighter-rouge">createOrReplaceTempView</code>가 View를 생성하는 함수인데,
Spark은 Lazy evaluation이기 때문에 아직 실행 되기 이전 입니다.
이후 두 번째 줄에서 count() 함수를 실행하면 생성되며,
TempView라는 이름으로 메모리에 두고 사용할 수 있게 됩니다.
다 사용한 다음에는 꼭 <code class="highlighter-rouge">unpersist</code> 함수로 할당된 메모리를 해제시켜줘야 합니다.</p>

<p>위와 다르게 Temp View에 대한 명령만 내리고 마지막에 한번에 처리해도 되지만,
여러 개로 쪼개서 명령을 내리는 것이 상대적으로 빠르다고 합니다.</p>

<p>​</p>

<h3 id="global-temporary-view">Global Temporary View</h3>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">GLOBAL</span> <span class="k">TEMPORARY</span> <span class="k">VIEW</span> <span class="n">temp_view</span> <span class="k">AS</span> <span class="k">SELECT</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="k">FROM</span> <span class="n">tbl</span>
<span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">global_temp</span><span class="p">.</span><span class="n">temp_view</span>
<span class="k">DROP</span> <span class="k">VIEW</span> <span class="n">global_temp</span><span class="p">.</span><span class="n">temp_view</span></code></pre></figure>

<p>위의 예시는 Spark SQL 코드입니다.
Global Temporary View는 Spark 2.1.0에서 처음 소개되었으며, <code class="highlighter-rouge">GLOBAL TEMPORARY VIEW</code> 라는 키워드로 생성합니다.
그렇게 선언하고 나면 일종의 임시 테이블로 접근할 수 있습니다.
삭제할 때는 <code class="highlighter-rouge">DROP VIEW</code> 라는 키워드로 삭제합니다.</p>

<p>하지만 Global Temporary View는 조금 위험합니다.
이 View는 말 그대로 전역적인 상태로 남기 위해 시스템의 임시 데이터베이스로 연결됩니다.
그래서 접근할 때, <code class="highlighter-rouge">global_temp</code>로 접근하게 됩니다.</p>

<p>결론부터 말하자면 Global Temporary View는 모든 세션에서 공유 가능하며,
Spark 어플리케이션이 종료되기 전까지 살아있게 됩니다.
제 경우 Master 노드의 하드디스크에 저장되어 있었습니다.
이렇게 되면 일단 IO로 인해 로딩속도가 상당히 느려지고,
만일 View의 크기가 메모리 용량을 넘어갔더라면 Master가 내려갈 수도 있는 상황입니다.
이와 같은 이유로 Global Temporary View는 신중히 사용하는 것이 좋습니다.</p>

<p>​</p>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="https://spark.apache.org/docs/2.1.1/api/java/org/apache/spark/sql/catalog/Catalog.html">https://spark.apache.org/docs/2.1.1/api/java/org/apache/spark/sql/catalog/Catalog.html</a></li>
  <li>
    <p><a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#global-temporary-view">https://spark.apache.org/docs/latest/sql-programming-guide.html#global-temporary-view</a></p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>Jupyter에서 Scala로 Spark 사용하는 방법</title>
	  <link>//jupyter-spark</link>
	  <author>Swalloow</author>
	  <pubDate>2017-03-22T19:18:00+09:00</pubDate>
	  <guid>//jupyter-spark</guid>
	  <description><![CDATA[
	     <p>​   ​</p>

<p>이 글은 평소에 <strong>Jupyter Notebook</strong> 에 익숙해져있는 분들께 유용할 듯 합니다.
Zeppelin Notebook을 설정하는 방법은 <a href="http://swalloow.github.io/spark-zeppelin-install">이전 포스팅</a>을 참고하시면 됩니다.</p>

<p>​   ​</p>

<h2 id="apache-toree">Apache Toree</h2>

<p><img src="/assets/images/jupyter-toree.png" alt="Jupyter-Toree" /></p>

<p><strong>Apache Toree</strong> 는 Jupyter 커널을 통해 Spark에 접속하도록 해주는 아파치 오픈소스 프로젝트입니다.
기존의 IPython Notebook은 파이썬에 제한되어 있었지만
Jupyter Kernel을 통해 다른 언어까지 확장 가능하도록 바뀌었습니다 (왼쪽 그림 참조).</p>

<p>여기에서 더 나아가 Apache Toree는 <strong>Toree Kernel</strong> 을 통해 바로 Spark Driver에 연결함으로써,
Jupyter에서 Scala 언어로 Spark Driver/Context를 사용할 수 있게 만들었습니다.</p>

<p>Toree가 Zeppelin과 다른 점은 <strong>Jupyter protocol</strong> 을 사용할 수 있다는 점 입니다.
이미 수많은 생태계가 구축되어 있는 Jupyter에서 Spark가 잘 돌아간다면 굳이 Zeppelin을 쓸 필요가 있을까요 (<em>시각화가 어마어마한 강점이긴 합니다</em>).
Zeppelin도 결국 Jupyter와 같은 플랫폼이 되기를 희망할거라 생각합니다.</p>

<p>GitHub : <a href="https://github.com/apache/incubator-toree">https://github.com/apache/incubator-toree</a></p>

<p>​   ​</p>

<h2 id="jupyter-notebook-toree-">Jupyter Notebook에 Toree 설치하기</h2>

<p>Jupyter 노트북 커널 설정하는 방법은 <a href="http://swalloow.github.io/jupyter-notebook-kernel">Jupyter Notebook 다중커널 설정하기</a>을,
Scala와 Spark을 설치하는 방법은 <a href="http://swalloow.github.io/spark-zeppelin-install">OS X에서 Homebrew로 Spark, Zeppelin 설치하기</a>를 참고하시기 바랍니다.</p>

<p>Toree는 아직 pre 버전만 존재하기 때문에 <code class="highlighter-rouge">--pre</code> 옵션을 붙여주시거나 파이썬 패키지를 통해 설치해주시면 됩니다.
설치가 완료되면 jupyter kernel에 toree kernel을 설치해주는 과정이 필요한데 명령어를 통해 이 과정을 자동으로 진행합니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>pip install https://dist.apache.org/repos/dist/dev/incubator/toree/0.2.0/snapshots/dev1/toree-pip/toree-0.2.0.dev1.tar.gz
<span class="gp">$ </span>jupyter toree install</code></pre></figure>

<p>혹시 <code class="highlighter-rouge">FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/spark/python/lib'</code>
이런 오류가 난다면, Spark 경로 환경변수를 읽지 못하는 문제입니다. <strong>Homebrew</strong> 를 통해 설치하셨다면 다음과 같이 환경변수를 등록해주시면 됩니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span><span class="nb">export </span><span class="nv">SPARK_HOME</span><span class="o">=</span>/usr/local/Cellar/apache-spark/2.1.0/libexec</code></pre></figure>

<p>​   ​</p>

<h2 id="section">잘 동작하는지 테스트를 해보자</h2>

<p><img src="/assets/images/toree-kernel.png" alt="Toree-Kernel" /></p>

<p>잘 설치되었다면 <code class="highlighter-rouge">new</code> 했을 때 <code class="highlighter-rouge">Apache-Toree Scala</code>가 보이실 겁니다.
잘 동작하는지 간단한 WordCounter 예제를 실행시켜 보시면 잘 동작하는 것을 확인할 수 있습니다.</p>

<p><img src="/assets/images/toree-tuto.png" alt="Toree-Tuto" /></p>

<p>​   ​</p>

<h2 id="docker--jupyter--">Docker를 통해 Jupyter 설정하는 방법</h2>

<p>나는 이 모든 것이 귀찮다! 라면 Docker를 통해 노트북을 실행시키면 됩니다.
제가 Docker를 통해 Notebook을 사용하지 않는 이유는 딱 한 가지 입니다.
Spark만 쓰고 싶었는데 기타 등등이 많이 설치되어 있어서 컨테이너 메모리가 무려 <strong>4기가</strong> 나 됩니다…
그래도 쓰겠다 싶으신 분들은 아래 링크를 참고하시면 됩니다.</p>

<p><a href="https://hub.docker.com/r/jupyter/all-spark-notebook/">https://hub.docker.com/r/jupyter/all-spark-notebook/</a></p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>docker pull jupyter/all-spark-notebook
<span class="gp">$ </span>docker run -it --rm -p 8888:8888 jupyter/all-spark-notebook</code></pre></figure>

<p>이렇게 실행하고 8888번 포트로 접속하면 노트북을 실행할 수 있습니다.</p>

<p>​   ​</p>

	  ]]></description>
	</item>


</channel>
</rss>
