<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>swalloow.github.io/</title>
   
   <link>http://swalloow.github.io/</link>
   <description>About Data Science, Data Engineering</description>
   <language>ko-KO</language>
   <managingEditor> Swalloow</managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>Kafka Connect로 S3에 데이터를 저장해보자</title>
	  <link>//kafka-connect</link>
	  <author>Swalloow</author>
	  <pubDate>2018-11-16T19:18:00+09:00</pubDate>
	  <guid>//kafka-connect</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>Kafka에는 정말 유용한 컴포넌트들이 존재합니다.
오늘은 그 중 하나인 Kafka-Connect에 대해 알아보고,
Confluent에서 제공하는 Kafka-Connect-S3를 활용하여
S3로 데이터를 저장하는 방법에 대해 정리해보려고 합니다.</p>

<p>​</p>

<h2 id="kafka-connect">Kafka Connect</h2>

<p><img src="http://drive.google.com/uc?export=view&amp;id=172qcC4a0mgYnkeZHlH7HH4lar1dAebA3" alt="kafka-connect" /></p>

<p>우리는 서버로부터 생성되는 데이터를 실시간으로 Kafka에 보내기도 하고,
Kafka Topic에 쌓여있는 데이터를 실시간으로 RDBMS, Object Storage와 같은 시스템에 보내기도 합니다.
Kafka Connect는 위의 그림과 같이 다양한 시스템과 Kafka 사이의 연결을 도와주는 역할을 하는 컴포넌트입니다.
Source System에서 Kafka로 들어가는 Connector를 Source Connect라 부르고,
Kafka에서 Target System으로 보내는 Connector를 Sink Connect라 부릅니다.</p>

<p>Kafka Connect는 JSON, Avro, Protobuf 등의 다양한 직렬화 포멧을 지원하며
Kafka Schema Registry와 연동시켜 공통된 스키마 지정을 할 수도 있습니다.</p>

<p>사실 Fluentd와 ELK Stack에서 사용하는 Logstash 등 서로 다른 시스템 간의 브릿지 역할을 하는 프레임워크들은 다양하게 존재합니다.
하지만 Kafka Connect가 갖는 강점은 Kafka와 긴밀히 연동되어 있다는 점 입니다.</p>

<p>Kafka Connect를 사용하지 않고 데이터를 실시간으로 전달하기 위해서는 Producer, Consumer API를 사용해야 합니다.
이 과정에서 이미 처리되거나 실패한 데이터를 추적한다거나, 데이터 분산처리, 작업을 배포하는 등의 작업을 수행해야만 합니다.</p>

<p>Kafka Connect는 앞의 모든 작업을 수행할 뿐만 아니라 connector task를 클러스터 전체에 자동으로 배포합니다.
또한, Connect Worker 중에 하나가 실패하거나 Network partition이 발생하더라도 실행하던 작업을 나머지 Worker들에게 자동으로 재조정합니다.
Offset을 자동으로 관리, 유지하기 때문에 재시작하더라도 중단 시점부터 다시 시작할 수 있고 (Exactly Once Delivery),
High performance Kafka library로 작성되어 빠르며 불필요한 polling 작업을 수행하지 않습니다.
무엇보다 코드 한 줄 없이 사용하기 편하다는 것도 큰 강점입니다.
혹시 Kafka를 이미 중앙 집중형 로그 저장소로 사용하고 있다면 Kafka Connect를 고려해볼만 하다고 생각합니다.</p>

<p>​</p>

<h2 id="kafka-connect-s3">Kafka-Connect-S3</h2>

<p>이 글에서는 Confluent로 Kafka를 설치하지 않은 경우를 예시로 들겠습니다.
이미 confluent-hub를 설치하셨거나 Confluent로 Kafka를 설치하셨다면 공식문서를 따라가시면 됩니다.</p>

<p><img src="http://drive.google.com/uc?export=view&amp;id=1R80lOarW9k1RGv2kYAYxNz_-q6wUsm28" alt="aws-kafka-s3" /></p>

<p>데이터 인프라가 AWS 환경에 구축되어 있다면 S3를 Cold Storage로 많이 사용하게 됩니다.
최대한 단순하게 그림을 그려보면 위의 그림과 같은 아키텍쳐가 나오게 됩니다.
여기에서는 Kafka에서 S3로 실시간 데이터를 저장하기 위해 Kafka-Connect-S3를 사용하게 됩니다.</p>

<p>먼저 confluent에서 kafka-connect-s3를 다운받아 plugins 경로에 추가합니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>wget https://api.hub.confluent.io/api/plugins/confluentinc/kafka-connect-s3/versions/4.1.1/archive
<span class="gp">$ </span>unzip archive
<span class="gp">$ </span>mkdir -p plugins/kafka-connect-s3
<span class="gp">$ </span>cp confluentinc-kafka-connect-s3-4.1.1/lib/<span class="k">*</span> plugins/kafka-connect-s3/</code></pre></figure>

<p>이제 kafka config 경로에 <code class="highlighter-rouge">connect.properties</code>라는 이름으로 설정 파일을 추가합니다.
<code class="highlighter-rouge">bootstrap.servers</code>와 <code class="highlighter-rouge">plugin.path</code> 경로는 상황에 맞게 수정하시면 됩니다.
추가로 kafka 클러스터를 private network로 연결하고 싶다면 9093 포트를 사용해주시면 됩니다.</p>

<script src="https://gist.github.com/Swalloow/3657a0f3c44e969f388a39ff135d38d3.js"></script>

<p>기존 클러스터에 Authentication credentials, encryption이 설정되어 있다면,
connect.properties에 관련 설정을 추가해주셔야 합니다.</p>

<p>다음 S3에 데이터가 저장될 Bucket을 생성하고, AWS Credentials를 설정합니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>pip install awscli
<span class="gp">$ </span>aws configure</code></pre></figure>

<p>이제 sink connector 관련 설정 파일을 <code class="highlighter-rouge">s3-sink.properties</code>라는 이름으로 config 경로에 추가합니다.
topics와 s3.bucket.name의 이름은 맞게 수정해주셔야 합니다.</p>

<script src="https://gist.github.com/Swalloow/6dcc4604ca3847afa631d35ffaa24ac4.js"></script>

<p>이제 Kafka 설치 경로로 이동하고 Kafka-Connect를 실행시킵니다.
여기에서는 standalone mode로 실행시켰지만, 경우에 따라 cluster mode로 실행하거나
docker container로 실행시켜도 됩니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">./bin/connect-standalone.sh connect.properties s3-sink.properties</code></pre></figure>

<p>이제 지정한 S3 Bucket의 topic/my-topic-name/2018-11-16 경로에 가시면
지정한 설정 값에 따라 파일이 저장되는 것을 확인하실 수 있습니다.</p>

<p><img src="http://drive.google.com/uc?export=view&amp;id=1bepmpAHi7kwUnqvGOwyq0i8jSMIhhMeU" alt="" /></p>

<p>이미 Yahoo의 kafka-manager를 사용하고 계신 분들은 consumers 메뉴로 가시면
topic 마다 lag도 모니터링할 수 있습니다.</p>

<p>​</p>

<h3 id="kafka-connect-s3-configuration">Kafka-Connect-S3 Configuration</h3>

<p>데이터 인프라에 맞게 수정해야할 옵션은 아래와 같습니다.</p>

<ul>
  <li><strong>s3.part.size</strong>: S3의 multi part upload 사이즈를 지정</li>
  <li><strong>flush.size</strong>: file commit 시 저장할 record의 수 (파일 사이즈와 연관)</li>
  <li><strong>partitioner.class</strong>: partition 기준을 지정 (TimeBasedPartitioner는 시간을 기준으로 파티셔닝)</li>
</ul>

<p>이외에도 Avro Format과 Schema Registry를 사용하신다면 <code class="highlighter-rouge">format.class</code>, <code class="highlighter-rouge">schema.generator.class</code>를 수정해야 합니다.
더 자세한 내용은 <a href="https://docs.confluent.io/5.0.0/connect/kafka-connect-s3/configuration_options.html#s3-configuration-options">공식문서</a>에서 확인하시면 됩니다.</p>

<p>​</p>

<h2 id="reference">Reference</h2>

<p>사실 Kafka는 이미 대부분의 데이터 파이프라인에서 활용하고 있다는 것이 강점이라고 생각합니다.
ETL 과정이 다양하고 복잡할 수록 새로운 프레임워크가 추가되고 아키텍쳐가 복잡해지기 마련인데,
Kafka의 다양한 컴포넌트들을 잘 활용하면 아키텍쳐를 단순화시킬 수도 있습니다.</p>

<ul>
  <li>https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained</li>
  <li>https://docs.confluent.io/5.0.0/connect/kafka-connect-s3/index.html</li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>Raft consensus algorithm</title>
	  <link>//raft-consensus</link>
	  <author>Swalloow</author>
	  <pubDate>2018-09-01T19:18:00+09:00</pubDate>
	  <guid>//raft-consensus</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>Consensus란 분산 시스템에서 노드 간의 상태를 공유하는 알고리즘을 말합니다.
가장 유명한 알고리즘으로 Paxos가 있고, Zookeeper에서 사용하는 Zab이 있습니다.
Raft는 이해하기 어려운 기존의 알고리즘과 달리 쉽게 이해하고 구현하기 위해 설계되었습니다.
(PS. 이 글은 블록체인에서의 Consensus 알고리즘을 말하는 것이 아닙니다)</p>

<p>​</p>

<h2 id="what-is-consensus-problem">What is consensus problem</h2>

<p>하나의 클라이언트와 서버가 있고 클라이언트가 서버에게 데이터를 전달한다고 가정해보겠습니다.
서버는 하나의 노드로 이루어져있기 때문에 합의가 이루어지는건 아주 쉬운 문제입니다.
(여기에서 말하는 합의는 공유된 상태라고 이해하시면 됩니다)</p>

<p><img src="http://drive.google.com/uc?export=view&amp;id=1-KZIAy8UAFgLlUCHESjbOO7pgCVHByGS" alt="con1" /></p>

<p>만일 위 그림처럼 여러 노드로 이루어진 분산 서버에서 합의를 이루어내야한다면 어떻게 해야할까요?
이러한 문제를 <strong>distributed consensus problem</strong> 이라고 합니다.</p>

<p>​</p>

<h2 id="raft-algorithm">Raft Algorithm</h2>

<p>Raft의 node는 <strong>Follower, Candidate, Leader</strong>라는 3가지 state를 가집니다.
모든 노드는 처음에 Follower state를 가지고 시작합니다.
만일 Follower가 Leader의 응답을 받지 못하면 Candidate 상태로 전환될 수 있습니다.</p>

<p><img src="http://drive.google.com/uc?export=view&amp;id=1CiOwa7f7dv50HgqvF3Rm4MrWVQlQMRF4" alt="election" /></p>

<p>Candidate는 다른 노드들에게 투표를 요청하고 노드들은 투표 결과를 응답으로 전달합니다.
노드 중 가장 많은 표를 얻은 노드는 Leader가 될 수 있습니다.
이러한 프로세스를 <strong>Leader Election</strong> 이라고 부릅니다.</p>

<p>​</p>

<h3 id="leader-election">Leader Election</h3>

<p>Raft는 투표를 관리하기 위해 두 가지 timeout 설정을 가지고 있습니다.
첫 번째는 <strong>Election timeout</strong> 입니다.
Election timeout 이란, Follower에서 Candidate로 전환되기 위해 기다리는 시간을 의미합니다.
일반적으로 Election timeout은 150ms에서 300ms 사이의 값으로 랜덤하게 설정됩니다.</p>

<p><img src="http://drive.google.com/uc?export=view&amp;id=18EJCDHGadTVHAtFEoXIms6OnB8hn8DTY" alt="timeout" /></p>

<ol>
  <li>Election timeout이 끝나면 Follower는 Candidate가 되고 Election term을 시작합니다.</li>
  <li>Candidate는 본인에게 투표를 하고 다른 노드들에게 투표 요청 메세지를 전달합니다.</li>
  <li>만일 메세지를 받는 노드가 해당 Election term에서 아직 투표를 하지 않았다면, 먼저 메세지를 전달해준 Candidate에게 투표합니다.</li>
  <li>투표를 마친 해당 노드는 Election timeout이 초기화 됩니다.</li>
  <li>가장 많은 표를 받은 노드가 Leader로 선정됩니다.</li>
</ol>

<p><img src="http://drive.google.com/uc?export=view&amp;id=14gQt-B4NYslCtwca8xcATt9a2IVH7qqH" alt="reelection" /></p>

<ol>
  <li>선정 이후 Leader는 Append Entries 메세지를 Follower들에게 전송합니다.
(이 메세지는 <strong>Heartbeat timeout</strong> 에 설정된 간격마다 보내게 됩니다)</li>
  <li>Follower들은 Append Entries 메세지를 받으면 Election timeout이 초기화되고 메세지에 대한 응답을 Leader에게 보냅니다.</li>
  <li>만일 Follower에게 Heartbeat가 도달하지 않으면 다시 Election term이 시작되고, Follower는 Candidate 상태로 전환됩니다.
(위 그림은 노드A가 죽고 난 이후 노드B가 Leader로 선정되고 Heartbeat 메세지를 전달하는 예시입니다)</li>
</ol>

<p><img src="http://drive.google.com/uc?export=view&amp;id=1OtRVbaqBh-QmGg5JjDF4hxt4LFxK4oTP" alt="same1" /></p>

<ol>
  <li>만일 두 개의 노드가 동시에 Election term을 시작하고 메세지가 동시에 Follower에게 도달한다고 가정해보겠습니다.</li>
  <li>이러한 경우 노드A, 노드B는 2표씩 얻게 되고, 표가 동일하므로 해당 Election term에는 Leader가 선정되지 않습니다.</li>
  <li>Leader가 선정되지 않았으므로 Election timeout에 따라 새로운 Election term을 시작하게 됩니다.</li>
</ol>

<p><img src="http://drive.google.com/uc?export=view&amp;id=1o-llsn2rZop8u0jxzam8_K2hu-3n-nvD" alt="same2" /></p>

<p>​</p>

<h3 id="log-replication">Log Replication</h3>

<p><img src="http://drive.google.com/uc?export=view&amp;id=13PUDuWgR3bl8frAbiHhpkRieWuKrygVI" alt="message" /></p>

<p>Leader가 선정되고 난 이후, 시스템의 모든 변화는 Leader를 통해 이루어집니다.
클라이언트는 Leader에게 데이터를 전달하고, Leader는 데이터의 복제하여 Follower에게 전달합니다.
이 과정은 앞서 언급했던 <strong>Append Entries</strong> 메세지를 통해 이루어집니다.</p>

<p><img src="http://drive.google.com/uc?export=view&amp;id=1RQVmzrOGOg0HZnZ8fO5dpZwoW7XVMd79" alt="res" /></p>

<p>Follower는 받은 데이터를 commit 하고 결과를 Leader에게 전달합니다.
Leader는 Follow로부터 받은 결과를 Client에게 전달합니다.</p>

<p>​</p>

<h2 id="reference">Reference</h2>

<p>정리하자면 분산 시스템은 fault-tolerence를 보장하기 위해 consensus algorithm을 사용하고 있고,
분산 시스템을 다루는 프레임워크마다 Consensus 구현이 조금씩 다를 수 있습니다.
그리고 원활한 Leader Election을 위해 클러스터 노드의 개수는 홀수로 구성하는 것이 좋습니다.</p>

<p>Raft의 경우 Redis cluster에서 응용하여 사용하고 있고,
Elasticsearch cluster 또한 quorum-based consensus algorithm을 사용하고 있습니다.
아래의 Raft 논문과 시각화 자료 링크를 보시면 더 쉽게 이해할 수 있습니다.</p>

<ul>
  <li>https://raft.github.io/raft.pdf</li>
  <li>http://thesecretlivesofdata.com/raft/</li>
  <li>https://raft.github.io/</li>
</ul>


	  ]]></description>
	</item>

	<item>
	  <title>AWS에 Hadoop MR 어플리케이션 환경 구축하기</title>
	  <link>//aws-hadoop</link>
	  <author>Swalloow</author>
	  <pubDate>2018-06-13T19:18:00+09:00</pubDate>
	  <guid>//aws-hadoop</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>이번 학기에 하둡 프로그래밍 강의를 들으면서 정말 실습 환경의 개선이 필요하다는 생각이 들었습니다…
나약한 실습 환경속에서 과제와 기말 프로젝트를 제출해야하는 후배들을 위해 AWS를 추천합니다!</p>

<p>​</p>

<h2 id="ec2-amazon-linux2---">EC2 Amazon Linux2에 기본 환경 구축</h2>

<p>AWS에는 EMR이라는 클러스터 서비스가 있지만, 스터디 목적이라면 비용을 생각해서 사용하지 않겠습니다.
Amazon Linux AMI는 EC2에서 편하게 사용할 수 있도록 지원하고 관리하는 리눅스 이미지입니다.
만일 학생용 크레딧이 있다면 <strong>t2.medium</strong> 인스턴스를 추천합니다.</p>

<p>먼저, JAVA JDK와 Hadoop 파일을 받겠습니다. 실습 환경은 자바 7, 하둡 1.2 버전입니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>sudo yum update -y
<span class="gp">$ </span>sudo yum install -y java-1.7.0-openjdk-devel
<span class="gp">$ </span>wget https://archive.apache.org/dist/hadoop/core/hadoop-1.2.1/hadoop-1.2.1.tar.gz
<span class="gp">$ </span>tar xvfz hadoop-1.2.1</code></pre></figure>

<p>그리고 자바 프로젝트를 위해 Maven도 설치해줍니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>wget http://mirror.navercorp.com/apache/maven/maven-3/3.5.3/binaries/apache-maven-3.5.3-bin.tar.gz
<span class="gp">$ </span>tar xvfs apache-maven-3.5.3-bin.tar.gz
<span class="gp">$ </span>mv apache-maven-3.5.3/ apache-maven
<span class="gp">$ </span>sudo vi /etc/profile.d/maven.sh

<span class="c"># Apache Maven Environment Variables</span>
<span class="c"># MAVEN_HOME for Maven 1 - M2_HOME for Maven 2</span>
<span class="gp">$ </span><span class="nb">export </span><span class="nv">M2_HOME</span><span class="o">=</span>/home/ec2-user/apache-maven
<span class="gp">$ </span><span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="k">${</span><span class="nv">M2_HOME</span><span class="k">}</span>/bin:<span class="k">${</span><span class="nv">PATH</span><span class="k">}</span>

<span class="gp">$ </span>chmod +x maven.sh
<span class="gp">$ </span><span class="nb">source</span> /etc/profile.d/maven.sh</code></pre></figure>

<p>정상적으로 설치가 되었다면 아래의 명령어에 대한 결과가 나옵니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>java --version
<span class="gp">$ </span>mvn --version</code></pre></figure>

<p>​</p>

<h3 id="hadoop--">Hadoop 환경 구축</h3>

<p>실습환경은 <strong>Pseudo-Distibuted</strong> 모드로 진행합니다.
먼저 Password less SSH Login을 설정해주어야 합니다.
그리고 편의를 위해 hadoop-1.2.1 폴더에 Symbolic link를 생성하겠습니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># ssh login setting</span>
<span class="gp">$ </span>ssh-keygen -t rsa -P <span class="s2">""</span>
<span class="gp">$ </span>cat /home/ec2-user/.ssh/id_rsa.pub &gt;&gt; /home/ec2-user/.ssh/authorized_keys

<span class="c"># symbolic link</span>
<span class="gp">$ </span>ln -s hadoop-1.2.1 hadoop</code></pre></figure>

<p>이제 HDFS와 MR 실행을 위해 설정파일을 수정해줍니다.
먼저 <code class="highlighter-rouge">hadoop-env.sh</code>을 열어 <code class="highlighter-rouge">JAVA_HOME</code> 환경변수를 지정해줍니다.
가상분산모드에서는 masters, slaves 파일을 수정할 필요가 없습니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span><span class="nb">cd </span>hadoop
<span class="gp">$ </span>vi conf/hadoop-env.sh

<span class="c"># set JAVA_HOME in this file, so that it is correctly defined on</span>
<span class="c"># remote nodes.</span>

<span class="c"># The java implementation to use. Required.</span>
<span class="nb">export </span><span class="nv">JAVA_HOME</span><span class="o">=</span>/usr/lib/jvm/java-1.7.0

<span class="c"># Extra Java CLASSPATH elements.  Optional.</span>
<span class="c"># export HADOOP_CLASSPATH=</span></code></pre></figure>

<p>이제 <code class="highlighter-rouge">core-site.xml</code> 파일을 아래와 같이 수정해줍니다.
HDFS 데이터 파일들은 홈 디렉토리의 hadoop-data 폴더에 저장하겠습니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>vi conf/core-site.xml

&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;fs.default.name&lt;/name&gt;
        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
        &lt;value&gt;/home/ec2-user/hadoop-data/&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;</code></pre></figure>

<p><code class="highlighter-rouge">hdfs-site.xml</code> 파일도 수정해줍니다.
dfs.replication 프로퍼티는 복제 개수를 의미합니다.
일반적으로 복제 개수를 3으로 두는 것을 권장하지만,
실습에서는 Fully-Distributed 모드가 아니기 때문에 1로 설정하겠습니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>vi conf/hdfs-site.xml

&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;1&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;</code></pre></figure>

<p><code class="highlighter-rouge">mapred-site.xml</code> 파일도 수정해줍니다.
mapred.job.tracker 프로퍼티는 job tracker가 동작하는 서버를 말합니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>vi conf/mapred-site.xml

&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;mapred.job.tracker&lt;/name&gt;
        &lt;value&gt;localhost:9001&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;</code></pre></figure>

<p>​</p>

<h3 id="hadoop-mr">Hadoop MR</h3>

<p>이제 NameNode를 초기화하고 하둡과 관련된 모든 데몬을 실행합니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">./bin/hadoop namenode-format
./bin/start-all.sh</code></pre></figure>

<p>jps를 통해 자바 프로세스가 제대로 실행되었는지 확인할 수 있습니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>jps
3368 TaskTracker
2991 DataNode
3241 JobTracker
3480 Jps
2872 NameNode
3139 SecondaryNameNode</code></pre></figure>

<p>HDFS 웹 인터페이스 주소는 http://localhost:50070 이며,
MapReduce 웹 인터페이스 주소는 http://localhost:50030 입니다.
들어가시면 아래와 같은 화면이 나타납니다.</p>

<p><img src="http://drive.google.com/uc?export=view&amp;id=15OIYCbnc1cy93jJgqX1y8a5vfpCBkpqM" alt="" /></p>

<p>이제 기본으로 설치되어 있는 WordCount 예제를 실행시켜보겠습니다.
먼저 WordCount 예제의 input 데이터를 HDFS에 업로드하고 jar 파일과 output 경로를 지정해줍니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>./bin/hadoop fs -put conf/hadoop-env.sh ./hadoop-env.sh
<span class="gp">$ </span>./bin/hadoop jar hadoop-examples-1.2.1.jar wordcount hadoop-env.sh output</code></pre></figure>

<p>HDFS에 write한 결과는 HDFS의 output 경로에서 확인하실 수 있습니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>./bin/hadoop fs -ls output
<span class="gp">$ </span>./bin/hadoop fs -cat output/part-r-00000</code></pre></figure>

<p>​</p>

<h3 id="intellij">IntelliJ</h3>

<p>이번엔 예제가 아니라 Hadoop MR 어플리케이션 프로젝트를 새로 생성해보겠습니다.
IntelliJ에서 JAVA, maven 프로젝트를 생성하시면 됩니다.</p>

<p>그리고 pom.xml은 아래와 같이 수정해줍니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">&lt;groupId&gt;org.swalloow.hadoop&lt;/groupId&gt;
&lt;artifactId&gt;hadoop&lt;/artifactId&gt;
&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-core --&gt;
&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
        &lt;artifactId&gt;hadoop-core&lt;/artifactId&gt;
        &lt;version&gt;1.2.1&lt;/version&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;</code></pre></figure>

<p>Mapper와 Reducer 클래스를 수정한 다음, <code class="highlighter-rouge">mvn packages</code> 명령어를 통해 jar 파일을 생성합니다.
그리고 input 파일을 이전과 동일하게 HDFS에 추가하고 생성한 jar 파일을 통해 MR job을 실행시키시면 됩니다.</p>

<p>아래 링크는 코인 거래 데이터를 입력받아 이동평균선(SMA) 추세를 계산해주는 간단한 예시 프로젝트입니다.
템플릿은 자유롭게 참고하셔도 됩니다!</p>

<p>https://github.com/Swalloow/hadoop-mr-project</p>

<p>​</p>

	  ]]></description>
	</item>

	<item>
	  <title>제플린 노트북 자동 실행 스크립트 만들기</title>
	  <link>//zeppelin-bootstrap</link>
	  <author>Swalloow</author>
	  <pubDate>2017-09-13T19:18:00+09:00</pubDate>
	  <guid>//zeppelin-bootstrap</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>제플린 노트북을 사용하다보면 가끔 제플린 어플리케이션을 재시작해야 하는 경우가 있습니다.
이 때, view 또는 udf 등록을 위해 처음 실행시켜야 하는 노트북이 있다면 참 번거롭습니다.
하지만 <strong>Zeppelin Notebook API</strong> 사용한다면 이를 쉽게 자동화 할 수 있습니다.</p>

<p>​</p>

<h2 id="zeppelin-notebook-api">Zeppelin Notebook API</h2>

<p>제플린은 노트북 자동실행을 위한 REST API를 제공합니다.
하지만 제플린에 인증이 걸려있다면, 인증을 거쳐야만 API를 사용할 수 있습니다.
따라서, 먼저 curl로 세션 값을 받고 해당 노트북 아이디를 호출하시면 됩니다.</p>

<p>노트북 아이디는 해당 노트 URL의 가장 마지막 값 입니다. (ex 2AZPHY918)
아래의 스크립트는 아이디가 user, 패스워드가 1234인 경우를 예시로 들었습니다.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c">#!/bin/sh</span>
sudo /usr/lib/zeppelin/bin/zeppelin-daemon.sh stop
sleep 3
sudo /usr/lib/zeppelin/bin/zeppelin-daemon.sh start

sleep 15

<span class="nv">SESSION</span><span class="o">=</span><span class="s2">"</span><span class="sb">`</span>curl -i --data <span class="s1">'userName=user&amp;password=1234)'</span> -X POST http://zeppelin-url.com:8890/api/login | grep <span class="s1">'Set-Cookie: JSESSIONID='</span> | cut -d <span class="s1">':'</span> -f2 |  tail -1 | cut -d <span class="s1">';'</span> -f1<span class="sb">`</span><span class="s2">"</span>
<span class="nb">echo</span> <span class="nv">$SESSION</span>
curl -i -b <span class="k">${</span><span class="nv">SESSION</span><span class="k">}</span> -X POST http://zeppelin-url.com:8890/api/notebook/job/NOTEBOOK_ID</code></pre></figure>

<p>Notebook API를 활용하면 노트북 실행 뿐만 아니라, Cron이나 노트북 권한 설정도 자동화할 수 있습니다.
자세한 내용은 아래의 공식문서에서 확인하실 수 있습니다.</p>

<p>​</p>

<h3 id="reference">Reference</h3>

<ul>
  <li>
    <p><a href="https://zeppelin.apache.org/docs/0.7.3/rest-api/rest-notebook.html">https://zeppelin.apache.org/docs/0.7.3/rest-api/rest-notebook.html</a></p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>AWS EMR에서 S3 사용 시 주의사항</title>
	  <link>//aws-emr-s3-spark</link>
	  <author>Swalloow</author>
	  <pubDate>2017-09-09T19:18:00+09:00</pubDate>
	  <guid>//aws-emr-s3-spark</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>AWS EMR에서 Spark을 사용하는 경우, S3를 저장소로 사용하는 경우가 많습니다.
이때 주의해야 할 사항들을 정리해보았습니다.</p>

<p>​</p>

<h2 id="aws-emr-spark--s3">AWS EMR, Spark 그리고 S3</h2>

<p><img src="/assets/images/aws_etl.png" alt="" /></p>

<p>Daily로 돌려야 하는 ETL 작업의 경우 위와 같이 간단한 아키텍쳐로 구성하는 경우가 많습니다.
대부분의 경우 저장소로 S3를 적극 활용하게 됩니다.
최초 입수되는 로그를 저장하기도 하고, Transformation 작업 이후 중간 또는 최종 데이터로 저장하기도 합니다.</p>

<p>​</p>

<h2 id="section">문제 상황</h2>

<figure class="highlight"><pre><code class="language-md" data-lang="md">java.io.IOException: Connection reset by peer
ERROR ContextCleaner: Error cleaning broadcast 5</code></pre></figure>

<p>최근 Spark RDD 코드를 DataFrame으로 리팩토링 하던 중에 위와 같은 오류를 겪었습니다.
일별 로그를 불러와서 전처리하고 다시 저장하는데 s3 write 부분에서 갑자기 Executor의 Connection이 끊기는 문제였습니다.</p>

<p><img src="/assets/images/ganglia_a.png" alt="" /></p>

<p>Ganglia 모니터링 결과를 보면 중간에 약 15분의 공백이 있는데,
이 부분이 Connection이 중간에 끊기고 다시 뜰 때까지 걸리는 시간입니다.</p>

<p>​</p>

<h2 id="s3n-s3a-s3">S3N, S3A, S3</h2>

<p>먼저 S3는 File System이 아닌 <strong>Object Storage</strong> 라는 점을 알고 계셔야 합니다.
따라서, S3에 분산저장하는 경우, 우리는 Hadoop 클라이언트를 거쳐 저장하게 됩니다.
Hadoop은 <code class="highlighter-rouge">S3N, S3A, S3</code> 이렇게 세 가지 시스템 클라이언트를 제공합니다. 각 클라이언트는 URI 스키마를 통해 접근할 수 있습니다.</p>

<ul>
  <li><strong>S3N (s3n://)</strong> : S3N은 S3에 일반 파일을 읽고 쓰는 기본 파일 시스템입니다. S3N은 안정적이며 널리 사용되고 있지만 현재는 업데이트가 중단되었습니다. S3N의 단점은 파일 엑세스가 한번에 5GB로 제한되어 있다는 점입니다.</li>
  <li><strong>S3A (s3a://)</strong> : S3A는 S3N을 개선한 다음 버전의 파일 시스템입니다. S3A는 Amazon의 라이브러리를 사용하여 S3와 상호 작용합니다. S3A는 5GB 이상의 파일 액세스를 지원하며 성능이 많이 향상되었습니다.</li>
  <li><strong>S3 (s3://)</strong> : S3는 Hadoop 0.10 버전부터 나온 블록 기반의 S3 파일 시스템 입니다. 따라서 파일이 HDFS에 있는 것과 같이 블록으로 저장됩니다.</li>
</ul>

<p>EMR은 EMRFS 라는 파일 시스템이 별도로 존재합니다.
EMR의 S3 파일 시스템과 Hadoop에서의 S3 파일 시스템은 서로 다르기 때문에 항상 주의하셔야 합니다.
EMR의 경우 <strong>s3</strong> 로 사용하는 것을 권장하고 있습니다. 반면에 s3a의 경우 EMRFS와 호환되지 않는다고 합니다.
물론 실행 될 때도 있지만 위와 같은 오류가 발생할 수도 있습니다.</p>

<p>​</p>

<h2 id="parquet---">Parquet 저장 성능 개선하기</h2>

<p>위의 오류는 URI를 s3로 수정해서 해결할 수 있었습니다.
하지만 S3에 parquet로 저장하는 속도가 너무 느려 이 부분을 개선해보기로 했습니다.</p>

<p>먼저 Spark에는 Parquet 빌드 속도를 개선하기 위해 <code class="highlighter-rouge">DirectParquetOutputCommitter</code>라는 기능이 있었습니다.
하지만, S3에 저장할 때 이 기능을 사용하는 경우 데이터 유실이 발생할 수 있었습니다.
<a href="https://issues.apache.org/jira/browse/SPARK-10063">SPARK-10063 JIRA 티켓 참고</a></p>

<p>이러한 이유로 Spark 2.0 버전부터 이 옵션은 사라졌습니다. 그러나, 성능 개선이 필요했기 때문에 Spark 사용자들은 대안을 요구했습니다.
본래의 FileCommiter가 느린 이유는 rename 연산 때문이었습니다.
실제 파일 시스템(HDFS)에서 rename 연산은 대상 파일 시스템의 임시 디렉토리로 출력 한 다음, 디렉토리의 이름을 커밋하는 방식으로 O(1)이 소요됩니다.
하지만 Object Storage에 저장하는 경우, 데이터 사이즈만큼 O(N)이 소요됩니다.</p>

<p>이 문제는 s3guard와 s3a의 도움으로 해결되었습니다.
getFileStatus()에서의 S3 HTTP 콜을 생략하고 dynamo metadata 저장 등을 통해 해결했다는데 자세한 내용은 <a href="https://issues.apache.org/jira/browse/MAPREDUCE-4815">MAPREDUCE-4815 JIRA 티켓</a>을 보시는게 나을 듯 합니다.</p>

<figure class="highlight"><pre><code class="language-md" data-lang="md">spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version 2
spark.speculation False</code></pre></figure>

<p>적용하는 방법은 위의 Spark property 옵션을 추가해주시면 됩니다. Spark 2.1, Hadoop 2.7.2 버전 이상부터 사용가능 합니다.
하지만 Spark 문서에도 나와있듯이 아직 failure에 대한 보장이 떨어집니다.
따라서 먼저 로컬 HDFS에 임시저장 후 distcp 명령어를 사용하여 S3로 저장해주시면 됩니다.
Hadoop 2.8 버전부터는 s3guard가 기본으로 들어가기 때문에 안정화 될 것 이라고 합니다.</p>

<p>결과는 로그 1억 건 기준 <strong>약 10배</strong> 의 성능 개선을 확인할 수 있었습니다.
두서없이 정리하다보니 좀 글이 복잡해졌네요. 결론은 ‘옵션을 추가하자’ 입니다.</p>

<p>​</p>

<h3 id="reference">Reference</h3>

<ul>
  <li><a href="https://github.com/steveloughran/hadoop/blob/s3guard/HADOOP-13786-committer/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/s3a_committer_architecture.md">S3A Commiter가 아키텍쳐 및 구현 세부사항에 대하여 정리한 글</a></li>
  <li>
    <p><a href="https://aws.amazon.com/ko/premiumsupport/knowledge-center/emr-file-system-s3/">AWS 공식 문서에서 정리한 글 : S3N, S3A, S3</a></p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>Spark의 Shuffling 이해하기</title>
	  <link>//spark-shuffling</link>
	  <author>Swalloow</author>
	  <pubDate>2017-08-25T19:18:00+09:00</pubDate>
	  <guid>//spark-shuffling</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>효율적인 Spark Application을 개발하기 위해 <strong>Shuffling</strong> 은 상당히 중요한 개념입니다.
이에 대해 간단히 정리해보았습니다.</p>

<p>​</p>

<h2 id="spark-architecture-shuffle">Spark Architecture: Shuffle</h2>

<p><img src="/assets/images/shuffle.png" alt="" /></p>

<p>Shuffle을 설명하기 전에 한 가지 예시를 들어보겠습니다.
테이블에 전화 통화 기록 목록이 있고 매일 발생한 통화량을 계산한다고 가정 해보겠습니다.
“날짜”를 키로 설정하고 각 레코드에 대해 값으로 “1”을 지정한 다음, 각 키의 값을 합산하여 결과 값을 계산할 수 있을 것 입니다.</p>

<p>만일 데이터가 여러 클러스터에 저장되어 있다면 어떻게 해야 동일한 키의 값을 합산할 수 있을까요?
이를 위한 유일한 방법은 같은 키의 모든 값을 동일한 시스템에 두는 것입니다. 그런 다음 이 값들을 합치면 됩니다.</p>

<p>​</p>

<h3 id="narrow-and-wide-transformation">Narrow and Wide Transformation</h3>

<p><img src="/assets/images/narrow_and_wide.png" alt="" /></p>

<p>몇 가지 사례를 통해 더 자세히 알아보겠습니다.
만일 데이터가 이미 키 값으로 파티셔닝 되어 있고 키 값에 대해 변화를 주고 싶다면, 좌측의 그림처럼 수행하게 됩니다.
<code class="highlighter-rouge">filter(), sample(), map(), flatMap()</code> 등의 transformation이 이에 해당하며, 이 경우 Shuffle이 필요 없습니다.
이를 <strong>Narrow Transformation</strong> 이라고 합니다.</p>

<p>반면, 서로 다른 파티션으로부터 특정한 값을 기준으로 추출하고 싶은 경우, 그 값을 기준으로 Shuffle이 발생하게 됩니다.
<code class="highlighter-rouge">groupByKey(), reduceByKey()</code> 등이 이에 해당하며, 이를 <strong>Wide Transformation</strong> 이라고 합니다.</p>

<p>​</p>

<h3 id="shuffled-hashjoin">Shuffled HashJoin</h3>

<p><img src="/assets/images/shuffle_join.png" alt="" /></p>

<p>두 개의 테이블을 <code class="highlighter-rouge">Join</code> 할 때에도 Shuffle 이 발생할 수 있습니다.
위의 예시 처럼 두 테이블에서 키 값을 기준으로 Join 하게 되면, 동일한 키를 가진 데이터가 동일한 파티션으로 이동합니다.</p>

<p>하지만 이 때, 셔플 되는 데이터의 양이 성능에 영향을 미칠 수 있습니다.
만일 C의 데이터의 크기가 A보다 훨씬 크다면, C에 대한 작업으로 인해 전체의 수행시간이 오래 걸리게 될 것 입니다.</p>

<p>​</p>

<h3 id="broadcast-hashjoin">Broadcast HashJoin</h3>

<p><img src="/assets/images/broadcast_join.png" alt="" /></p>

<p>이를 개선하기 위해 Spark에서는 <strong>Broadcast Join</strong> 을 제공합니다.
이 경우 RDD 중 하나가 모든 파티션으로 브로드 캐스팅되며 복사됩니다.
만일 RDD 중 하나가 다른 것에 비해 상당히 작다면 큰 RDD가 전혀 셔플 할 필요가 없습니다.
작은 RDD 만 모든 작업자 서버에 복사해야 하므로 Broadcast Join은 전체적으로 네트워크 트래픽을 줄여주는 효과가 있습니다.</p>

<p>Spark 1.2에서는 <code class="highlighter-rouge">spark.sql.autoBroadcastJoinThreshold</code> 값을 설정해주어야 했지만,
2.0 이후 버전의 경우 Spark SQL이 알아서 최적화 잘 해줍니다.</p>

<p>​</p>

<h3 id="spark-shuffle-properties">Spark Shuffle Properties</h3>

<ul>
  <li><code class="highlighter-rouge">spark.shuffle.compress</code>: 엔진이 shuffle 출력을 압축할지 여부를 지정</li>
  <li><code class="highlighter-rouge">spark.shuffle.spill.compress</code>: 중간 shuffle spill 파일을 압축할지 여부를 지정</li>
</ul>

<p>Shuffle에는 위의 두 가지 중요한 Spark Property 가 있습니다.</p>

<p>둘 다 기본적으로 값이 “true”이며, <code class="highlighter-rouge">spark.io.compression.codec</code> 압축 코덱을 기본으로합니다.
그리고 위에서 설명한 것처럼 Spark에는 여러 가지 셔플 구현이 있습니다.
특정 구현에서 사용되는 Shuffle은 <code class="highlighter-rouge">spark.shuffle.manager</code> 값에 의해 결정됩니다.
가능한 옵션은 <strong>hash, sort, tungsten-sort</strong> 이며, “sort” 옵션은 기본적으로 Spark 1.2.0부터 시작합니다.</p>

<p>이외에도 Spark Shuffle 관련된 Property는 아래의 공식문서에서 확인하실 수 있습니다.
<a href="https://spark.apache.org/docs/latest/configuration.html#shuffle-behavior">https://spark.apache.org/docs/latest/configuration.html#shuffle-behavior</a></p>

<p>​</p>

<h3 id="reference">Reference</h3>

<ul>
  <li><a href="https://0x0fff.com/spark-architecture-shuffle">https://0x0fff.com/spark-architecture-shuffle</a></li>
  <li>
    <p><a href="https://www.slideshare.net/databricks/strata-sj-everyday-im-shuffling-tips-for-writing-better-spark-programs">https://www.slideshare.net/databricks/strata-sj-everyday-im-shuffling-tips-for-writing-better-spark-programs</a></p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>Spark groupByKey vs reduceByKey</title>
	  <link>//spark-reduceByKey-groupByKey</link>
	  <author>Swalloow</author>
	  <pubDate>2017-08-22T19:18:00+09:00</pubDate>
	  <guid>//spark-reduceByKey-groupByKey</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>Spark Application 성능 개선을 위한 <code class="highlighter-rouge">groupByKey, reduceBykey</code>에 대해 알아보겠습니다.</p>

<p>​</p>

<h2 id="groupbykey-vs-reducebykey">groupByKey vs reduceBykey</h2>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># reduceByKey</span>
<span class="n">spark</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s">"hdfs://..."</span><span class="p">)</span>
 <span class="o">.</span><span class="n">flatMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
 <span class="o">.</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">word</span><span class="p">:</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
 <span class="o">.</span><span class="n">reduceByKey</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>

<span class="c"># groupByKey</span>
<span class="n">spark</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s">"hdfs://..."</span><span class="p">)</span>
 <span class="o">.</span><span class="n">flatMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
 <span class="o">.</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">word</span><span class="p">:</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
 <span class="o">.</span><span class="n">groupByKey</span><span class="p">()</span>
 <span class="o">.</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">counts</span><span class="p">):</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">counts</span><span class="p">)))</span></code></pre></figure>

<p>가장 흔히 알고 있는 word count 예제를 예로 들어보겠습니다.
위의 예시는 reduceByKey를 사용했으며, 아래의 예시는 groupByKey를 사용했습니다.
둘의 결과는 같지만 성능은 확인히 차이가 납니다.</p>

<p>먼저 위의 코드에서 <code class="highlighter-rouge">flatMap, map</code> 까지는 동일한 노드에서 실행이 됩니다.
하지만 reducer 부분에서는 모든 동일한 단어 쌍을 같은 노드로 이동시켜야 하기 때문에 <strong>Shuffle</strong> 이 발생합니다.</p>

<p><img src="/assets/images/reduceByKey.png" alt="" /></p>

<p>우선 reduceByKey의 경우, 먼저 각 노드에서 중간 집계를 진행하고 이에 대한 결과를 동일한 키 값으로 전송합니다.</p>

<p><img src="/assets/images/groupByKey.png" alt="" /></p>

<p>반면, groupByKey는 각 노드에 있는 데이터에 대해 바로 Shuffle 과정을 거치게 되고 결과를 내보냅니다.
따라서 groupByKey는 네트워크를 통해 전송되는 데이터의 양이 많아질 뿐만 아니라, <strong>Out of disk</strong> 문제가 발생할 수도 있습니다.</p>

<p>Shuffle은 기본적으로 비용이 큰 연산입니다.
groupByKey는 reduceByKey로 대체될 수 있기 때문에 많은 문서에서 이를 권장하고 있습니다.</p>

<p>​</p>

<h3 id="reference">Reference</h3>

<ul>
  <li>
    <p><a href="https://www.slideshare.net/databricks/strata-sj-everyday-im-shuffling-tips-for-writing-better-spark-programs">https://www.slideshare.net/databricks/strata-sj-everyday-im-shuffling-tips-for-writing-better-spark-programs</a></p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>Hive Metastore 구축 관련 문제와 해결과정</title>
	  <link>//hive-metastore-issue</link>
	  <author>Swalloow</author>
	  <pubDate>2017-08-11T19:18:00+09:00</pubDate>
	  <guid>//hive-metastore-issue</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>최근 Hive Metastore를 구축하면서 겪은 이슈와 해결과정을 기록해두려고 합니다.
사용 환경은 Spark 2.1.1, Hive 2.1.1 입니다.</p>

<p>​</p>

<h3 id="hive-partition">Hive Partition</h3>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">EXTERNAL</span> <span class="k">TABLE</span> <span class="k">table_name</span> <span class="p">(</span>
<span class="n">col1</span> <span class="n">STRING</span><span class="p">,</span>
<span class="n">col2</span> <span class="n">STRING</span>
<span class="p">)</span>
<span class="n">PARTITIONED</span> <span class="k">BY</span> <span class="p">(</span><span class="k">key</span> <span class="n">STRING</span><span class="p">)</span>
<span class="n">STORED</span> <span class="k">AS</span> <span class="n">PARQUET</span>
<span class="k">LOCATION</span> <span class="s1">'location'</span><span class="p">;</span></code></pre></figure>

<p>Hive에서 보통 위와 같은 쿼리로 테이블을 생성합니다.
Metastore는 말 그대로 외부에 있는 테이블의 정보(스키마, 파티션 등)를 저장하는 개념입니다.
따라서 <strong>EXTERNAL TABLE</strong> 로 생성하지 않은 상태에서 테이블을 DROP 시키면 다 날아가게 됩니다.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">ALTER</span> <span class="k">TABLE</span> <span class="k">table_name</span>
<span class="k">ADD</span> <span class="n">PARTITION</span> <span class="p">(</span><span class="k">key</span><span class="o">=</span><span class="s1">'2017-08-11'</span><span class="p">);</span></code></pre></figure>

<p>도중에 Partition key를 추가하고 싶을 때는 위와 같은 쿼리를 통해 추가할 수 있습니다.
그러나, 추가한 정보가 바로 반영이 안될 때가 있습니다.</p>

<p>이 경우에는 <code class="highlighter-rouge">MSCK REPAIR TABLE table_name;</code> 쿼리로 해결할 수 있습니다.
MSCK는 Metastore Check의 약자라고 합니다.</p>

<p>​</p>

<h3 id="hive-metastore-parquet">Hive Metastore, Parquet</h3>

<p>먼저 겪었던 문제에 대해 설명드리자면 Hive Metastore에 분명히 테이블이 들어가있고,
Hue에서는 잘 보이는데 Zeppelin에서는 모든 데이터에 null 값이 찍혀있었습니다.</p>

<p>우선 Spark으로 Hive를 사용하는 방식이 2.0 버전 이후 부터 조금 변경되었습니다.
이전에는 HiveContext를 사용했다면, 이제 SparkSession에서 <code class="highlighter-rouge">.enableHiveSupport()</code> 추가만 하면 됩니다.
제플린에서는 SparkSession이 spark이라는 변수로 제공되는데,
이 경우 interpreter에 <code class="highlighter-rouge">zeppelin.spark.useHiveContext=true</code>를 추가해서 사용할 수 있습니다.</p>

<p>다시 문제로 돌아와서 좀 더 확인해보니 컬럼명에 대문자가 들어가면 모든 값이 null로 출력되고 있었습니다.
Spark 공식문서에 이와 관련된 내용이 잘 나와있습니다.</p>

<p>Spark SQL에서 Hive metastore로 데이터를 불러오는 경우, 성능 상의 이슈로 SerDe 대신 Spark SQL의 <strong>MetastoreParquet</strong> 를 사용합니다.
이때 주의사항으로 Hive는 대소문자를 구분하지 않지만, Parquet는 구분합니다. (Hive is case insensitive, while Parquet is not)</p>

<p>이를 위해 Spark 2.1.1 버전부터 새로운 Spark Properties가 추가되었습니다.</p>

<p>따라서, Zeppelin interpreter에 아래의 설정 값을 추가해주시면 해결됩니다.
<code class="highlighter-rouge">spark.sql.hive.caseSensitiveInferenceMode = INFER_AND_SAVE</code></p>

<p>​</p>

<h3 id="hive-tblproperties">Hive TBLPROPERTIES</h3>

<p>위에서 말한대로 Spark Properties를 추가하면,
Hive metastore의 parameter에 <code class="highlighter-rouge">spark.sql.sources.schema.part</code>가 생기게 됩니다.</p>

<p>여기에서 “field: name”에 대소문자가 잘 구분되는 경우, 문제가 없지만 간혹 소문자로 들어오는 경우가 있습니다.
이 경우에는 아래의 쿼리를 통해 Hive parameter를 수정해주시면 됩니다.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">ALTER</span> <span class="k">TABLE</span> <span class="k">table_name</span> <span class="k">SET</span> <span class="n">TBLPROPERTIES</span> <span class="p">(</span><span class="nv">"spark.sql.sources.schema.part.0"</span> <span class="o">=</span> <span class="nv">"fix this line"</span><span class="p">);</span></code></pre></figure>

<p>​</p>

<h3 id="reference">Reference</h3>

<ul>
  <li><a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#hive-metastore-parquet-table-conversion">https://spark.apache.org/docs/latest/sql-programming-guide.html#hive-metastore-parquet-table-conversion</a></li>
  <li>
    <p><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#upgrading-from-spark-sql-21-to-22">http://spark.apache.org/docs/latest/sql-programming-guide.html#upgrading-from-spark-sql-21-to-22</a></p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>Spark DataFrame을 MySQL에 저장하는 방법</title>
	  <link>//spark-df-mysql</link>
	  <author>Swalloow</author>
	  <pubDate>2017-07-17T19:18:00+09:00</pubDate>
	  <guid>//spark-df-mysql</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>Spark에서 MySQL에 접근하고 DataFrame을 read, write 하는 방법에 대해 정리해보았습니다.
참고로 저는 Spark 2.1.0 버전을 사용 중 입니다.</p>

<p>​</p>

<h3 id="mysql-jdbc-driver">MySQL JDBC Driver</h3>

<p>JDBC를 통해 접근하기 때문에 드라이버가 필요합니다.
만일 SBT를 사용하신다면, build.sbt에 maven의 <code class="highlighter-rouge">mysql-connector-java</code> 를 추가하시면 됩니다.</p>

<p>직접 jar 파일을 사용해야하는 상황이라면, 다음 링크를 통해 다운받으시면 됩니다.
<a href="https://dev.mysql.com/downloads/connector/j/">https://dev.mysql.com/downloads/connector/j/</a></p>

<p>그리고 받으신 jar 파일을 -jars 옵션으로 추가해주셔야 합니다.</p>

<p><code class="highlighter-rouge">–jars /home/example/jars/mysql-connector-java-5.1.26.jar</code></p>

<p>마지막으로 spark-submit 을 사용하신다면, –packages 옵션을 추가해주시면 됩니다.</p>

<p><code class="highlighter-rouge">--packages mysql:mysql-connector-java:5.1.39</code></p>

<p>​</p>

<h3 id="spark-dataframe-mysql">Spark DataFrame MySQL</h3>

<p>Spark의 DataFrame은 read, write 함수를 통해 쉽게 데이터를 가져오거나 저장할 수 있습니다.
아래 예시는 Scala 언어로 작성했습니다.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.spark.sql.SaveMode</span>
<span class="k">import</span> <span class="nn">java.util.Properties</span>

<span class="k">val</span> <span class="n">tempDF</span> <span class="k">=</span> <span class="nc">List</span><span class="o">((</span><span class="s">"1"</span><span class="o">,</span> <span class="s">"2017-06-01"</span><span class="o">,</span> <span class="s">"2017-06-03"</span><span class="o">)).</span><span class="n">toDF</span><span class="o">(</span><span class="s">"id"</span><span class="o">,</span> <span class="s">"start"</span><span class="o">,</span> <span class="s">"end"</span><span class="o">)</span>
<span class="k">val</span> <span class="n">properties</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Properties</span><span class="o">()</span>
<span class="n">properties</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">"user"</span><span class="o">,</span> <span class="s">"userId"</span><span class="o">)</span>
<span class="n">properties</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="s">"password"</span><span class="o">,</span> <span class="s">"password"</span><span class="o">)</span>
<span class="n">tempDF</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">mode</span><span class="o">(</span><span class="nc">SaveMode</span><span class="o">.</span><span class="nc">Append</span><span class="o">).</span><span class="n">jdbc</span><span class="o">(</span><span class="s">"jdbc:mysql://url/database"</span><span class="o">,</span> <span class="s">"table"</span><span class="o">,</span> <span class="n">properties</span><span class="o">)</span></code></pre></figure>

<p>위 예제에서는 Properties를 통해 설정값을 넣어주었습니다.
유저 정보나 주소는 맞게 변경해주시면 됩니다.</p>

<p>mode 라는 것이 있는데 <code class="highlighter-rouge">SaveMode.Append</code>는 기존의 테이블에 추가하는 방식이고
<code class="highlighter-rouge">SaveMode.Overwrite</code>의 경우 기존의 테이블을 새로운 데이터로 대체하는 방식입니다.</p>

<p>​</p>

	  ]]></description>
	</item>

	<item>
	  <title>Spark 2.2.0 릴리즈 업데이트 정리</title>
	  <link>//spark22</link>
	  <author>Swalloow</author>
	  <pubDate>2017-07-14T19:18:00+09:00</pubDate>
	  <guid>//spark22</guid>
	  <description><![CDATA[
	     <p>​</p>

<p>7월 11일 약 2개월 만에 Spark 2.2.0이 릴리즈 되었습니다.
어떤 변경 사항들이 있었는지 릴리즈 노트를 통해 간략하게 정리해보았습니다.</p>

<p>​</p>

<h3 id="pypi---pyspark-">pypi 를 통한 PySpark 설치</h3>

<p>드디어 PySpark이 <code class="highlighter-rouge">pip</code>을 지원하게 되었습니다.
<code class="highlighter-rouge">pip install pyspark</code> 명령어를 통해 쉽게 설치 가능합니다.
설치된 버전은 Spark 2.2.0 버전 입니다.</p>

<p><img src="/assets/images/pyspark-install.png" alt="" /></p>

<p><code class="highlighter-rouge">numpy, pandas</code> 파이썬 패키지에 dependency가 있으며,
자세한 사항은 <a href="https://pypi.python.org/pypi/pyspark">pypi 패키지 링크</a>를 통해 확인하실 수 있습니다.
이번 업데이트를 통해 standalone cluster에서 누구나 쉽게 사용해 볼 수 있을 듯 합니다.</p>

<p>​</p>

<h3 id="structured-streaming">Structured Streaming</h3>

<p>이번 버전부터 Structured Streaming이 새로 추가 되었습니다.
Structured Streaming은 스트리밍 어플리케이션을 더 빠르고 쉽게 개발하기 위해 만들어진 패키지입니다.</p>

<p>Spark Streaming이 내부적으로 RDD API를 지원하는 반면, Structured Streaming은 DataFrame, Dataset API를 지원합니다.
언어는 Scala, Java, Python 모두 지원하며, <code class="highlighter-rouge">readStream</code> 이라는 메서드를 통해 다양한 저장소로부터 데이터를 읽을 수 있습니다.
특히 이번 업데이트를 통해 Apache Kafka 스트리밍 지원이 추가되었습니다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># Subscribe to 1 topic</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span> \
  <span class="o">.</span><span class="n">readStream</span> \
  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s">"kafka"</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s">"kafka.bootstrap.servers"</span><span class="p">,</span> <span class="s">"host1:port1,host2:port2"</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s">"subscribe"</span><span class="p">,</span> <span class="s">"topic1"</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">load</span><span class="p">()</span>
<span class="n">df</span><span class="o">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s">"CAST(key AS STRING)"</span><span class="p">,</span> <span class="s">"CAST(value AS STRING)"</span><span class="p">)</span></code></pre></figure>

<p>Structured Streaming에 대한 자세한 내용은 <a href="http://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html">http://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html</a> 에서 확인하실 수 있습니다.</p>

<p>​</p>

<h3 id="mllib">MLlib</h3>

<p>예상했던 대로 MLlib에도 많은 변화가 생겼습니다.
RDD-based MLlib이 아니라 DataFrame-based MLlib을 확인하시면 됩니다.</p>

<ul>
  <li>기존에 scala API만 지원하던 모델들에 <code class="highlighter-rouge">python, R API</code>가 추가되었습니다.</li>
  <li>지원이 추가된 모델은 <strong>Gradient Boosted Trees, Bisecting K-Means, LSH, Distributed PCA, SVD</strong> 입니다.</li>
  <li>DataFreame-based MLlib에 새로운 모델이 추가되었습니다.</li>
  <li>
    <p>추가된 모델은 <strong>LinearSVC (Linear SVM Classifier), ChiSquare test, Correlation,
Imputer feature transformer, Tweedie distribution, FPGrowth frequent pattern mining, AssociationRules</strong> 입니다.</p>

    <p>​</p>
  </li>
</ul>

<h3 id="sparkr">SparkR</h3>

<p>이번 업데이트를 통해 SparkR에서 Spark SQL API가 확대되었습니다.</p>

<ul>
  <li>R API에 Structured Streaming, Catalog가 추가되었습니다.</li>
  <li>to_json, from_json 메서드가 추가되었습니다.</li>
  <li>
    <p>Coalesce, DataFrame checkpointing, Multi-column approxQuantile 기능이 추가되었습니다.</p>

    <p>​</p>
  </li>
</ul>

<h3 id="graphx">GraphX</h3>

<p>GraphX는 버그 수정, 최적화 업데이트가 추가되었습니다.
이번 Structured Steaming이 메인에 추가된 것으로 보아,
추후에 DataFrame, DataSet API 기반의 GraphFrame이 추가될 수도 있다고 예상합니다.</p>

<ul>
  <li>PageRank, vertexRDD/EdgeRDD checkpoint 버그를 수정했습니다.</li>
  <li>
    <p>PageRank, Pregel API가 개선되었습니다.</p>

    <p>​</p>
  </li>
</ul>

<h3 id="core-and-sparksql-deprecations">Core and SparkSQL, Deprecations</h3>

<p>마지막으로 Core, SparkSQL 그리고 Deprecation 업데이트 입니다.
전체 업데이트 및 기타 자세한 내용은 맨 아래의 링크를 참고하시면 됩니다.</p>

<ul>
  <li>Python 2.6, Java 7, Hadoop 2.5 지원이 종료되었습니다.</li>
  <li><code class="highlighter-rouge">ALTER TABLE table_name ADD COLUMNS</code> 구문이 추가되었습니다.</li>
  <li>Cost-Based Optimizer 성능이 개선되었습니다.</li>
  <li>
    <p>CSV, JSON 포멧의 File listing/IO 성능이 개선되었습니다.</p>

    <p>​</p>
  </li>
</ul>

<h2 id="reference">Reference</h2>

<ul>
  <li>
    <p><a href="http://spark.apache.org/releases/spark-release-2-2-0.html">http://spark.apache.org/releases/spark-release-2-2-0.html</a></p>

    <p>​</p>
  </li>
</ul>

	  ]]></description>
	</item>


</channel>
</rss>
