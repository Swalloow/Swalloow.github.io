---
layout: post
title: "[CS224d] Lecture2. Word Vectors"
tags: [CS224d, NLP]
image:
  feature: cs224.jpg
---

   ​

### How do we represent the meaning of a word?

"단어의 의미를 어떻게 표현해야 할까?"

- 사람이 단어나 몸짓으로 표현하는 것은 쉽지만 컴퓨터가 이를 이해하기는 어렵다.
- 글쓰기, 미술 등으로 표현하는 것 또한 마찬가지이다.

   ​

### How to represent meaning in a computer?

과거에는 WordNet과 같은 방법을 사용했다. (전부 사람이 직접 구축)

WordNet : 각 단어끼리의 관계(상위단어, 동의어) 가 나타나 있는 트리구조의 그래프 모형

   ​

### Problems with this discrete representation

1. WordNet과 같은 경우 리소스로는 충분하지만 뉘앙스가 부족하다. (동의어 문제)

   `he's good / he's very proficient` 와 같은 문장을 비교해보면,

   proficient는 보통 말을 능숙하게 하는 사람을 표현하듯이 문맥에 따라 의미가 달라질 것이다.

   ​

2. 매번 신조어가 나타나기 때문에 최신화하기 어렵다.

   wicked, badass, ninjia... (미국에서 쓰는 신조어인가보다)

   ​

3. 주관적이고 유지하는데 사람의 노동이 필요하다.

   사람들이 직접 구축한 것이기 때문에 주관적이다.

   특히 WordNet은 영어 이외의 언어에서 잘 구축된 경우가 없다.

   ​

4. 단어 간의 유사도를 계산하기 어렵다.

   어떤 단어가 어느정도 동의어인지 아닌지 계산하기도 어렵다.

   ​

그래서 보통 Rule-based 와 통계적 NLP를 사용하는 모델들은 단어를 `atomic symbol`로써 사용한다.

예를 들어, 2만 개의 단어 중에 `hotel` 이라는 단어를 vector space로 나타낸다면,

`[0, 0, 0, 0, 0, 0, ..., 0, 0, 1, 0, 0, 0]` 이런 식이다. (2만 개의 차원을 가짐)

이런 경우, `motel` 이라는 단어와 AND operation을 통해 유사도를 계산한다면 

무조건 0이 나오게 될 것이다. (별로 좋지 않다...)

   ​

이러한 벡터를 `One-Hot Vector` 라고 부르며,

이러한 NLP 방법론을 `Bag of Words Representation` 이라고 부른다.



### Distributional similarity based representations

이 방법은 현대 통계적 NLP 방법론 중에서 가장 좋게 평가받는 모델 중 하나이다.

![distributional](..\images\distributional.PNG)

위 그림을 예로 들면, banking 이라는 단어를 표현할 때,

단어의 왼쪽과 오른쪽으로부터 banking이라는 단어의 정보를 얻는다. (dept, crises...)

이러한 방법을 통해 문맥으로부터 뭔가 더 얻을 수 있지 않을까? 라는 아이디어

   ​

### How to make neighbors represent words?

그렇다면 어떻게 이웃된 정보를 표현할 수 있을까? `cooccurrence matrix X`를 통해 표현한다.

여기에는 2가지 옵션이 있는데 full document 와 window 이다.

   ​

1. Full document matrix

   전체 문서의 matrix X를 통해 일반적인 토픽을 얻는다.

   예를 들면, `swimming / wet / boat / ship` 은 문맥에 의해 어떤 하나의 공통된 토픽을 갖게 될 것이다.

   대표적으로 `Latent Semantic Analysis` 와 같은 모델이 있다. (이 강의에서는 크게 다루지 않음)

   ​

2. Window based matrix

   이 방법은 Window의 길이 (일반적으로 5 - 10) 에 따라 대칭적으로 이동하면서 확인하는 방법

   - I like deep learning.
   - Iike NLP.
   - I enjoy flying

   ​

   위와 같은 corpus가 있을 때, 이를 matrix로 표현하면 다음과 같다.

   간단히 보면 각 단어의 빈도 수를 체크한 것이다.

![matrix](..\images\matrix.PNG)

   ​

### Problems with simple cooccurrence vectors

이 방법의 문제점은 단어의 크기가 커지면 matrix의 차원이 엄청나게 커진다는 점이다.

그리고 차원이 아주 크기 때문에 메모리를 많이 잡아먹는다.

이 문제를 해결하는 방법은 Low dimensional vector를 사용하는 것이다.



### Method 1: Dimensionality Reduction on X

그렇다면 차원은 어떻게 낮출 수 있을까? 이에 대한 방법을 소개한다.

- Singular Value Decomposition (SVD)
  - Decomposition (행렬 내의 dependancy를 풀어주기 위한 방법)

![svd](..\images\svd.PNG)

​	SVD는 원래의 matrix X를 3개의 matrix로 분해한다.

​	SVD는 n보다 훨씬 작은 값 k를 설정해서 top - k만 가져가고,

​	그 이상의 차원은 날려버리는 방법으로 차원 축소를 해버린다.



### Hacks to X

아직 몇 가지 문제가 남아있다.

function words (the, he, has)가 너무 빈번하게 나타나는데,

count를 세게 되면 위와 같은 단어가 큰 영향을 미친다.

   ​

이를 해결하기 위한 방법으로는

1. corpus 자체에서 저런 단어를 모두 지운다.

2. min(X, t), with t~100 : min을 씌워 빈번하게 발생하는 단어의 영향력을 낮춘다.

   (100번이 넘으면 무조건 100으로 고정)

3. Ramped windows : window가 맞물려 있을 때 가까운 단어에 더 가중치

4. Pearson correlations

   ​

### Problems with SVD

SVD는 다음과 같은 문제가 있다.

1. O($$ mn^2 $$) 의 복잡도를 갖기 때문에 Computational cost가 심하다.

   수백만의 단어나 문서를 사용하는 경우 좋지 않다.

   ​

2. 새로운 단어를 새로 적용하기가 어렵다.

   매번 새로 돌려야 하며, 다른 딥러닝 모델과 학습체계가 다르다.

   ​

## Word2Vec

아이디어 : 처음부터 바로 낮은 차원의 word vectors를 학습시키자!

이전에는 cooccurrence를 직접 count 했다면,

Word2Vec은 모든 단어에 대해 주변의 단어를 예측한다.

더 빠르고 쉽게 새로운 문장과 어울리며, 새로운 단어를 추가할 수도 있다.

   ​

본인이 참여하신 논문 : "Pennington 2014, Glove : Global Vectors for Word Representation"

(개념은 비슷한데 Word2Vec은 구글에서 만들었고, Glove는 스탠포드에서 만들었다)

   ​

### Details of Word2Vec

window 크기만큼 양 옆으로 확장해가며 sliding 방식으로 이동해서 전체 단어로 확장한다.

Objective Function : 중심단어를 기준으로 log probability가 최대가 되도록 한다.

J Function : wt가 나왔을 때, wt 주변의 단어가 나올 확률이 얼마나 되는지를 의미



![word2vec cost](..\images\word2vec cost.PNG)

- window length : m
- T token의 아주 큰 corpus
- m만큼 좌우 확인
- 하나의 중심 단어로 주변의 단어를 찾는 probability를 최대화하는 것이 목표

   ​

![word2vec cost](..\images\word2vec cost2.PNG)

- o : outside word id
- c : center word id
- u : outside vectors of o
- v : center vectors of c
- 일반적으로 하나의 벡터를 전치시켜 두 벡터를 내적한다. (uTv)

   ​

내적한 값은 단어 간의 유사도를 의미한다.

p가 최대가 되게 하려면, 분자가 크고 분모가 작아야 한다.

   ​

항상 두개의 벡터가 나오는데 하나의 벡터는 outside word를 표현하고,

다른 하나의 벡터는 outside words를 예측하는데 사용한다.

   ​

여기서부터는 화이트보드에 수식을 풀어서 설명

50분부터 `Chain Rule`에 대한 설명 참조

59분부터 `log probability`를 적용한 설명 참조

   ​

### Approximate the Normalization

이 부분은 Nagative Sampling에 관련된 내용입니다.

전체 데이터를 보면 어차피 대부분은 연관이 없고(negative), 확실히 연관이 있는 것은 적다 (positive)

그래서 문맥에 나타나지 않는 단어들 몇 개만 샘플링하는 `Nagative Prediction` 방법을 사용해보자.

이렇게 해보면 속도는 더 빠르고 성능이 괜찮게 나온다. (구글 제프딘의 아이디어!)

   ​

### Linear Relationships in word2vec

이러한 표현 방식은 유사도를 계산하는데 아주 좋은 결과를 보인다.

단순히 vector subtraction으로도 연산 가능

한국어 word2vec 데모 : http://w.elnn.kr/search/

   ​

### Count based vs Direct prediction

1. Count based

   - LSA, HAL, COALS, PCA
   - Training 속도가 빠름
   - 통계기법을 효율적으로 사용
   - (단점) 단어의 유사도를 계산하는데만 사용 (Relationship 계산 불가)
   - (단점) count가 큰 경우에 불균등한 중요성이 주어짐 (?)

​

2.  Direct prediction
    - NNLM, HLBL, RNN, Skip-gram/CBOW
    - (단점) 말뭉치의 크기에 영향을 받음 (많이 필요)
    - (단점) 통계기법을 활용하지 않음
    - 대부분의 영역에서 좋은 성능을 내는 모델 생성
    - 단어 유사도에 대해 복잡한 패턴도 잡아냄

